{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHhUMVsj67vqncaTb2spP8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import wandb"],"metadata":{"id":"ryGvDWvTPLHZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -r M2M-100/random*"],"metadata":{"id":"llHJJgqn-Hvs","executionInfo":{"status":"ok","timestamp":1743447705034,"user_tz":-120,"elapsed":2049,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Log in with your API key\n","wandb.login(key=\"7f13d9fe09d0856f7c12099a27ccda7aa15c8afd\")\n","\n","# Initialize WandB\n","wandb.init(project=\"translation_project\", name=\"prelims\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"id":"tktgwDM-PIl_","executionInfo":{"status":"ok","timestamp":1740412584724,"user_tz":-120,"elapsed":3149,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"e3ec9af0-867a-4b48-b682-6a71127ba1ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250224_155622-6ugjt5h1</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/paderborn-university/translation_project/runs/6ugjt5h1' target=\"_blank\">prelims</a></strong> to <a href='https://wandb.ai/paderborn-university/translation_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/paderborn-university/translation_project' target=\"_blank\">https://wandb.ai/paderborn-university/translation_project</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/paderborn-university/translation_project/runs/6ugjt5h1' target=\"_blank\">https://wandb.ai/paderborn-university/translation_project/runs/6ugjt5h1</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/translation_project/runs/6ugjt5h1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a51f425c340>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3lwJJJuIOzrY","executionInfo":{"status":"ok","timestamp":1740417564487,"user_tz":-120,"elapsed":688333,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"1b50f088-8e9c-4944-c0a8-76973d6aeb0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-02-24 15:56:29.164325: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-02-24 15:56:29.186341: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-02-24 15:56:29.193137: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-02-24 15:56:29.209896: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-02-24 15:56:30.272704: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","02/24/2025 15:56:32 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","02/24/2025 15:56:32 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=MBART-50/runs/Feb24_15-56-32_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=MBART-50,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=MBART-50,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-7f46d737aa0519c2\n","02/24/2025 15:56:32 - INFO - datasets.builder - Using custom data configuration default-7f46d737aa0519c2\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","02/24/2025 15:56:32 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","02/24/2025 15:56:32 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","02/24/2025 15:56:32 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","02/24/2025 15:56:32 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","02/24/2025 15:56:32 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-02-24 15:56:33,088 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/4ef55a20b36c6903b832e38f0604ab4bdf22c7d6/config.json\n","[INFO|configuration_utils.py:771] 2025-02-24 15:56:33,090 >> Model config MBartConfig {\n","  \"_name_or_path\": \"facebook/mbart-large-50\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": true,\n","  \"architectures\": [\n","    \"MBartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"mbart\",\n","  \"normalize_before\": true,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"static_position_embeddings\": false,\n","  \"tokenizer_class\": \"MBart50Tokenizer\",\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250054\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-02-24 15:56:33,172 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/4ef55a20b36c6903b832e38f0604ab4bdf22c7d6/config.json\n","[INFO|configuration_utils.py:771] 2025-02-24 15:56:33,173 >> Model config MBartConfig {\n","  \"_name_or_path\": \"facebook/mbart-large-50\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": true,\n","  \"architectures\": [\n","    \"MBartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"mbart\",\n","  \"normalize_before\": true,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"static_position_embeddings\": false,\n","  \"tokenizer_class\": \"MBart50Tokenizer\",\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250054\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-02-24 15:56:33,174 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/4ef55a20b36c6903b832e38f0604ab4bdf22c7d6/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-02-24 15:56:33,174 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-02-24 15:56:33,174 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-02-24 15:56:33,174 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/4ef55a20b36c6903b832e38f0604ab4bdf22c7d6/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-02-24 15:56:33,174 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/4ef55a20b36c6903b832e38f0604ab4bdf22c7d6/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-02-24 15:56:33,174 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-02-24 15:56:33,175 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/4ef55a20b36c6903b832e38f0604ab4bdf22c7d6/config.json\n","[INFO|configuration_utils.py:771] 2025-02-24 15:56:33,175 >> Model config MBartConfig {\n","  \"_name_or_path\": \"facebook/mbart-large-50\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": true,\n","  \"architectures\": [\n","    \"MBartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"mbart\",\n","  \"normalize_before\": true,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"static_position_embeddings\": false,\n","  \"tokenizer_class\": \"MBart50Tokenizer\",\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250054\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-02-24 15:56:34,711 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/4ef55a20b36c6903b832e38f0604ab4bdf22c7d6/config.json\n","[INFO|configuration_utils.py:771] 2025-02-24 15:56:34,712 >> Model config MBartConfig {\n","  \"_name_or_path\": \"facebook/mbart-large-50\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": true,\n","  \"architectures\": [\n","    \"MBartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"mbart\",\n","  \"normalize_before\": true,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"static_position_embeddings\": false,\n","  \"tokenizer_class\": \"MBart50Tokenizer\",\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250054\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-02-24 15:56:36,004 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/4ef55a20b36c6903b832e38f0604ab4bdf22c7d6/pytorch_model.bin\n","[INFO|safetensors_conversion.py:61] 2025-02-24 15:56:36,120 >> Attempting to create safetensors variant\n","[INFO|safetensors_conversion.py:74] 2025-02-24 15:56:36,542 >> Safetensors PR exists\n","[INFO|configuration_utils.py:1140] 2025-02-24 15:56:36,605 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-02-24 15:56:36,716 >> All model checkpoint weights were used when initializing MBartForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-02-24 15:56:36,716 >> All the weights of MBartForConditionalGeneration were initialized from the model checkpoint at facebook/mbart-large-50.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-02-24 15:56:36,805 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/4ef55a20b36c6903b832e38f0604ab4bdf22c7d6/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-02-24 15:56:36,806 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","Running tokenizer on train dataset:   0% 0/10000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-400075fade822f9f.arrow\n","02/24/2025 15:56:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-400075fade822f9f.arrow\n","Running tokenizer on train dataset: 100% 10000/10000 [00:01<00:00, 9639.79 examples/s] \n","Running tokenizer on validation dataset:   0% 0/3000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-3c69a2a82fa19d93.arrow\n","02/24/2025 15:56:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-3c69a2a82fa19d93.arrow\n","Running tokenizer on validation dataset: 100% 3000/3000 [00:00<00:00, 13240.11 examples/s]\n","Running tokenizer on prediction dataset:   0% 0/1000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-990e1ca136dbc58f.arrow\n","02/24/2025 15:56:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-990e1ca136dbc58f.arrow\n","Running tokenizer on prediction dataset: 100% 1000/1000 [00:00<00:00, 14557.94 examples/s]\n","[INFO|trainer.py:2407] 2025-02-24 15:56:49,557 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-02-24 15:56:49,557 >>   Num examples = 10,000\n","[INFO|trainer.py:2409] 2025-02-24 15:56:49,557 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-02-24 15:56:49,557 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-02-24 15:56:49,558 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-02-24 15:56:49,558 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-02-24 15:56:49,558 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-02-24 15:56:49,558 >>   Total optimization steps = 3,750\n","[INFO|trainer.py:2416] 2025-02-24 15:56:49,559 >>   Number of trainable parameters = 610,879,488\n","[INFO|integration_utils.py:817] 2025-02-24 15:56:49,564 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250224_155653-uukkavjf\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMBART-50\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/uukkavjf\u001b[0m\n","  0% 0/3750 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.7658, 'grad_norm': 6.206345558166504, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}\n"," 13% 500/3750 [07:25<50:51,  1.07it/s][INFO|trainer.py:3944] 2025-02-24 16:04:20,286 >> Saving model checkpoint to MBART-50/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-02-24 16:04:20,296 >> Configuration saved in MBART-50/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 16:04:20,297 >> Configuration saved in MBART-50/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 16:04:28,491 >> Model weights saved in MBART-50/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 16:04:28,494 >> tokenizer config file saved in MBART-50/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 16:04:28,495 >> Special tokens file saved in MBART-50/checkpoint-500/special_tokens_map.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.6759, 'grad_norm': 6.123976707458496, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}\n"," 27% 1000/3750 [15:18<40:32,  1.13it/s][INFO|trainer.py:3944] 2025-02-24 16:12:12,912 >> Saving model checkpoint to MBART-50/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-02-24 16:12:12,914 >> Configuration saved in MBART-50/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 16:12:12,914 >> Configuration saved in MBART-50/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 16:12:20,358 >> Model weights saved in MBART-50/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 16:12:20,361 >> tokenizer config file saved in MBART-50/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 16:12:20,362 >> Special tokens file saved in MBART-50/checkpoint-1000/special_tokens_map.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.1208, 'grad_norm': 5.666250705718994, 'learning_rate': 3e-05, 'epoch': 1.2}\n"," 40% 1500/3750 [23:12<33:42,  1.11it/s][INFO|trainer.py:3944] 2025-02-24 16:20:06,824 >> Saving model checkpoint to MBART-50/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-02-24 16:20:06,826 >> Configuration saved in MBART-50/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 16:20:06,827 >> Configuration saved in MBART-50/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 16:20:14,220 >> Model weights saved in MBART-50/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 16:20:14,223 >> tokenizer config file saved in MBART-50/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 16:20:14,223 >> Special tokens file saved in MBART-50/checkpoint-1500/special_tokens_map.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.682, 'grad_norm': 6.200084686279297, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}\n"," 53% 2000/3750 [31:04<25:52,  1.13it/s][INFO|trainer.py:3944] 2025-02-24 16:27:59,061 >> Saving model checkpoint to MBART-50/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-02-24 16:27:59,063 >> Configuration saved in MBART-50/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 16:27:59,064 >> Configuration saved in MBART-50/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 16:28:06,263 >> Model weights saved in MBART-50/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 16:28:06,266 >> tokenizer config file saved in MBART-50/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 16:28:06,266 >> Special tokens file saved in MBART-50/checkpoint-2000/special_tokens_map.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.5841, 'grad_norm': 7.043832778930664, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 2500/3750 [38:56<17:59,  1.16it/s][INFO|trainer.py:3944] 2025-02-24 16:35:51,485 >> Saving model checkpoint to MBART-50/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-02-24 16:35:51,487 >> Configuration saved in MBART-50/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 16:35:51,488 >> Configuration saved in MBART-50/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 16:35:58,842 >> Model weights saved in MBART-50/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 16:35:58,844 >> tokenizer config file saved in MBART-50/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 16:35:58,845 >> Special tokens file saved in MBART-50/checkpoint-2500/special_tokens_map.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9399, 'grad_norm': 7.2533087730407715, 'learning_rate': 1e-05, 'epoch': 2.4}\n"," 80% 3000/3750 [46:49<11:01,  1.13it/s][INFO|trainer.py:3944] 2025-02-24 16:43:44,150 >> Saving model checkpoint to MBART-50/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-02-24 16:43:44,152 >> Configuration saved in MBART-50/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 16:43:44,153 >> Configuration saved in MBART-50/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 16:43:51,575 >> Model weights saved in MBART-50/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 16:43:51,578 >> tokenizer config file saved in MBART-50/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 16:43:51,578 >> Special tokens file saved in MBART-50/checkpoint-3000/special_tokens_map.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9254, 'grad_norm': 6.582479000091553, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}\n"," 93% 3500/3750 [54:41<03:44,  1.11it/s][INFO|trainer.py:3944] 2025-02-24 16:51:35,913 >> Saving model checkpoint to MBART-50/checkpoint-3500\n","[INFO|configuration_utils.py:423] 2025-02-24 16:51:35,915 >> Configuration saved in MBART-50/checkpoint-3500/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 16:51:35,916 >> Configuration saved in MBART-50/checkpoint-3500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 16:51:43,054 >> Model weights saved in MBART-50/checkpoint-3500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 16:51:43,057 >> tokenizer config file saved in MBART-50/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 16:51:43,058 >> Special tokens file saved in MBART-50/checkpoint-3500/special_tokens_map.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 3750/3750 [58:49<00:00,  1.16it/s][INFO|trainer.py:3944] 2025-02-24 16:55:43,926 >> Saving model checkpoint to MBART-50/checkpoint-3750\n","[INFO|configuration_utils.py:423] 2025-02-24 16:55:43,928 >> Configuration saved in MBART-50/checkpoint-3750/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 16:55:43,929 >> Configuration saved in MBART-50/checkpoint-3750/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 16:55:51,462 >> Model weights saved in MBART-50/checkpoint-3750/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 16:55:51,465 >> tokenizer config file saved in MBART-50/checkpoint-3750/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 16:55:51,466 >> Special tokens file saved in MBART-50/checkpoint-3750/special_tokens_map.json\n","[INFO|trainer.py:2659] 2025-02-24 16:56:10,275 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 3560.7164, 'train_samples_per_second': 8.425, 'train_steps_per_second': 1.053, 'train_loss': 2.885698494466146, 'epoch': 3.0}\n","100% 3750/3750 [59:15<00:00,  1.05it/s]\n","[INFO|trainer.py:3944] 2025-02-24 16:56:10,284 >> Saving model checkpoint to MBART-50\n","[INFO|configuration_utils.py:423] 2025-02-24 16:56:10,286 >> Configuration saved in MBART-50/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 16:56:10,286 >> Configuration saved in MBART-50/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 16:56:21,341 >> Model weights saved in MBART-50/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 16:56:21,344 >> tokenizer config file saved in MBART-50/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 16:56:21,344 >> Special tokens file saved in MBART-50/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  1966183GF\n","  train_loss               =     2.8857\n","  train_runtime            = 0:59:20.71\n","  train_samples            =      10000\n","  train_samples_per_second =      8.425\n","  train_steps_per_second   =      1.053\n","02/24/2025 16:56:21 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-02-24 16:56:21,410 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-02-24 16:56:21,410 >>   Num examples = 3000\n","[INFO|trainer.py:4265] 2025-02-24 16:56:21,410 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 375/375 [18:05<00:00,  2.90s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     7.0802\n","  eval_gen_len            =    23.1413\n","  eval_loss               =      2.823\n","  eval_runtime            = 0:18:07.08\n","  eval_samples            =       3000\n","  eval_samples_per_second =       2.76\n","  eval_steps_per_second   =      0.345\n","02/24/2025 17:14:28 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-02-24 17:14:28,518 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-02-24 17:14:28,518 >>   Num examples = 1000\n","[INFO|trainer.py:4265] 2025-02-24 17:14:28,518 >>   Batch size = 8\n","100% 125/125 [04:51<00:00,  2.33s/it]\n","***** predict metrics *****\n","  predict_bleu               =     8.4454\n","  predict_gen_len            =     20.553\n","  predict_loss               =     2.4694\n","  predict_runtime            = 0:04:53.11\n","  predict_samples            =       1000\n","  predict_samples_per_second =      3.412\n","  predict_steps_per_second   =      0.426\n","[INFO|modelcard.py:449] 2025-02-24 17:19:21,785 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 7.0802}]}\n"]}],"source":["#!wandb off\n","!python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/mbart-large-50 \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en_XX \\\n","    --target_lang xh_ZA \\\n","    --train_file /content/data/en-xh/trainset.json \\\n","    --validation_file /content/data/en-xh/valset.json \\\n","    --test_file /content/data/en-xh/testset.json \\\n","    --num_beams 10 \\\n","    --output_dir MBART-50 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]}]}