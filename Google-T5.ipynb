{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOc/VnWpBotNv7nOWOkLvt4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"kJBZKtQVdUJ7","executionInfo":{"status":"ok","timestamp":1740408764025,"user_tz":-120,"elapsed":1282,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"outputs":[],"source":["import numpy as np\n","import wandb\n","import jsonlines"]},{"cell_type":"code","source":["!rm -r Google_T5/"],"metadata":{"id":"48MtqLmQcKGp","executionInfo":{"status":"ok","timestamp":1740408764026,"user_tz":-120,"elapsed":8,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Log in with your API key\n","wandb.login(key=\"7f13d9fe09d0856f7c12099a27ccda7aa15c8afd\")\n","\n","# Initialize WandB\n","wandb.init(project=\"translation_project\", name=\"prelims\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"id":"u-koqG2Idfn3","executionInfo":{"status":"ok","timestamp":1740408766913,"user_tz":-120,"elapsed":2893,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"04db7c21-be88-4d2b-c5ba-5748b7b3e625"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250224_145245-adv57iy4</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/paderborn-university/translation_project/runs/adv57iy4' target=\"_blank\">prelims</a></strong> to <a href='https://wandb.ai/paderborn-university/translation_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/paderborn-university/translation_project' target=\"_blank\">https://wandb.ai/paderborn-university/translation_project</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/paderborn-university/translation_project/runs/adv57iy4' target=\"_blank\">https://wandb.ai/paderborn-university/translation_project/runs/adv57iy4</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/translation_project/runs/adv57iy4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7ab9e2aaded0>"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# def convert_jsonl(input_file, output_file):\n","#     \"\"\"\n","#     Reads a JSONL file, remaps keys inside 'translation' from 'en' to 'eng_Latn' and 'xh' to 'xho_Latn',\n","#     then writes to a new JSONL file.\n","#     \"\"\"\n","#     with jsonlines.open(input_file, mode='r') as reader, jsonlines.open(output_file, mode='w') as writer:\n","#         for obj in reader:\n","#             if \"translation\" not in obj or \"en\" not in obj[\"translation\"] or \"xh\" not in obj[\"translation\"]:\n","#                 print(f\"Skipping entry: {obj}\")  # Debugging message for missing keys\n","#                 continue\n","\n","#             # Convert keys inside \"translation\"\n","#             new_data = {\n","#                 \"translation\": {\n","#                     \"eng\": obj[\"translation\"][\"en\"],  # Change 'en' to 'eng_Latn'\n","#                     \"xho\": obj[\"translation\"][\"xh\"]   # Change 'xh' to 'xho_Latn'\n","#                 }\n","#             }\n","\n","#             writer.write(new_data)\n","\n","\n","# # Convert all datasets\n","# convert_jsonl(\"/content/data/en-xh/trainset.json\", \"/content/data/en-xh/trainset_converted.json\")\n","# convert_jsonl(\"/content/data/en-xh/valset.json\", \"/content/data/en-xh/valset_converted.json\")\n","# convert_jsonl(\"/content/data/en-xh/testset.json\", \"/content/data/en-xh/testset_converted.json\")\n","\n","# print(\"✅ Dataset conversion complete! Use the new files in training.\")"],"metadata":{"id":"XPkjp_7iAs0F","executionInfo":{"status":"ok","timestamp":1740408766913,"user_tz":-120,"elapsed":6,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path google-t5/t5-base \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_prefix \"translate English to Xhosa: \" \\\n","    --source_lang en \\\n","    --target_lang xh \\\n","    --train_file /content/data/en-xh/trainset.json \\\n","    --validation_file /content/data/en-xh/valset.json \\\n","    --test_file /content/data/en-xh/testset.json \\\n","    --num_beams 10 \\\n","    --output_dir Google_T5 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1DfmpmMEdiRM","outputId":"bc859044-ccb6-486e-e2d0-e951d0d57c83","executionInfo":{"status":"ok","timestamp":1740412485040,"user_tz":-120,"elapsed":3718132,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-02-24 14:52:50.667666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-02-24 14:52:50.696819: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-02-24 14:52:50.708524: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-02-24 14:52:50.733285: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-02-24 14:52:52.131883: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","02/24/2025 14:52:54 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","02/24/2025 14:52:54 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=Google_T5/runs/Feb24_14-52-54_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=Google_T5,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=Google_T5,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-7f46d737aa0519c2\n","02/24/2025 14:52:55 - INFO - datasets.builder - Using custom data configuration default-7f46d737aa0519c2\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","02/24/2025 14:52:55 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","02/24/2025 14:52:55 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","02/24/2025 14:52:55 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","02/24/2025 14:52:55 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","02/24/2025 14:52:55 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","config.json: 100% 1.21k/1.21k [00:00<00:00, 9.80MB/s]\n","[INFO|configuration_utils.py:699] 2025-02-24 14:52:55,360 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/config.json\n","[INFO|configuration_utils.py:771] 2025-02-24 14:52:55,361 >> Model config T5Config {\n","  \"_name_or_path\": \"google-t5/t5-base\",\n","  \"architectures\": [\n","    \"T5ForConditionalGeneration\"\n","  ],\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 3072,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"relu\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"relu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": false,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n","[INFO|tokenization_auto.py:730] 2025-02-24 14:52:55,454 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:699] 2025-02-24 14:52:55,539 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/config.json\n","[INFO|configuration_utils.py:771] 2025-02-24 14:52:55,540 >> Model config T5Config {\n","  \"_name_or_path\": \"google-t5/t5-base\",\n","  \"architectures\": [\n","    \"T5ForConditionalGeneration\"\n","  ],\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 3072,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"relu\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"relu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": false,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n","spiece.model: 100% 792k/792k [00:00<00:00, 18.0MB/s]\n","tokenizer.json: 100% 1.39M/1.39M [00:00<00:00, 4.44MB/s]\n","[INFO|tokenization_utils_base.py:2050] 2025-02-24 14:52:56,763 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google-t5--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/spiece.model\n","[INFO|tokenization_utils_base.py:2050] 2025-02-24 14:52:56,763 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/tokenizer.json\n","[INFO|tokenization_utils_base.py:2050] 2025-02-24 14:52:56,763 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-02-24 14:52:56,763 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-02-24 14:52:56,763 >> loading file tokenizer_config.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-02-24 14:52:56,763 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-02-24 14:52:56,763 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/config.json\n","[INFO|configuration_utils.py:771] 2025-02-24 14:52:56,764 >> Model config T5Config {\n","  \"_name_or_path\": \"google-t5/t5-base\",\n","  \"architectures\": [\n","    \"T5ForConditionalGeneration\"\n","  ],\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 3072,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"relu\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"relu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": false,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n","model.safetensors: 100% 892M/892M [00:03<00:00, 228MB/s]\n","[INFO|modeling_utils.py:3984] 2025-02-24 14:53:01,266 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google-t5--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/model.safetensors\n","[INFO|configuration_utils.py:1140] 2025-02-24 14:53:01,278 >> Generate config GenerationConfig {\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-02-24 14:53:01,575 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-02-24 14:53:01,575 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google-t5/t5-base.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n","generation_config.json: 100% 147/147 [00:00<00:00, 1.27MB/s]\n","[INFO|configuration_utils.py:1095] 2025-02-24 14:53:01,788 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-02-24 14:53:01,788 >> Generate config GenerationConfig {\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0\n","}\n","\n","Running tokenizer on train dataset:   0% 0/10000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-05754ca2e47fe0bd.arrow\n","02/24/2025 14:53:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-05754ca2e47fe0bd.arrow\n","Running tokenizer on train dataset: 100% 10000/10000 [00:01<00:00, 7724.67 examples/s]\n","Running tokenizer on validation dataset:   0% 0/3000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7ba3a41fc3d65dd5.arrow\n","02/24/2025 14:53:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7ba3a41fc3d65dd5.arrow\n","Running tokenizer on validation dataset: 100% 3000/3000 [00:00<00:00, 10500.26 examples/s]\n","Running tokenizer on prediction dataset:   0% 0/1000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-22be261b1bbe1a83.arrow\n","02/24/2025 14:53:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f46d737aa0519c2/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-22be261b1bbe1a83.arrow\n","Running tokenizer on prediction dataset: 100% 1000/1000 [00:00<00:00, 10941.99 examples/s]\n","[INFO|trainer.py:2407] 2025-02-24 14:53:05,377 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-02-24 14:53:05,377 >>   Num examples = 10,000\n","[INFO|trainer.py:2409] 2025-02-24 14:53:05,377 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-02-24 14:53:05,377 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-02-24 14:53:05,377 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-02-24 14:53:05,377 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-02-24 14:53:05,377 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-02-24 14:53:05,377 >>   Total optimization steps = 3,750\n","[INFO|trainer.py:2416] 2025-02-24 14:53:05,378 >>   Number of trainable parameters = 222,903,552\n","[INFO|integration_utils.py:817] 2025-02-24 14:53:05,384 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250224_145305-nvh7t8lc\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mGoogle_T5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/nvh7t8lc\u001b[0m\n","  0% 0/3750 [00:00<?, ?it/s][WARNING|logging.py:329] 2025-02-24 14:53:08,098 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.8008, 'grad_norm': 1.9721357822418213, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}\n"," 13% 500/3750 [03:03<20:39,  2.62it/s][INFO|trainer.py:3944] 2025-02-24 14:56:10,112 >> Saving model checkpoint to Google_T5/checkpoint-500\n","[INFO|configuration_utils.py:423] 2025-02-24 14:56:10,114 >> Configuration saved in Google_T5/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 14:56:10,115 >> Configuration saved in Google_T5/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 14:56:12,208 >> Model weights saved in Google_T5/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 14:56:12,210 >> tokenizer config file saved in Google_T5/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 14:56:12,211 >> Special tokens file saved in Google_T5/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:176] 2025-02-24 14:56:12,212 >> Copy vocab file to Google_T5/checkpoint-500/spiece.model\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.3386, 'grad_norm': 2.346818685531616, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}\n"," 27% 1000/3750 [06:14<16:25,  2.79it/s][INFO|trainer.py:3944] 2025-02-24 14:59:20,736 >> Saving model checkpoint to Google_T5/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-02-24 14:59:20,738 >> Configuration saved in Google_T5/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 14:59:20,739 >> Configuration saved in Google_T5/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 14:59:22,771 >> Model weights saved in Google_T5/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 14:59:22,773 >> tokenizer config file saved in Google_T5/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 14:59:22,774 >> Special tokens file saved in Google_T5/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:176] 2025-02-24 14:59:22,775 >> Copy vocab file to Google_T5/checkpoint-1000/spiece.model\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.1551, 'grad_norm': 1.7407435178756714, 'learning_rate': 3e-05, 'epoch': 1.2}\n"," 40% 1500/3750 [09:27<13:51,  2.71it/s][INFO|trainer.py:3944] 2025-02-24 15:02:33,562 >> Saving model checkpoint to Google_T5/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-02-24 15:02:33,564 >> Configuration saved in Google_T5/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 15:02:33,565 >> Configuration saved in Google_T5/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 15:02:35,472 >> Model weights saved in Google_T5/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 15:02:35,475 >> tokenizer config file saved in Google_T5/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 15:02:35,475 >> Special tokens file saved in Google_T5/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:176] 2025-02-24 15:02:35,477 >> Copy vocab file to Google_T5/checkpoint-1500/spiece.model\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.0322, 'grad_norm': 2.252631902694702, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}\n"," 53% 2000/3750 [12:39<10:53,  2.68it/s][INFO|trainer.py:3944] 2025-02-24 15:05:46,067 >> Saving model checkpoint to Google_T5/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-02-24 15:05:46,068 >> Configuration saved in Google_T5/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 15:05:46,069 >> Configuration saved in Google_T5/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 15:05:48,133 >> Model weights saved in Google_T5/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 15:05:48,135 >> tokenizer config file saved in Google_T5/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 15:05:48,136 >> Special tokens file saved in Google_T5/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:176] 2025-02-24 15:05:48,137 >> Copy vocab file to Google_T5/checkpoint-2000/spiece.model\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.966, 'grad_norm': 2.3174030780792236, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 2500/3750 [15:50<07:22,  2.83it/s][INFO|trainer.py:3944] 2025-02-24 15:08:56,899 >> Saving model checkpoint to Google_T5/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-02-24 15:08:56,901 >> Configuration saved in Google_T5/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 15:08:56,902 >> Configuration saved in Google_T5/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 15:08:58,794 >> Model weights saved in Google_T5/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 15:08:58,797 >> tokenizer config file saved in Google_T5/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 15:08:58,797 >> Special tokens file saved in Google_T5/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:176] 2025-02-24 15:08:58,799 >> Copy vocab file to Google_T5/checkpoint-2500/spiece.model\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.9026, 'grad_norm': 3.1562745571136475, 'learning_rate': 1e-05, 'epoch': 2.4}\n"," 80% 3000/3750 [19:02<04:40,  2.67it/s][INFO|trainer.py:3944] 2025-02-24 15:12:08,497 >> Saving model checkpoint to Google_T5/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-02-24 15:12:08,499 >> Configuration saved in Google_T5/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 15:12:08,500 >> Configuration saved in Google_T5/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 15:12:10,569 >> Model weights saved in Google_T5/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 15:12:10,572 >> tokenizer config file saved in Google_T5/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 15:12:10,572 >> Special tokens file saved in Google_T5/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:176] 2025-02-24 15:12:10,574 >> Copy vocab file to Google_T5/checkpoint-3000/spiece.model\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.8812, 'grad_norm': 2.2009663581848145, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}\n"," 93% 3500/3750 [22:13<01:32,  2.69it/s][INFO|trainer.py:3944] 2025-02-24 15:15:20,137 >> Saving model checkpoint to Google_T5/checkpoint-3500\n","[INFO|configuration_utils.py:423] 2025-02-24 15:15:20,139 >> Configuration saved in Google_T5/checkpoint-3500/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 15:15:20,139 >> Configuration saved in Google_T5/checkpoint-3500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 15:15:22,168 >> Model weights saved in Google_T5/checkpoint-3500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 15:15:22,171 >> tokenizer config file saved in Google_T5/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 15:15:22,171 >> Special tokens file saved in Google_T5/checkpoint-3500/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:176] 2025-02-24 15:15:22,173 >> Copy vocab file to Google_T5/checkpoint-3500/spiece.model\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 3750/3750 [23:52<00:00,  2.83it/s][INFO|trainer.py:3944] 2025-02-24 15:16:58,947 >> Saving model checkpoint to Google_T5/checkpoint-3750\n","[INFO|configuration_utils.py:423] 2025-02-24 15:16:58,949 >> Configuration saved in Google_T5/checkpoint-3750/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 15:16:58,950 >> Configuration saved in Google_T5/checkpoint-3750/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 15:17:00,907 >> Model weights saved in Google_T5/checkpoint-3750/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 15:17:00,910 >> tokenizer config file saved in Google_T5/checkpoint-3750/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 15:17:00,910 >> Special tokens file saved in Google_T5/checkpoint-3750/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:176] 2025-02-24 15:17:00,911 >> Copy vocab file to Google_T5/checkpoint-3750/spiece.model\n","[INFO|trainer.py:2659] 2025-02-24 15:17:04,661 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 1439.2825, 'train_samples_per_second': 20.844, 'train_steps_per_second': 2.605, 'train_loss': 3.1359600748697916, 'epoch': 3.0}\n","100% 3750/3750 [23:58<00:00,  2.61it/s]\n","[INFO|trainer.py:3944] 2025-02-24 15:17:04,669 >> Saving model checkpoint to Google_T5\n","[INFO|configuration_utils.py:423] 2025-02-24 15:17:04,670 >> Configuration saved in Google_T5/config.json\n","[INFO|configuration_utils.py:909] 2025-02-24 15:17:04,671 >> Configuration saved in Google_T5/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-02-24 15:17:08,789 >> Model weights saved in Google_T5/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-02-24 15:17:08,793 >> tokenizer config file saved in Google_T5/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-02-24 15:17:08,794 >> Special tokens file saved in Google_T5/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:176] 2025-02-24 15:17:08,795 >> Copy vocab file to Google_T5/spiece.model\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  1332858GF\n","  train_loss               =      3.136\n","  train_runtime            = 0:23:59.28\n","  train_samples            =      10000\n","  train_samples_per_second =     20.844\n","  train_steps_per_second   =      2.605\n","02/24/2025 15:17:08 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-02-24 15:17:08,808 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-02-24 15:17:08,808 >>   Num examples = 3000\n","[INFO|trainer.py:4265] 2025-02-24 15:17:08,808 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 375/375 [28:15<00:00,  4.52s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     0.9196\n","  eval_gen_len            =    93.2893\n","  eval_loss               =      2.628\n","  eval_runtime            = 0:28:20.04\n","  eval_samples            =       3000\n","  eval_samples_per_second =      1.765\n","  eval_steps_per_second   =      0.221\n","02/24/2025 15:45:28 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-02-24 15:45:28,863 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-02-24 15:45:28,863 >>   Num examples = 1000\n","[INFO|trainer.py:4265] 2025-02-24 15:45:28,863 >>   Batch size = 8\n","100% 125/125 [09:09<00:00,  4.40s/it]\n","***** predict metrics *****\n","  predict_bleu               =     0.9952\n","  predict_gen_len            =     88.552\n","  predict_loss               =     2.5112\n","  predict_runtime            = 0:09:13.44\n","  predict_samples            =       1000\n","  predict_samples_per_second =      1.807\n","  predict_steps_per_second   =      0.226\n","[INFO|modelcard.py:449] 2025-02-24 15:54:42,503 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 0.9196}]}\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Y_bKVr5FbQQp","executionInfo":{"status":"ok","timestamp":1740412485042,"user_tz":-120,"elapsed":26,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":5,"outputs":[]}]}