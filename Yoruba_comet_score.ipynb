{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ce83a02-7f08-4ebe-8208-aa7e0a181ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 7796.10it/s]\n",
      "Encoder model frozen.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|██████████| 64/64 [00:23<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Scored: en-yo_comet_score_africomet_base_32000_ssa.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import jsonlines\n",
    "\n",
    "base_dir = \"/home/emmanuelka\"\n",
    "\n",
    "# Helper: Extract source and reference from FLORES json\n",
    "def extract_source_and_reference(lang_pair, flores_base=f\"{base_dir}/AIMS-NLP-Project/data/flores\"):\n",
    "    src, tgt = lang_pair.split(\"-\")\n",
    "    flores_path = Path(flores_base) / lang_pair / \"devtest.json\"\n",
    "    src_file = flores_path.parent / f\"source_{src}.txt\"\n",
    "    ref_file = flores_path.parent / f\"reference_{tgt}.txt\"\n",
    "\n",
    "    if not src_file.exists() or not ref_file.exists():\n",
    "        with jsonlines.open(flores_path) as reader:\n",
    "            src_lines, tgt_lines = [], []\n",
    "            for obj in reader:\n",
    "                trans = obj[\"translation\"]\n",
    "                src_lines.append(trans[src])\n",
    "                tgt_lines.append(trans[tgt])\n",
    "\n",
    "        with open(src_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(src_lines))\n",
    "        with open(ref_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(tgt_lines))\n",
    "\n",
    "    return str(src_file), str(ref_file)\n",
    "\n",
    "# Evaluate generated_predictions.txt using COMET-QE\n",
    "def evaluate_generation(gen_path, lang_pair, run_type, run_size=None, suffix=None):\n",
    "    src_file, ref_file = extract_source_and_reference(lang_pair)\n",
    "    lang_output_dir = Path(f\"{base_dir}/AIMS-NLP-Project/Results\") / lang_pair\n",
    "    lang_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Construct suffix string\n",
    "    suffix_str = f\"_{suffix}\" if suffix else \"\"\n",
    "\n",
    "    # Build output filename\n",
    "    if run_type == \"baseline\":\n",
    "        output_score_path = lang_output_dir / f\"{lang_pair}_comet_score_baseline{suffix_str}.txt\"\n",
    "    else:\n",
    "        output_score_path = lang_output_dir / f\"{lang_pair}_comet_score_{run_type}_{run_size}{suffix_str}.txt\"\n",
    "\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"comet-score\",\n",
    "            \"-s\", src_file,\n",
    "            \"-t\", gen_path,\n",
    "            \"-r\", ref_file,\n",
    "            \"--model\", \"McGill-NLP/ssa-comet-mtl\"\n",
    "        ],\n",
    "        stdout=open(output_score_path, \"w\")\n",
    "    )\n",
    "    print(f\"✔ Scored: {output_score_path.name}\")\n",
    "\n",
    "# Walk through M2M-100 outputs and evaluate\n",
    "base_dir2 = f\"{base_dir}/M2M-100\"\n",
    "valid_run_types = [\"africomet\", \"africomet_base\", \"random\"]\n",
    "valid_run_sizes = [\"1000\", \"2000\", \"4000\", \"8000\", \"16000\", \"32000\"]\n",
    "\n",
    "# Handle baseline runs\n",
    "baseline_dir = os.path.join(base_dir2, \"baseline\")\n",
    "if os.path.isdir(baseline_dir):\n",
    "    for lang_pair in os.listdir(baseline_dir):\n",
    "        gen_path = os.path.join(baseline_dir, lang_pair, \"generated_predictions.txt\")\n",
    "        if os.path.isfile(gen_path):\n",
    "            evaluate_generation(gen_path, lang_pair, run_type=\"baseline\", suffix=\"ssa\")\n",
    "\n",
    "# Handle all other experimental runs\n",
    "for lang_pair in os.listdir(base_dir2):\n",
    "    lang_path = os.path.join(base_dir2, lang_pair)\n",
    "    if not os.path.isdir(lang_path) or lang_pair == \"baseline\":\n",
    "        continue\n",
    "\n",
    "    for run_folder in os.listdir(lang_path):\n",
    "        run_path = os.path.join(lang_path, run_folder)\n",
    "        if not os.path.isdir(run_path):\n",
    "            continue\n",
    "\n",
    "        parts = run_folder.split(\"_\")\n",
    "        if len(parts) < 3:\n",
    "            continue  # Need at least run_type, size, and suffix\n",
    "        \n",
    "        suffix = parts[-1]                   # e.g., 'ssa'\n",
    "        run_size = parts[-2]                 # e.g., '1000'\n",
    "        run_type = \"_\".join(parts[:-2])     # everything else (e.g., 'africomet_base')\n",
    "\n",
    "        if run_type in valid_run_types and run_size in valid_run_sizes:\n",
    "            gen_path = os.path.join(run_path, \"generated_predictions.txt\")\n",
    "            if os.path.isfile(gen_path):\n",
    "                evaluate_generation(gen_path, lang_pair, run_type=run_type, run_size=run_size, suffix=suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a423ba5-4acf-48fb-82e8-914a50ff2746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9143502-e35c-410a-bb55-c84bfaf63f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Copied: en-yo_all_results_africomet_base_32000_ssa.json\n",
      "✔ Copied: en-yo_generated_predictions_africomet_base_32000_ssa.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# === Configuration ===\n",
    "base_dir = \"/home/emmanuelka/M2M-100\"\n",
    "results_dir = \"/home/emmanuelka/AIMS-NLP-Project/Results\"\n",
    "generations_dir = \"/home/emmanuelka/AIMS-NLP-Project/generations\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(generations_dir, exist_ok=True)\n",
    "\n",
    "# Valid run types and sizes\n",
    "valid_run_types = [\"africomet\", \"africomet_base\", \"random\"]\n",
    "valid_run_sizes = [\"1000\", \"2000\", \"4000\", \"8000\", \"16000\", \"32000\"]\n",
    "\n",
    "# === Helper Function ===\n",
    "def extract_run_parts(folder_name):\n",
    "    parts = run_folder.split(\"_\")\n",
    "    if len(parts) < 3:\n",
    "        return None, None, None  # Need at least run_type, size, and suffix\n",
    "    \n",
    "    suffix = parts[-1]                   # e.g., 'ssa'\n",
    "    run_size = parts[-2]                 # e.g., '1000'\n",
    "    run_type = \"_\".join(parts[:-2])     # everything else (e.g., 'africomet_base')\n",
    "    # parts = folder_name.split(\"_\")\n",
    "    # if len(parts) < 2:\n",
    "    #     return None, None, None\n",
    "    # run_type = parts[0]\n",
    "    # run_size = parts[1]\n",
    "    # suffix = parts[2] if len(parts) > 2 else None\n",
    "    return run_type, run_size, suffix\n",
    "\n",
    "# === Handle baseline ===\n",
    "baseline_dir = os.path.join(base_dir, \"baseline\")\n",
    "if os.path.isdir(baseline_dir):\n",
    "    for lang_pair in os.listdir(baseline_dir):\n",
    "        lang_path = os.path.join(baseline_dir, lang_pair)\n",
    "\n",
    "        # === Handle all_results.json ===\n",
    "        all_results = os.path.join(lang_path, \"all_results.json\")\n",
    "        if os.path.isfile(all_results):\n",
    "            lang_output_dir = os.path.join(results_dir, lang_pair)\n",
    "            os.makedirs(lang_output_dir, exist_ok=True)\n",
    "            new_name = f\"{lang_pair}_all_results_baseline_ssa.json\"\n",
    "            shutil.copyfile(all_results, os.path.join(lang_output_dir, new_name))\n",
    "            print(f\"✔ Copied: {new_name}\")\n",
    "\n",
    "        # === Handle generated_predictions.txt ===\n",
    "        gen_path = os.path.join(lang_path, \"generated_predictions.txt\")\n",
    "        if os.path.isfile(gen_path):\n",
    "            lang_gen_dir = os.path.join(generations_dir, lang_pair)\n",
    "            os.makedirs(lang_gen_dir, exist_ok=True)\n",
    "            new_name = f\"{lang_pair}_generated_predictions_baseline_ssa.txt\"\n",
    "            shutil.copyfile(gen_path, os.path.join(lang_gen_dir, new_name))\n",
    "            print(f\"✔ Copied: {new_name}\")\n",
    "\n",
    "# === Handle experimental runs ===\n",
    "for lang_pair in os.listdir(base_dir):\n",
    "    lang_path = os.path.join(base_dir, lang_pair)\n",
    "    if not os.path.isdir(lang_path) or lang_pair == \"baseline\":\n",
    "        continue\n",
    "\n",
    "    for run_folder in os.listdir(lang_path):\n",
    "        run_path = os.path.join(lang_path, run_folder)\n",
    "        if not os.path.isdir(run_path):\n",
    "            continue\n",
    "\n",
    "        run_type, run_size, suffix = extract_run_parts(run_folder)\n",
    "        if not run_type or not run_size:\n",
    "            continue\n",
    "\n",
    "        if run_type in valid_run_types and run_size in valid_run_sizes:\n",
    "            suffix_str = f\"_{suffix}\" if suffix else \"\"\n",
    "\n",
    "            # === Copy all_results.json ===\n",
    "            all_results = os.path.join(run_path, \"all_results.json\")\n",
    "            if os.path.isfile(all_results):\n",
    "                lang_output_dir = os.path.join(results_dir, lang_pair)\n",
    "                os.makedirs(lang_output_dir, exist_ok=True)\n",
    "                new_name = f\"{lang_pair}_all_results_{run_type}_{run_size}{suffix_str}.json\"\n",
    "                shutil.copyfile(all_results, os.path.join(lang_output_dir, new_name))\n",
    "                print(f\"✔ Copied: {new_name}\")\n",
    "\n",
    "            # === Copy generated_predictions.txt ===\n",
    "            gen_path = os.path.join(run_path, \"generated_predictions.txt\")\n",
    "            if os.path.isfile(gen_path):\n",
    "                lang_gen_dir = os.path.join(generations_dir, lang_pair)\n",
    "                os.makedirs(lang_gen_dir, exist_ok=True)\n",
    "                new_name = f\"{lang_pair}_generated_predictions_{run_type}_{run_size}{suffix_str}.txt\"\n",
    "                shutil.copyfile(gen_path, os.path.join(lang_gen_dir, new_name))\n",
    "                print(f\"✔ Copied: {new_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c05c853-4f18-4ccb-955e-a958b5cb45b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: en-yo_comet_score_africomet_base_32000_ssa.txt\n",
      "✔ Renamed: en-yo_comet_score_africomet_base_32000_ssa.txt → en-yo_comet_score_africomet_base_32000_ssaMTL.txt\n",
      "Found: en-yo_all_results_africomet_base_32000_ssa.json\n",
      "✔ Renamed: en-yo_all_results_africomet_base_32000_ssa.json → en-yo_all_results_africomet_base_32000_ssaMTL.json\n",
      "Found: en-yo_generated_predictions_africomet_base_32000_ssa.txt\n",
      "✔ Renamed: en-yo_generated_predictions_africomet_base_32000_ssa.txt → en-yo_generated_predictions_africomet_base_32000_ssaMTL.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "\n",
    "# Set your base directory\n",
    "base_dir = \"/home/emmanuelka/AIMS-NLP-Project/Results/en-yo\"\n",
    "gen_dir = \"/home/emmanuelka/AIMS-NLP-Project/generations/en-yo\"\n",
    "\n",
    "# Traverse recursively\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for filename in files:\n",
    "\n",
    "        # Case 1: new_africomet_base → africomet_base + _ssa\n",
    "        if fnmatch.fnmatch(filename, \"*new_africomet_base_*.json\"):\n",
    "            print(f\"Found: {filename}\")\n",
    "            new_filename = filename.replace(\"new_africomet_base_\", \"africomet_base_\") \\\n",
    "                                   .replace(\".json\", \"_ssa.json\")\n",
    "            old_path = os.path.join(root, filename)\n",
    "            new_path = os.path.join(root, new_filename)\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"✔ Renamed: {filename} → {new_filename}\")\n",
    "\n",
    "        # Case 2: africomet_new → africomet + _ssaQE\n",
    "        elif fnmatch.fnmatch(filename, \"*africomet_new_*.json\"):\n",
    "            print(f\"Found: {filename}\")\n",
    "            new_filename = filename.replace(\"africomet_new_\", \"africomet_\") \\\n",
    "                                   .replace(\".json\", \"_ssaQE.json\")\n",
    "            old_path = os.path.join(root, filename)\n",
    "            new_path = os.path.join(root, new_filename)\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"✔ Renamed: {filename} → {new_filename}\")\n",
    "\n",
    "        # Case 3: anything ending in _ssa.json or _ssa.txt → replace _ssa. with _ssaMTL.\n",
    "        elif \"_ssa.\" in filename:\n",
    "            print(f\"Found: {filename}\")\n",
    "            new_filename = filename.replace(\"_ssa.\", \"_ssaMTL.\")\n",
    "            old_path = os.path.join(root, filename)\n",
    "            new_path = os.path.join(root, new_filename)\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"✔ Renamed: {filename} → {new_filename}\")\n",
    "\n",
    "# Traverse recursively\n",
    "for root, dirs, files in os.walk(gen_dir):\n",
    "    for filename in files:\n",
    "\n",
    "        # Case 1: new_africomet_base → africomet_base + _ssa\n",
    "        if fnmatch.fnmatch(filename, \"*new_africomet_base_*.json\"):\n",
    "            print(f\"Found: {filename}\")\n",
    "            new_filename = filename.replace(\"new_africomet_base_\", \"africomet_base_\") \\\n",
    "                                   .replace(\".json\", \"_ssa.json\")\n",
    "            old_path = os.path.join(root, filename)\n",
    "            new_path = os.path.join(root, new_filename)\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"✔ Renamed: {filename} → {new_filename}\")\n",
    "\n",
    "        # Case 2: africomet_new → africomet + _ssaQE\n",
    "        elif fnmatch.fnmatch(filename, \"*africomet_new_*.json\"):\n",
    "            print(f\"Found: {filename}\")\n",
    "            new_filename = filename.replace(\"africomet_new_\", \"africomet_\") \\\n",
    "                                   .replace(\".json\", \"_ssaQE.json\")\n",
    "            old_path = os.path.join(root, filename)\n",
    "            new_path = os.path.join(root, new_filename)\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"✔ Renamed: {filename} → {new_filename}\")\n",
    "\n",
    "        # Case 3: anything ending in _ssa.json or _ssa.txt → replace _ssa. with _ssaMTL.\n",
    "        elif \"_ssa.\" in filename:\n",
    "            print(f\"Found: {filename}\")\n",
    "            new_filename = filename.replace(\"_ssa.\", \"_ssaMTL.\")\n",
    "            old_path = os.path.join(root, filename)\n",
    "            new_path = os.path.join(root, new_filename)\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"✔ Renamed: {filename} → {new_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7653da-2edd-4e6f-8485-e71984ab3647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e0df3-d129-4dd9-8af3-949c4bfd0a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
