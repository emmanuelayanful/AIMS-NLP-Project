{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOcVl2UqqAso7dhijjQWSZ7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import wandb\n","import os"],"metadata":{"id":"vqsa3QW1weGy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !rm -rf Google_T5"],"metadata":{"id":"6ICCsdcJZpub"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!chmod u+x /content/AIMS-NLP-Project/run_translation.py"],"metadata":{"id":"JEU-1o2gFQkC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Baseline Run"],"metadata":{"id":"qZQLQPZjy2QO"}},{"cell_type":"code","source":["# Log in with your API key\n","wandb.login(key=\"7f13d9fe09d0856f7c12099a27ccda7aa15c8afd\")\n","\n","# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"baseline\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"id":"aggyfHIUR2zp","executionInfo":{"status":"ok","timestamp":1742633941883,"user_tz":-120,"elapsed":12867,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"bf1462d2-74be-4094-d305-ebc5aa0d39a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250322_085854-0njzblna</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna' target=\"_blank\">baseline</a></strong> to <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a7e2df34250>"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/mafand/en-zu/merged.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/baseline \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvM-Zf4jUO81","outputId":"342ba6d4-0c7f-4ffa-f7a4-5ac29cda097e","executionInfo":{"status":"ok","timestamp":1742637345127,"user_tz":-120,"elapsed":1874933,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-22 09:00:54.631515: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 09:00:54.653331: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 09:00:54.660040: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-22 09:00:54.676194: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-22 09:00:55.669976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/22/2025 09:00:58 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/22/2025 09:00:58 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/baseline/runs/Mar22_09-00-57_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/baseline,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/baseline,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-fcf11d994dda434e\n","03/22/2025 09:00:58 - INFO - datasets.builder - Using custom data configuration default-fcf11d994dda434e\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/22/2025 09:00:58 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/22/2025 09:00:58 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 09:00:58 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/22/2025 09:00:58 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 09:00:58 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-22 09:00:58,508 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 09:00:58,510 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-22 09:00:58,595 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 09:00:58,596 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:00:58,596 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:00:58,596 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:00:58,597 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:00:58,597 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:00:58,597 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:00:58,597 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:00:58,597 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-22 09:00:58,597 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 09:00:58,598 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-22 09:00:59,550 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|safetensors_conversion.py:61] 2025-03-22 09:00:59,735 >> Attempting to create safetensors variant\n","[INFO|safetensors_conversion.py:74] 2025-03-22 09:01:00,105 >> Safetensors PR exists\n","[INFO|configuration_utils.py:1140] 2025-03-22 09:01:00,192 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-22 09:01:00,363 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-22 09:01:00,363 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-22 09:01:00,453 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-22 09:01:00,454 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-500813e1b41c1f29.arrow\n","03/22/2025 09:01:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-500813e1b41c1f29.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-24d83da570939502.arrow\n","03/22/2025 09:01:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-24d83da570939502.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c4510aa2c36da9e3.arrow\n","03/22/2025 09:01:04 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c4510aa2c36da9e3.arrow\n","Downloading builder script: 100% 9.01k/9.01k [00:00<00:00, 15.4MB/s]\n","[INFO|trainer.py:2407] 2025-03-22 09:01:14,521 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-22 09:01:14,521 >>   Num examples = 5,737\n","[INFO|trainer.py:2409] 2025-03-22 09:01:14,521 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-22 09:01:14,521 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-22 09:01:14,521 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-22 09:01:14,521 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-22 09:01:14,521 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-22 09:01:14,521 >>   Total optimization steps = 2,154\n","[INFO|trainer.py:2416] 2025-03-22 09:01:14,522 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-22 09:01:14,529 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250322_090114-yu3f7j7b\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/baseline\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/yu3f7j7b\u001b[0m\n","  0% 0/2154 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.6395, 'grad_norm': 6.515448093414307, 'learning_rate': 3.8393686165273915e-05, 'epoch': 0.7}\n"," 23% 500/2154 [07:26<23:44,  1.16it/s][INFO|trainer.py:3944] 2025-03-22 09:08:41,635 >> Saving model checkpoint to M2M-100/baseline/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-22 09:08:41,644 >> Configuration saved in M2M-100/baseline/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 09:08:41,645 >> Configuration saved in M2M-100/baseline/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 09:08:53,122 >> Model weights saved in M2M-100/baseline/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 09:08:53,126 >> tokenizer config file saved in M2M-100/baseline/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 09:08:53,127 >> Special tokens file saved in M2M-100/baseline/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 09:08:53,127 >> added tokens file saved in M2M-100/baseline/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.7846, 'grad_norm': 6.764095783233643, 'learning_rate': 2.678737233054782e-05, 'epoch': 1.39}\n"," 46% 1000/2154 [15:24<15:48,  1.22it/s][INFO|trainer.py:3944] 2025-03-22 09:16:39,965 >> Saving model checkpoint to M2M-100/baseline/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-22 09:16:39,970 >> Configuration saved in M2M-100/baseline/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 09:16:39,971 >> Configuration saved in M2M-100/baseline/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 09:16:51,462 >> Model weights saved in M2M-100/baseline/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 09:16:51,465 >> tokenizer config file saved in M2M-100/baseline/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 09:16:51,466 >> Special tokens file saved in M2M-100/baseline/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 09:16:51,466 >> added tokens file saved in M2M-100/baseline/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3359, 'grad_norm': 6.238084316253662, 'learning_rate': 1.518105849582173e-05, 'epoch': 2.09}\n"," 70% 1500/2154 [23:24<09:20,  1.17it/s][INFO|trainer.py:3944] 2025-03-22 09:24:39,800 >> Saving model checkpoint to M2M-100/baseline/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-22 09:24:39,809 >> Configuration saved in M2M-100/baseline/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 09:24:39,810 >> Configuration saved in M2M-100/baseline/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 09:24:51,286 >> Model weights saved in M2M-100/baseline/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 09:24:51,288 >> tokenizer config file saved in M2M-100/baseline/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 09:24:51,289 >> Special tokens file saved in M2M-100/baseline/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 09:24:51,289 >> added tokens file saved in M2M-100/baseline/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9657, 'grad_norm': 6.731072425842285, 'learning_rate': 3.574744661095636e-06, 'epoch': 2.79}\n"," 93% 2000/2154 [31:22<02:20,  1.10it/s][INFO|trainer.py:3944] 2025-03-22 09:32:38,220 >> Saving model checkpoint to M2M-100/baseline/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-22 09:32:38,225 >> Configuration saved in M2M-100/baseline/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 09:32:38,226 >> Configuration saved in M2M-100/baseline/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 09:32:49,793 >> Model weights saved in M2M-100/baseline/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 09:32:49,797 >> tokenizer config file saved in M2M-100/baseline/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 09:32:49,798 >> Special tokens file saved in M2M-100/baseline/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 09:32:49,798 >> added tokens file saved in M2M-100/baseline/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 2154/2154 [34:09<00:00,  1.51it/s][INFO|trainer.py:3944] 2025-03-22 09:35:24,559 >> Saving model checkpoint to M2M-100/baseline/checkpoint-2154\n","[INFO|configuration_utils.py:423] 2025-03-22 09:35:24,562 >> Configuration saved in M2M-100/baseline/checkpoint-2154/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 09:35:24,563 >> Configuration saved in M2M-100/baseline/checkpoint-2154/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 09:35:36,155 >> Model weights saved in M2M-100/baseline/checkpoint-2154/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 09:35:36,158 >> tokenizer config file saved in M2M-100/baseline/checkpoint-2154/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 09:35:36,159 >> Special tokens file saved in M2M-100/baseline/checkpoint-2154/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 09:35:36,159 >> added tokens file saved in M2M-100/baseline/checkpoint-2154/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-22 09:35:54,195 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2079.6733, 'train_samples_per_second': 8.276, 'train_steps_per_second': 1.036, 'train_loss': 2.626155800141996, 'epoch': 3.0}\n","100% 2154/2154 [34:38<00:00,  1.04it/s]\n","[INFO|trainer.py:3944] 2025-03-22 09:35:54,204 >> Saving model checkpoint to M2M-100/baseline\n","[INFO|configuration_utils.py:423] 2025-03-22 09:35:54,205 >> Configuration saved in M2M-100/baseline/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 09:35:54,206 >> Configuration saved in M2M-100/baseline/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 09:36:05,773 >> Model weights saved in M2M-100/baseline/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 09:36:05,776 >> tokenizer config file saved in M2M-100/baseline/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 09:36:05,776 >> Special tokens file saved in M2M-100/baseline/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 09:36:05,777 >> added tokens file saved in M2M-100/baseline/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  2504202GF\n","  train_loss               =     2.6262\n","  train_runtime            = 0:34:39.67\n","  train_samples            =       5737\n","  train_samples_per_second =      8.276\n","  train_steps_per_second   =      1.036\n","03/22/2025 09:36:05 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-22 09:36:05,916 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-22 09:36:05,916 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-22 09:36:05,916 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:10<00:00,  4.40s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =      7.053\n","  eval_chrf               =    41.9664\n","  eval_gen_len            =    46.8676\n","  eval_loss               =     2.5353\n","  eval_runtime            = 0:09:16.35\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.792\n","  eval_steps_per_second   =      0.225\n","03/22/2025 09:45:22 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-22 09:45:22,277 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-22 09:45:22,277 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-22 09:45:22,278 >>   Batch size = 8\n","100% 127/127 [09:48<00:00,  4.63s/it]\n","***** predict metrics *****\n","  predict_bleu               =     7.2825\n","  predict_chrf               =    41.7196\n","  predict_gen_len            =    48.5593\n","  predict_loss               =     2.5738\n","  predict_runtime            = 0:09:55.51\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.699\n","  predict_steps_per_second   =      0.213\n","[INFO|modelcard.py:449] 2025-03-22 09:55:33,296 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 7.053}]}\n"]}]},{"cell_type":"markdown","source":["# Africomet with top 1000"],"metadata":{"id":"1OEaIczVafcf"}},{"cell_type":"code","source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet-1000\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"Mp6AahGfufdK","executionInfo":{"status":"ok","timestamp":1742637406571,"user_tz":-120,"elapsed":648,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"c1457e4e-1886-4351-af51-e7abef95c143"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a7e2df34250>"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_1000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-1000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"id":"hbNrB5boVIgU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1fe98c73-6580-46b2-ee67-fef61d3771e5","executionInfo":{"status":"ok","timestamp":1742639311479,"user_tz":-120,"elapsed":1353332,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-22 09:56:56.806833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 09:56:56.828987: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 09:56:56.836168: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-22 09:56:56.852748: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-22 09:56:57.914509: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/22/2025 09:57:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/22/2025 09:57:00 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-1000/runs/Mar22_09-57-00_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-1000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-1000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-d22542853ecd2369\n","03/22/2025 09:57:00 - INFO - datasets.builder - Using custom data configuration default-d22542853ecd2369\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/22/2025 09:57:00 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/22/2025 09:57:00 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 09:57:00 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/22/2025 09:57:00 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 09:57:00 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-22 09:57:01,010 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 09:57:01,011 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-22 09:57:01,095 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 09:57:01,096 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:57:01,097 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:57:01,097 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:57:01,097 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:57:01,097 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:57:01,097 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:57:01,097 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:57:01,097 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-22 09:57:01,097 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 09:57:01,098 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-22 09:57:02,147 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-22 09:57:02,217 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-22 09:57:02,254 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-22 09:57:02,327 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-22 09:57:02,327 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-22 09:57:02,411 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-22 09:57:02,412 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-22 09:57:02,688 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e70b427f4acd1a90.arrow\n","03/22/2025 09:57:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e70b427f4acd1a90.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1e1e9cb1ba5e0af7.arrow\n","03/22/2025 09:57:05 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1e1e9cb1ba5e0af7.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b37e93e0dc9f95ce.arrow\n","03/22/2025 09:57:06 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b37e93e0dc9f95ce.arrow\n","[INFO|trainer.py:2407] 2025-03-22 09:57:16,033 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-22 09:57:16,033 >>   Num examples = 1,000\n","[INFO|trainer.py:2409] 2025-03-22 09:57:16,033 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-22 09:57:16,033 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-22 09:57:16,033 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-22 09:57:16,033 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-22 09:57:16,033 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-22 09:57:16,033 >>   Total optimization steps = 375\n","[INFO|trainer.py:2416] 2025-03-22 09:57:16,034 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-22 09:57:16,047 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250322_095716-zqoky06s\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-1000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/zqoky06s\u001b[0m\n","  0% 0/375 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 375/375 [04:32<00:00,  1.41it/s][INFO|trainer.py:3944] 2025-03-22 10:01:49,415 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-22 10:01:49,424 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 10:01:49,425 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 10:02:01,112 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 10:02:01,116 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 10:02:01,116 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 10:02:01,117 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-22 10:02:18,962 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 302.9291, 'train_samples_per_second': 9.903, 'train_steps_per_second': 1.238, 'train_loss': 3.718626627604167, 'epoch': 3.0}\n","100% 375/375 [05:02<00:00,  1.24it/s]\n","[INFO|trainer.py:3944] 2025-03-22 10:02:18,966 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-1000\n","[INFO|configuration_utils.py:423] 2025-03-22 10:02:18,968 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 10:02:18,968 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 10:02:30,512 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 10:02:30,516 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 10:02:30,516 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 10:02:30,517 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =   205627GF\n","  train_loss               =     3.7186\n","  train_runtime            = 0:05:02.92\n","  train_samples            =       1000\n","  train_samples_per_second =      9.903\n","  train_steps_per_second   =      1.238\n","03/22/2025 10:02:30 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-22 10:02:30,660 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-22 10:02:30,660 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-22 10:02:30,661 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [12:24<00:00,  5.96s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     2.1764\n","  eval_chrf               =     20.964\n","  eval_gen_len            =    55.7432\n","  eval_loss               =     3.9585\n","  eval_runtime            = 0:12:31.81\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.326\n","  eval_steps_per_second   =      0.166\n","03/22/2025 10:15:02 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-22 10:15:02,488 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-22 10:15:02,488 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-22 10:15:02,488 >>   Batch size = 8\n","100% 127/127 [13:01<00:00,  6.15s/it]\n","***** predict metrics *****\n","  predict_bleu               =     1.7597\n","  predict_chrf               =    20.5157\n","  predict_gen_len            =       57.0\n","  predict_loss               =     3.9932\n","  predict_runtime            = 0:13:10.13\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.281\n","  predict_steps_per_second   =      0.161\n","[INFO|modelcard.py:449] 2025-03-22 10:28:28,622 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 2.1764}]}\n"]}]},{"cell_type":"code","source":["wandb.init(project=\"machine translation\", name=\"africomet-plus-1000\")"],"metadata":{"id":"hNlChp_8JQJr","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1742639474702,"user_tz":-120,"elapsed":1552,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"8f6ab5e6-85a9-43a7-9853-48d28d0f6ff3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a7e2df34250>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/trainer_1000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-plus-1000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"id":"XCaK_LmtJbGp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"58942fe0-0f64-4d52-a41f-fbb018ef354e","executionInfo":{"status":"ok","timestamp":1742643222135,"user_tz":-120,"elapsed":869476,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-22 10:31:54.836387: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 10:31:54.858128: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 10:31:54.864716: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-22 10:31:54.883951: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-22 10:31:55.950268: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/22/2025 10:31:58 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/22/2025 10:31:58 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-plus-1000/runs/Mar22_10-31-58_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-plus-1000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-plus-1000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-d09fb30739d7f4ef\n","03/22/2025 10:31:58 - INFO - datasets.builder - Using custom data configuration default-d09fb30739d7f4ef\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/22/2025 10:31:58 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/22/2025 10:31:58 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 10:31:58 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/22/2025 10:31:58 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 10:31:58 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-22 10:31:58,960 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 10:31:58,961 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-22 10:31:59,046 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 10:31:59,046 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 10:31:59,047 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 10:31:59,047 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 10:31:59,047 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 10:31:59,047 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 10:31:59,047 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 10:31:59,047 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 10:31:59,047 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-22 10:31:59,048 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 10:31:59,048 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-22 10:31:59,966 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-22 10:32:00,035 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-22 10:32:00,077 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-22 10:32:00,139 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-22 10:32:00,139 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-22 10:32:00,230 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-22 10:32:00,231 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-22 10:32:00,459 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-23e62197603ff7b5.arrow\n","03/22/2025 10:32:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-23e62197603ff7b5.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0d3debe34b3fe7c6.arrow\n","03/22/2025 10:32:02 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0d3debe34b3fe7c6.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7608692f880482db.arrow\n","03/22/2025 10:32:04 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7608692f880482db.arrow\n","[INFO|trainer.py:2407] 2025-03-22 10:32:09,639 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-22 10:32:09,639 >>   Num examples = 6,737\n","[INFO|trainer.py:2409] 2025-03-22 10:32:09,639 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-22 10:32:09,639 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-22 10:32:09,639 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-22 10:32:09,639 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-22 10:32:09,639 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-22 10:32:09,639 >>   Total optimization steps = 2,529\n","[INFO|trainer.py:2416] 2025-03-22 10:32:09,641 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-22 10:32:09,647 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250322_103209-0toqe8lw\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-plus-1000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/0toqe8lw\u001b[0m\n","  0% 0/2529 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.7582, 'grad_norm': 5.092475414276123, 'learning_rate': 4.011466982997232e-05, 'epoch': 0.59}\n"," 20% 500/2529 [07:27<30:42,  1.10it/s][INFO|trainer.py:3944] 2025-03-22 10:39:38,039 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-22 10:39:38,044 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 10:39:38,045 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 10:39:49,478 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 10:39:49,481 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 10:39:49,482 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 10:39:49,482 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.9769, 'grad_norm': 5.819267272949219, 'learning_rate': 3.0229339659944645e-05, 'epoch': 1.19}\n"," 40% 1000/2529 [15:23<22:59,  1.11it/s][INFO|trainer.py:3944] 2025-03-22 10:47:33,653 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-22 10:47:33,656 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 10:47:33,657 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 10:47:45,177 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 10:47:45,182 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 10:47:45,182 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 10:47:45,183 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4669, 'grad_norm': 5.872149467468262, 'learning_rate': 2.0344009489916967e-05, 'epoch': 1.78}\n"," 59% 1500/2529 [23:19<15:47,  1.09it/s][INFO|trainer.py:3944] 2025-03-22 10:55:30,146 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-22 10:55:30,151 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 10:55:30,151 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 10:55:41,735 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 10:55:41,739 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 10:55:41,740 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 10:55:41,740 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.13, 'grad_norm': 6.943439960479736, 'learning_rate': 1.0458679319889285e-05, 'epoch': 2.37}\n"," 79% 2000/2529 [31:16<07:47,  1.13it/s][INFO|trainer.py:3944] 2025-03-22 11:03:26,797 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-22 11:03:26,802 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 11:03:26,803 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 11:03:38,366 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 11:03:38,369 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 11:03:38,370 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 11:03:38,370 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9484, 'grad_norm': 5.667131423950195, 'learning_rate': 5.733491498616055e-07, 'epoch': 2.97}\n"," 99% 2500/2529 [39:12<00:25,  1.14it/s][INFO|trainer.py:3944] 2025-03-22 11:11:22,975 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-22 11:11:22,980 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 11:11:22,980 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 11:11:34,485 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 11:11:34,489 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 11:11:34,490 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 11:11:34,491 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 2529/2529 [40:07<00:00,  1.36it/s][INFO|trainer.py:3944] 2025-03-22 11:12:17,646 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529\n","[INFO|configuration_utils.py:423] 2025-03-22 11:12:17,649 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 11:12:17,650 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 11:12:29,343 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 11:12:29,347 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 11:12:29,348 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 11:12:29,348 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-22 11:12:47,583 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2437.9438, 'train_samples_per_second': 8.29, 'train_steps_per_second': 1.037, 'train_loss': 2.647326598010925, 'epoch': 3.0}\n","100% 2529/2529 [40:36<00:00,  1.04it/s]\n","[INFO|trainer.py:3944] 2025-03-22 11:12:47,591 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000\n","[INFO|configuration_utils.py:423] 2025-03-22 11:12:47,593 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 11:12:47,593 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 11:12:59,040 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 11:12:59,043 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 11:12:59,043 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 11:12:59,044 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  2830543GF\n","  train_loss               =     2.6473\n","  train_runtime            = 0:40:37.94\n","  train_samples            =       6737\n","  train_samples_per_second =       8.29\n","  train_steps_per_second   =      1.037\n","03/22/2025 11:12:59 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-22 11:12:59,190 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-22 11:12:59,190 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-22 11:12:59,190 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:47<00:00,  4.70s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     7.5784\n","  eval_chrf               =    42.7323\n","  eval_gen_len            =    47.5797\n","  eval_loss               =      2.476\n","  eval_runtime            = 0:09:53.81\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.679\n","  eval_steps_per_second   =      0.211\n","03/22/2025 11:22:53 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-22 11:22:53,016 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-22 11:22:53,016 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-22 11:22:53,017 >>   Batch size = 8\n","100% 127/127 [10:22<00:00,  4.90s/it]\n","***** predict metrics *****\n","  predict_bleu               =     6.9985\n","  predict_chrf               =    41.5549\n","  predict_gen_len            =    49.2016\n","  predict_loss               =     2.5135\n","  predict_runtime            = 0:10:30.58\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.605\n","  predict_steps_per_second   =      0.201\n","[INFO|modelcard.py:449] 2025-03-22 11:33:39,797 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 7.5784}]}\n"]}]},{"cell_type":"code","source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet-2000\")"],"metadata":{"id":"oAhoqvCbuk06","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1742643238911,"user_tz":-120,"elapsed":592,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"7ca2308e-1f3c-46dd-f52f-dad773deea17"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a7e2df34250>"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_2000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-2000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"id":"S0Vip1YNtpQn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8bef0194-7dec-47d9-d889-9d1e5bff57d0","executionInfo":{"status":"ok","timestamp":1742645413262,"user_tz":-120,"elapsed":1854023,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-22 11:34:07.492651: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 11:34:07.514254: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 11:34:07.520847: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-22 11:34:07.536833: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-22 11:34:08.551261: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/22/2025 11:34:10 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/22/2025 11:34:10 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-2000/runs/Mar22_11-34-10_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-2000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-2000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-63fa3ea70d279152\n","03/22/2025 11:34:11 - INFO - datasets.builder - Using custom data configuration default-63fa3ea70d279152\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/22/2025 11:34:11 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/22/2025 11:34:11 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 11:34:11 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/22/2025 11:34:11 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 11:34:11 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-22 11:34:11,355 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 11:34:11,357 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-22 11:34:11,440 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 11:34:11,441 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 11:34:11,442 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 11:34:11,442 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 11:34:11,442 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 11:34:11,442 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 11:34:11,442 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 11:34:11,442 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 11:34:11,442 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-22 11:34:11,442 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 11:34:11,443 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-22 11:34:12,314 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-22 11:34:12,378 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-22 11:34:12,427 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-22 11:34:12,483 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-22 11:34:12,483 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-22 11:34:12,569 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-22 11:34:12,569 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-22 11:34:12,891 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-239a0c8168036379.arrow\n","03/22/2025 11:34:13 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-239a0c8168036379.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4e37c8bc3d2bbba6.arrow\n","03/22/2025 11:34:15 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4e37c8bc3d2bbba6.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-283cd450680fa67d.arrow\n","03/22/2025 11:34:16 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-283cd450680fa67d.arrow\n","[INFO|trainer.py:2407] 2025-03-22 11:34:19,466 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-22 11:34:19,466 >>   Num examples = 2,000\n","[INFO|trainer.py:2409] 2025-03-22 11:34:19,466 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-22 11:34:19,466 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-22 11:34:19,466 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-22 11:34:19,466 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-22 11:34:19,466 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-22 11:34:19,466 >>   Total optimization steps = 750\n","[INFO|trainer.py:2416] 2025-03-22 11:34:19,467 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-22 11:34:19,478 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250322_113419-oj48qc4m\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-2000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/oj48qc4m\u001b[0m\n","  0% 0/750 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.0068, 'grad_norm': 9.566374778747559, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 500/750 [06:08<03:02,  1.37it/s][INFO|trainer.py:3944] 2025-03-22 11:40:28,779 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-22 11:40:28,784 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 11:40:28,785 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 11:40:40,273 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 11:40:40,276 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 11:40:40,276 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 11:40:40,277 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 750/750 [09:41<00:00,  1.37it/s][INFO|trainer.py:3944] 2025-03-22 11:44:02,043 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750\n","[INFO|configuration_utils.py:423] 2025-03-22 11:44:02,046 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 11:44:02,046 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 11:44:13,808 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 11:44:13,813 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 11:44:13,813 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 11:44:13,814 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-22 11:44:32,070 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 612.603, 'train_samples_per_second': 9.794, 'train_steps_per_second': 1.224, 'train_loss': 3.6032134602864585, 'epoch': 3.0}\n","100% 750/750 [10:11<00:00,  1.23it/s]\n","[INFO|trainer.py:3944] 2025-03-22 11:44:32,078 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-2000\n","[INFO|configuration_utils.py:423] 2025-03-22 11:44:32,080 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 11:44:32,080 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 11:44:43,632 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 11:44:43,635 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 11:44:43,635 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 11:44:43,636 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =   409031GF\n","  train_loss               =     3.6032\n","  train_runtime            = 0:10:12.60\n","  train_samples            =       2000\n","  train_samples_per_second =      9.794\n","  train_steps_per_second   =      1.224\n","03/22/2025 11:44:43 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-22 11:44:43,776 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-22 11:44:43,776 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-22 11:44:43,776 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [11:53<00:00,  5.71s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     2.8885\n","  eval_chrf               =    24.2956\n","  eval_gen_len            =    54.0822\n","  eval_loss               =     3.7205\n","  eval_runtime            = 0:12:02.26\n","  eval_samples            =        997\n","  eval_samples_per_second =       1.38\n","  eval_steps_per_second   =      0.173\n","03/22/2025 11:56:46 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-22 11:56:46,052 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-22 11:56:46,052 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-22 11:56:46,052 >>   Batch size = 8\n","100% 127/127 [13:00<00:00,  6.14s/it]\n","***** predict metrics *****\n","  predict_bleu               =     2.5867\n","  predict_chrf               =    23.9403\n","  predict_gen_len            =    55.6423\n","  predict_loss               =     3.7725\n","  predict_runtime            = 0:13:08.91\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.283\n","  predict_steps_per_second   =      0.161\n","[INFO|modelcard.py:449] 2025-03-22 12:10:10,690 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 2.8885}]}\n"]}]},{"cell_type":"code","source":["wandb.init(project=\"machine translation\", name=\"africomet-plus-2000\")"],"metadata":{"id":"NRaFN03_pcfD","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1742645413263,"user_tz":-120,"elapsed":215,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"7a20afbb-9ed7-4e98-dfd9-e6525e71490d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a7e2df34250>"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/trainer_2000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-plus-2000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"id":"sEJT63ZLpVl-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5b9e554f-64e5-4bd2-e072-0a25a4676489","executionInfo":{"status":"ok","timestamp":1742649304176,"user_tz":-120,"elapsed":2465846,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-22 12:10:16.914157: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 12:10:16.936412: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 12:10:16.943127: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-22 12:10:16.959268: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-22 12:10:17.957737: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/22/2025 12:10:20 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/22/2025 12:10:20 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-plus-2000/runs/Mar22_12-10-20_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-plus-2000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-plus-2000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-00c1f33ddcf7c443\n","03/22/2025 12:10:20 - INFO - datasets.builder - Using custom data configuration default-00c1f33ddcf7c443\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/22/2025 12:10:20 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/22/2025 12:10:20 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 12:10:20 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/22/2025 12:10:20 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 12:10:20 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-22 12:10:20,738 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 12:10:20,739 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-22 12:10:20,828 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 12:10:20,829 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 12:10:20,829 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 12:10:20,829 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 12:10:20,829 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 12:10:20,829 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 12:10:20,830 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 12:10:20,830 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 12:10:20,830 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-22 12:10:20,830 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 12:10:20,831 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-22 12:10:21,993 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-22 12:10:22,058 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-22 12:10:22,159 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-22 12:10:22,159 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|safetensors_conversion.py:61] 2025-03-22 12:10:22,233 >> Attempting to create safetensors variant\n","[INFO|configuration_utils.py:1095] 2025-03-22 12:10:22,252 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-22 12:10:22,252 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-22 12:10:22,771 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d9db24e4ea92c1e2.arrow\n","03/22/2025 12:10:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d9db24e4ea92c1e2.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9e62119e384f3dba.arrow\n","03/22/2025 12:10:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9e62119e384f3dba.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2488e0046852a6d9.arrow\n","03/22/2025 12:10:26 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2488e0046852a6d9.arrow\n","[INFO|trainer.py:2407] 2025-03-22 12:10:28,846 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-22 12:10:28,846 >>   Num examples = 7,737\n","[INFO|trainer.py:2409] 2025-03-22 12:10:28,846 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-22 12:10:28,846 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-22 12:10:28,846 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-22 12:10:28,846 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-22 12:10:28,846 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-22 12:10:28,846 >>   Total optimization steps = 2,904\n","[INFO|trainer.py:2416] 2025-03-22 12:10:28,847 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-22 12:10:28,854 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250322_121028-xdtr0l5a\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-plus-2000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/xdtr0l5a\u001b[0m\n","  0% 0/2904 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.8378, 'grad_norm': 5.276322841644287, 'learning_rate': 4.139118457300275e-05, 'epoch': 0.52}\n"," 17% 500/2904 [07:21<36:08,  1.11it/s][INFO|trainer.py:3944] 2025-03-22 12:17:50,743 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-22 12:17:50,746 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 12:17:50,747 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 12:17:55,568 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 12:17:55,571 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 12:17:55,572 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 12:17:55,572 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.1627, 'grad_norm': 5.931947231292725, 'learning_rate': 3.278236914600551e-05, 'epoch': 1.03}\n"," 34% 1000/2904 [14:58<28:36,  1.11it/s][INFO|trainer.py:3944] 2025-03-22 12:25:27,745 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-22 12:25:27,747 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 12:25:27,747 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 12:25:32,664 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 12:25:32,667 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 12:25:32,668 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 12:25:32,668 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.5523, 'grad_norm': 5.885161876678467, 'learning_rate': 2.4173553719008264e-05, 'epoch': 1.55}\n"," 52% 1500/2904 [22:33<20:29,  1.14it/s][INFO|trainer.py:3944] 2025-03-22 12:33:03,687 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-22 12:33:03,689 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 12:33:03,690 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 12:33:08,539 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 12:33:08,542 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 12:33:08,543 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 12:33:08,543 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3528, 'grad_norm': 6.115939617156982, 'learning_rate': 1.5564738292011018e-05, 'epoch': 2.07}\n"," 69% 2000/2904 [30:10<13:14,  1.14it/s][INFO|trainer.py:3944] 2025-03-22 12:40:39,945 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-22 12:40:39,947 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 12:40:39,947 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 12:40:44,829 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 12:40:44,832 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 12:40:44,832 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 12:40:44,833 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9936, 'grad_norm': 6.911382675170898, 'learning_rate': 6.955922865013774e-06, 'epoch': 2.58}\n"," 86% 2500/2904 [37:50<05:38,  1.19it/s][INFO|trainer.py:3944] 2025-03-22 12:48:20,187 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-22 12:48:20,188 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 12:48:20,189 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 12:48:25,111 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 12:48:25,114 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 12:48:25,115 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 12:48:25,115 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 2904/2904 [43:58<00:00,  1.40it/s][INFO|trainer.py:3944] 2025-03-22 12:54:27,744 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2904\n","[INFO|configuration_utils.py:423] 2025-03-22 12:54:27,746 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2904/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 12:54:27,747 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2904/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 12:54:32,793 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2904/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 12:54:32,795 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2904/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 12:54:32,796 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2904/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 12:54:32,796 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2904/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-22 12:54:46,617 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2657.7704, 'train_samples_per_second': 8.733, 'train_steps_per_second': 1.093, 'train_loss': 2.66652855991332, 'epoch': 3.0}\n","100% 2904/2904 [44:16<00:00,  1.09it/s]\n","[INFO|trainer.py:3944] 2025-03-22 12:54:46,622 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-2000\n","[INFO|configuration_utils.py:423] 2025-03-22 12:54:46,624 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 12:54:46,625 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 12:54:55,583 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 12:54:55,587 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 12:54:55,588 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 12:54:55,588 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  3124969GF\n","  train_loss               =     2.6665\n","  train_runtime            = 0:44:17.77\n","  train_samples            =       7737\n","  train_samples_per_second =      8.733\n","  train_steps_per_second   =      1.093\n","03/22/2025 12:54:55 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-22 12:54:55,729 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-22 12:54:55,729 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-22 12:54:55,729 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:20<00:00,  4.48s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =      7.606\n","  eval_chrf               =    42.7291\n","  eval_gen_len            =    46.9649\n","  eval_loss               =     2.4326\n","  eval_runtime            = 0:09:26.32\n","  eval_samples            =        997\n","  eval_samples_per_second =       1.76\n","  eval_steps_per_second   =      0.221\n","03/22/2025 13:04:22 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-22 13:04:22,064 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-22 13:04:22,064 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-22 13:04:22,065 >>   Batch size = 8\n","100% 127/127 [10:13<00:00,  4.83s/it]\n","***** predict metrics *****\n","  predict_bleu               =     7.5372\n","  predict_chrf               =    42.2996\n","  predict_gen_len            =     48.833\n","  predict_loss               =     2.4674\n","  predict_runtime            = 0:10:20.33\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.631\n","  predict_steps_per_second   =      0.205\n","[INFO|modelcard.py:449] 2025-03-22 13:14:58,109 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 7.606}]}\n"]}]},{"cell_type":"code","source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet-4000\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0v4YeYRgumqu","executionInfo":{"status":"ok","timestamp":1742649366291,"user_tz":-120,"elapsed":1435,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"aef268cc-af4b-4a8d-c6a0-87c8aa83c6d0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a7e2df34250>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_4000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-4000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"id":"yGUYnc2wtrby","colab":{"base_uri":"https://localhost:8080/"},"outputId":"97b59ec9-0741-4582-8899-f451dd2e5fa0","executionInfo":{"status":"ok","timestamp":1742651990496,"user_tz":-120,"elapsed":315259,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-22 13:16:13.983970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 13:16:14.005927: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 13:16:14.013098: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-22 13:16:14.030972: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-22 13:16:15.362446: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/22/2025 13:16:18 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/22/2025 13:16:18 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-4000/runs/Mar22_13-16-18_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-4000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-4000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-2c18f5caf6d15f80\n","03/22/2025 13:16:18 - INFO - datasets.builder - Using custom data configuration default-2c18f5caf6d15f80\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/22/2025 13:16:18 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/22/2025 13:16:18 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 13:16:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/22/2025 13:16:18 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 13:16:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-22 13:16:18,687 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 13:16:18,688 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-22 13:16:18,894 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 13:16:18,895 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 13:16:18,896 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 13:16:18,896 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 13:16:18,896 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 13:16:18,896 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 13:16:18,896 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 13:16:18,896 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 13:16:18,896 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-22 13:16:18,896 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 13:16:18,897 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-22 13:16:19,894 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-22 13:16:19,970 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-22 13:16:20,006 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-22 13:16:20,075 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-22 13:16:20,075 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-22 13:16:20,163 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-22 13:16:20,164 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-22 13:16:20,400 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b3e5c8b83b5f5d81.arrow\n","03/22/2025 13:16:21 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b3e5c8b83b5f5d81.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d80ea2e702926f3c.arrow\n","03/22/2025 13:16:22 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d80ea2e702926f3c.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-94a06ef0cd1537df.arrow\n","03/22/2025 13:16:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-94a06ef0cd1537df.arrow\n","[INFO|trainer.py:2407] 2025-03-22 13:16:26,887 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-22 13:16:26,887 >>   Num examples = 4,000\n","[INFO|trainer.py:2409] 2025-03-22 13:16:26,887 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-22 13:16:26,887 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-22 13:16:26,887 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-22 13:16:26,887 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-22 13:16:26,887 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-22 13:16:26,887 >>   Total optimization steps = 1,500\n","[INFO|trainer.py:2416] 2025-03-22 13:16:26,888 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-22 13:16:26,899 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250322_131627-yxsftebf\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-4000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/yxsftebf\u001b[0m\n","  0% 0/1500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.3832, 'grad_norm': 6.679620265960693, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"," 33% 500/1500 [06:12<12:41,  1.31it/s][INFO|trainer.py:3944] 2025-03-22 13:22:40,359 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-22 13:22:40,365 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 13:22:40,366 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 13:22:51,898 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 13:22:51,904 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 13:22:51,904 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 13:22:51,905 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.2835, 'grad_norm': 7.958261489868164, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 1000/1500 [12:53<06:44,  1.24it/s][INFO|trainer.py:3944] 2025-03-22 13:29:21,123 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-22 13:29:21,126 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 13:29:21,127 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 13:29:32,646 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 13:29:32,650 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 13:29:32,650 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 13:29:32,650 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.6419, 'grad_norm': 9.801591873168945, 'learning_rate': 0.0, 'epoch': 3.0}\n","100% 1500/1500 [19:34<00:00,  1.37it/s][INFO|trainer.py:3944] 2025-03-22 13:36:02,341 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-22 13:36:02,345 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 13:36:02,346 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 13:36:13,868 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 13:36:13,872 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 13:36:13,873 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 13:36:13,873 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-22 13:36:31,709 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 1204.8211, 'train_samples_per_second': 9.96, 'train_steps_per_second': 1.245, 'train_loss': 3.436231201171875, 'epoch': 3.0}\n","100% 1500/1500 [20:03<00:00,  1.25it/s]\n","[INFO|trainer.py:3944] 2025-03-22 13:36:31,712 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-4000\n","[INFO|configuration_utils.py:423] 2025-03-22 13:36:31,714 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 13:36:31,715 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 13:36:43,184 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 13:36:43,188 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 13:36:43,188 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 13:36:43,189 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =   849267GF\n","  train_loss               =     3.4362\n","  train_runtime            = 0:20:04.82\n","  train_samples            =       4000\n","  train_samples_per_second =       9.96\n","  train_steps_per_second   =      1.245\n","03/22/2025 13:36:43 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-22 13:36:43,327 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-22 13:36:43,327 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-22 13:36:43,328 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [10:49<00:00,  5.20s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     3.7529\n","  eval_chrf               =    28.1439\n","  eval_gen_len            =    49.7332\n","  eval_loss               =     3.3484\n","  eval_runtime            = 0:10:58.37\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.514\n","  eval_steps_per_second   =       0.19\n","03/22/2025 13:47:41 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-22 13:47:41,708 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-22 13:47:41,708 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-22 13:47:41,708 >>   Batch size = 8\n","100% 127/127 [11:42<00:00,  5.53s/it]\n","***** predict metrics *****\n","  predict_bleu               =      3.384\n","  predict_chrf               =    27.3592\n","  predict_gen_len            =    51.6087\n","  predict_loss               =     3.4078\n","  predict_runtime            = 0:11:51.00\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.423\n","  predict_steps_per_second   =      0.179\n","[INFO|modelcard.py:449] 2025-03-22 13:59:48,360 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 3.7529}]}\n"]}]},{"cell_type":"code","source":["wandb.init(project=\"machine translation\", name=\"africomet-plus-4000\")"],"metadata":{"id":"NbZGn4hdAM3V","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1742656189031,"user_tz":-120,"elapsed":534,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"2f954fa1-4b78-4494-c05a-796be1d8f31c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a7e2df34250>"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/trainer_4000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-plus-4000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"id":"pC_AF5g-AVy3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"336fc9ce-e522-4e59-8bd1-33efb7d085e3","executionInfo":{"status":"ok","timestamp":1742660701508,"user_tz":-120,"elapsed":2013339,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-22 15:09:56.838909: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 15:09:56.860287: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 15:09:56.866879: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-22 15:09:56.882884: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-22 15:09:57.914709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/22/2025 15:10:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/22/2025 15:10:00 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-plus-4000/runs/Mar22_15-10-00_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-plus-4000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-plus-4000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-f3c362cd2250520f\n","03/22/2025 15:10:00 - INFO - datasets.builder - Using custom data configuration default-f3c362cd2250520f\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/22/2025 15:10:00 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Generating dataset json (/root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/22/2025 15:10:00 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","03/22/2025 15:10:00 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","Downloading took 0.0 min\n","03/22/2025 15:10:00 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","03/22/2025 15:10:00 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Generating train split\n","03/22/2025 15:10:00 - INFO - datasets.builder - Generating train split\n","Generating train split: 9737 examples [00:00, 207328.27 examples/s]\n","Generating validation split\n","03/22/2025 15:10:00 - INFO - datasets.builder - Generating validation split\n","Generating validation split: 997 examples [00:00, 218047.82 examples/s]\n","Generating test split\n","03/22/2025 15:10:00 - INFO - datasets.builder - Generating test split\n","Generating test split: 1012 examples [00:00, 234756.69 examples/s]\n","Unable to verify splits sizes.\n","03/22/2025 15:10:00 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","03/22/2025 15:10:00 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","[INFO|configuration_utils.py:699] 2025-03-22 15:10:00,804 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 15:10:00,806 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-22 15:10:00,905 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 15:10:00,905 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 15:10:00,906 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 15:10:00,906 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 15:10:00,906 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 15:10:00,906 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 15:10:00,906 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 15:10:00,906 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 15:10:00,906 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-22 15:10:00,907 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 15:10:00,907 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-22 15:10:01,817 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-22 15:10:01,881 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-22 15:10:01,977 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-22 15:10:01,977 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-22 15:10:02,067 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-22 15:10:02,067 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-22 15:10:02,748 >> Attempting to create safetensors variant\n","[INFO|safetensors_conversion.py:74] 2025-03-22 15:10:03,168 >> Safetensors PR exists\n","Running tokenizer on train dataset:   0% 0/9737 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a84d01f0e3c506e2.arrow\n","03/22/2025 15:10:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a84d01f0e3c506e2.arrow\n","Running tokenizer on train dataset: 100% 9737/9737 [00:04<00:00, 2147.16 examples/s]\n","Running tokenizer on validation dataset:   0% 0/997 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-3ba84a83df7f0235.arrow\n","03/22/2025 15:10:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-3ba84a83df7f0235.arrow\n","Running tokenizer on validation dataset: 100% 997/997 [00:00<00:00, 1854.50 examples/s]\n","Running tokenizer on prediction dataset:   0% 0/1012 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-6ca21061951ac471.arrow\n","03/22/2025 15:10:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-6ca21061951ac471.arrow\n","Running tokenizer on prediction dataset: 100% 1012/1012 [00:00<00:00, 1856.60 examples/s]\n","[INFO|trainer.py:2407] 2025-03-22 15:10:15,163 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-22 15:10:15,163 >>   Num examples = 9,737\n","[INFO|trainer.py:2409] 2025-03-22 15:10:15,163 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-22 15:10:15,163 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-22 15:10:15,163 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-22 15:10:15,164 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-22 15:10:15,164 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-22 15:10:15,164 >>   Total optimization steps = 3,654\n","[INFO|trainer.py:2416] 2025-03-22 15:10:15,165 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-22 15:10:15,170 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250322_151015-tpjlmnb0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-plus-4000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/tpjlmnb0\u001b[0m\n","  0% 0/3654 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.949, 'grad_norm': 8.702216148376465, 'learning_rate': 4.315818281335523e-05, 'epoch': 0.41}\n"," 14% 500/3654 [07:09<42:47,  1.23it/s][INFO|trainer.py:3944] 2025-03-22 15:17:26,301 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-22 15:17:26,304 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 15:17:26,305 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 15:17:31,194 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 15:17:31,197 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 15:17:31,197 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 15:17:31,198 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.3469, 'grad_norm': 7.719375133514404, 'learning_rate': 3.6316365626710456e-05, 'epoch': 0.82}\n"," 27% 1000/3654 [14:39<39:48,  1.11it/s][INFO|trainer.py:3944] 2025-03-22 15:24:55,714 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-22 15:24:55,716 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 15:24:55,717 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 15:25:00,711 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 15:25:00,714 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 15:25:00,714 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 15:25:00,715 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.8412, 'grad_norm': 6.0307440757751465, 'learning_rate': 2.947454844006568e-05, 'epoch': 1.23}\n"," 41% 1500/3654 [22:06<28:17,  1.27it/s][INFO|trainer.py:3944] 2025-03-22 15:32:22,783 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-22 15:32:22,784 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 15:32:22,785 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 15:32:27,716 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 15:32:27,720 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 15:32:27,720 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 15:32:27,721 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.5354, 'grad_norm': 5.58447790145874, 'learning_rate': 2.263273125342091e-05, 'epoch': 1.64}\n"," 55% 2000/3654 [29:35<23:13,  1.19it/s][INFO|trainer.py:3944] 2025-03-22 15:39:51,869 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-22 15:39:51,871 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 15:39:51,872 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 15:39:56,733 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 15:39:56,736 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 15:39:56,736 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 15:39:56,737 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3739, 'grad_norm': 5.18902587890625, 'learning_rate': 1.5790914066776137e-05, 'epoch': 2.05}\n"," 68% 2500/3654 [37:04<16:35,  1.16it/s][INFO|trainer.py:3944] 2025-03-22 15:47:21,098 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-22 15:47:21,100 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 15:47:21,101 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 15:47:26,014 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 15:47:26,017 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 15:47:26,018 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 15:47:26,018 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.025, 'grad_norm': 6.271111488342285, 'learning_rate': 8.949096880131364e-06, 'epoch': 2.46}\n"," 82% 3000/3654 [44:34<09:10,  1.19it/s][INFO|trainer.py:3944] 2025-03-22 15:54:51,017 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-22 15:54:51,019 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 15:54:51,020 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 15:54:55,934 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 15:54:55,937 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 15:54:55,938 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 15:54:55,938 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9933, 'grad_norm': 6.618305683135986, 'learning_rate': 2.1072796934865904e-06, 'epoch': 2.87}\n"," 96% 3500/3654 [52:05<02:09,  1.19it/s][INFO|trainer.py:3944] 2025-03-22 16:02:22,253 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3500\n","[INFO|configuration_utils.py:423] 2025-03-22 16:02:22,255 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 16:02:22,256 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 16:02:27,145 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 16:02:27,149 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 16:02:27,150 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 16:02:27,150 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 3654/3654 [54:35<00:00,  1.40it/s][INFO|trainer.py:3944] 2025-03-22 16:04:51,818 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3654\n","[INFO|configuration_utils.py:423] 2025-03-22 16:04:51,819 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3654/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 16:04:51,820 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3654/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 16:04:56,840 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3654/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 16:04:56,843 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3654/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 16:04:56,843 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3654/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 16:04:56,844 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3654/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-22 16:05:10,707 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 3295.5424, 'train_samples_per_second': 8.864, 'train_steps_per_second': 1.109, 'train_loss': 2.6922839435646293, 'epoch': 3.0}\n","100% 3654/3654 [54:54<00:00,  1.11it/s]\n","[INFO|trainer.py:3944] 2025-03-22 16:05:10,716 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000\n","[INFO|configuration_utils.py:423] 2025-03-22 16:05:10,718 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 16:05:10,719 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 16:05:19,653 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 16:05:19,657 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 16:05:19,658 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 16:05:19,658 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  3718749GF\n","  train_loss               =     2.6923\n","  train_runtime            = 0:54:55.54\n","  train_samples            =       9737\n","  train_samples_per_second =      8.864\n","  train_steps_per_second   =      1.109\n","03/22/2025 16:05:19 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-22 16:05:19,820 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-22 16:05:19,821 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-22 16:05:19,821 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:19<00:00,  4.47s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     8.3068\n","  eval_chrf               =    43.7736\n","  eval_gen_len            =    46.2227\n","  eval_loss               =      2.353\n","  eval_runtime            = 0:09:25.60\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.763\n","  eval_steps_per_second   =      0.221\n","03/22/2025 16:14:45 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-22 16:14:45,430 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-22 16:14:45,430 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-22 16:14:45,430 >>   Batch size = 8\n","100% 127/127 [09:50<00:00,  4.65s/it]\n","***** predict metrics *****\n","  predict_bleu               =     7.9085\n","  predict_chrf               =    43.2905\n","  predict_gen_len            =     48.083\n","  predict_loss               =     2.3917\n","  predict_runtime            = 0:09:58.16\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.692\n","  predict_steps_per_second   =      0.212\n","[INFO|modelcard.py:449] 2025-03-22 16:24:58,942 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 8.3068}]}\n"]}]},{"cell_type":"code","source":["# !rm rf wandb"],"metadata":{"id":"vccJW8nKsBGP","executionInfo":{"status":"ok","timestamp":1742660994243,"user_tz":-120,"elapsed":2151,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"51756e7e-072c-4dc1-f494-8de293aa74cc","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove 'rf': No such file or directory\n","rm: cannot remove 'wandb': Is a directory\n"]}]},{"cell_type":"code","source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet-8000\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"hKmXZ3xLuoYh","executionInfo":{"status":"ok","timestamp":1741626214755,"user_tz":-120,"elapsed":351,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"ce91efb5-bca3-48d9-bb5e-be0800661186"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_8000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-8000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TZSOQFxvttWv","outputId":"747535e8-c367-45a9-d48d-fc14c8439854","executionInfo":{"status":"ok","timestamp":1741634746236,"user_tz":-120,"elapsed":59743,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-10 17:03:38.550278: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-10 17:03:38.583109: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-10 17:03:38.590670: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-10 17:03:38.609732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-10 17:03:39.832340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/10/2025 17:03:42 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/10/2025 17:03:42 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-8000/runs/Mar10_17-03-42_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-8000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-8000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-5bc63a9eaf840ac0\n","03/10/2025 17:03:43 - INFO - datasets.builder - Using custom data configuration default-5bc63a9eaf840ac0\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/10/2025 17:03:43 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/10/2025 17:03:43 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 17:03:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/10/2025 17:03:43 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 17:03:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-10 17:03:43,240 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 17:03:43,242 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-10 17:03:43,395 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 17:03:43,396 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,397 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,397 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,397 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,398 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,398 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,398 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,398 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-10 17:03:43,398 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 17:03:43,399 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-10 17:03:44,414 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-10 17:03:44,491 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-10 17:03:44,531 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-10 17:03:44,617 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-10 17:03:44,617 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-10 17:03:44,715 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-10 17:03:44,715 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-10 17:03:44,904 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-805d80dbaf682edc.arrow\n","03/10/2025 17:03:46 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-805d80dbaf682edc.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-8d35743ea3961495.arrow\n","03/10/2025 17:03:47 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-8d35743ea3961495.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a687ca1a7df8988f.arrow\n","03/10/2025 17:03:49 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a687ca1a7df8988f.arrow\n","[INFO|trainer.py:2407] 2025-03-10 17:03:51,848 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-10 17:03:51,848 >>   Num examples = 8,000\n","[INFO|trainer.py:2409] 2025-03-10 17:03:51,848 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-10 17:03:51,848 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-10 17:03:51,848 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-10 17:03:51,848 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-10 17:03:51,848 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-10 17:03:51,848 >>   Total optimization steps = 3,000\n","[INFO|trainer.py:2416] 2025-03-10 17:03:51,849 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-10 17:03:51,855 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_170351-7zajcprb\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-8000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/7zajcprb\u001b[0m\n","  0% 0/3000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.3678, 'grad_norm': 8.133068084716797, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n"," 17% 500/3000 [06:18<31:10,  1.34it/s][INFO|trainer.py:3944] 2025-03-10 17:10:11,245 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-10 17:10:11,248 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 17:10:11,249 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 17:10:16,754 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:10:16,757 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:10:16,758 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:10:16,758 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.8433, 'grad_norm': 8.525639533996582, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"," 33% 1000/3000 [12:57<25:38,  1.30it/s][INFO|trainer.py:3944] 2025-03-10 17:16:50,074 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-10 17:16:50,076 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 17:16:50,077 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 17:16:55,629 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:16:55,632 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:16:55,632 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:16:55,633 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.0452, 'grad_norm': 7.349609851837158, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n"," 50% 1500/3000 [19:33<18:57,  1.32it/s][INFO|trainer.py:3944] 2025-03-10 17:23:26,116 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-10 17:23:26,118 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 17:23:26,119 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 17:23:31,635 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:23:31,638 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:23:31,639 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:23:31,639 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.95, 'grad_norm': 8.351008415222168, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 2000/3000 [26:11<12:11,  1.37it/s][INFO|trainer.py:3944] 2025-03-10 17:30:03,819 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-10 17:30:03,821 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 17:30:03,822 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 17:30:09,382 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:30:09,386 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:30:09,387 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:30:09,388 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4116, 'grad_norm': 9.160988807678223, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"," 83% 2500/3000 [32:46<06:13,  1.34it/s][INFO|trainer.py:3944] 2025-03-10 17:36:39,364 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-10 17:36:39,366 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 17:36:39,367 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 17:36:44,901 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:36:44,908 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:36:44,909 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:36:44,909 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3545, 'grad_norm': 6.916102886199951, 'learning_rate': 0.0, 'epoch': 3.0}\n","100% 3000/3000 [39:25<00:00,  1.31it/s][INFO|trainer.py:3944] 2025-03-10 17:43:18,656 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-10 17:43:18,658 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 17:43:18,659 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 17:43:24,214 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:43:24,217 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:43:24,217 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:43:24,218 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-10 17:43:37,703 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2385.8542, 'train_samples_per_second': 10.059, 'train_steps_per_second': 1.257, 'train_loss': 3.1620497639973957, 'epoch': 3.0}\n","100% 3000/3000 [39:44<00:00,  1.26it/s]\n","[INFO|trainer.py:3944] 2025-03-10 17:43:37,707 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000\n","[INFO|configuration_utils.py:423] 2025-03-10 17:43:37,709 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 17:43:37,710 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 17:43:46,668 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:43:46,672 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:43:46,672 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:43:46,673 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  1672691GF\n","  train_loss               =      3.162\n","  train_runtime            = 0:39:45.85\n","  train_samples            =       8000\n","  train_samples_per_second =     10.059\n","  train_steps_per_second   =      1.257\n","03/10/2025 17:43:46 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-10 17:43:46,838 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-10 17:43:46,838 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-10 17:43:46,839 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:45<00:00,  4.68s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     5.3524\n","  eval_gen_len            =    46.8586\n","  eval_loss               =     2.9557\n","  eval_runtime            = 0:09:51.81\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.685\n","  eval_steps_per_second   =      0.211\n","03/10/2025 17:53:38 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-10 17:53:38,657 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-10 17:53:38,657 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-10 17:53:38,657 >>   Batch size = 8\n","100% 127/127 [10:52<00:00,  5.14s/it]\n","***** predict metrics *****\n","  predict_bleu               =     4.9433\n","  predict_gen_len            =    49.2668\n","  predict_loss               =     3.0128\n","  predict_runtime            = 0:10:58.87\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.536\n","  predict_steps_per_second   =      0.193\n","[INFO|modelcard.py:449] 2025-03-10 18:04:55,614 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 5.3524}]}\n"]}]},{"cell_type":"code","source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet-16000\")"],"metadata":{"id":"50vt5hMgup68","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1741634746530,"user_tz":-120,"elapsed":37197,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"63b5de95-854a-42da-ca9c-2117e0317b58"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_16000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-16000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"id":"mlfAMTxMtvOi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5aa226a1-9582-4fb7-80bd-d80b5cf5b37f","executionInfo":{"status":"ok","timestamp":1741635930446,"user_tz":-120,"elapsed":1221111,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-10 18:05:02.627668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-10 18:05:02.653295: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-10 18:05:02.660907: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-10 18:05:02.679637: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-10 18:05:03.876059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/10/2025 18:05:06 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/10/2025 18:05:06 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-16000/runs/Mar10_18-05-06_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-16000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-16000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-8dd97eb64194ed63\n","03/10/2025 18:05:06 - INFO - datasets.builder - Using custom data configuration default-8dd97eb64194ed63\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/10/2025 18:05:06 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/10/2025 18:05:06 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 18:05:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/10/2025 18:05:06 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 18:05:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-10 18:05:07,170 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 18:05:07,172 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-10 18:05:07,258 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 18:05:07,259 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,260 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,260 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,260 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,260 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,260 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,261 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,261 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-10 18:05:07,261 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 18:05:07,262 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-10 18:05:08,314 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-10 18:05:08,391 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-10 18:05:08,426 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-10 18:05:08,524 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-10 18:05:08,524 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-10 18:05:08,671 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-10 18:05:08,671 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4f73fd23cbbc2044.arrow\n","03/10/2025 18:05:10 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4f73fd23cbbc2044.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-61f573cc25eca874.arrow\n","03/10/2025 18:05:11 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-61f573cc25eca874.arrow\n","[INFO|safetensors_conversion.py:74] 2025-03-10 18:05:12,713 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-29c03a83c2242779.arrow\n","03/10/2025 18:05:13 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-29c03a83c2242779.arrow\n","[INFO|trainer.py:2407] 2025-03-10 18:05:15,747 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-10 18:05:15,747 >>   Num examples = 16,000\n","[INFO|trainer.py:2409] 2025-03-10 18:05:15,747 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-10 18:05:15,747 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-10 18:05:15,747 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-10 18:05:15,747 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-10 18:05:15,747 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-10 18:05:15,747 >>   Total optimization steps = 6,000\n","[INFO|trainer.py:2416] 2025-03-10 18:05:15,749 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-10 18:05:15,755 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_180515-53flw2ac\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-16000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/53flw2ac\u001b[0m\n","  0% 0/6000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.4093, 'grad_norm': 8.515572547912598, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}\n","  8% 500/6000 [06:17<1:07:03,  1.37it/s][INFO|trainer.py:3944] 2025-03-10 18:11:34,382 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-10 18:11:34,386 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 18:11:34,387 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 18:11:39,789 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:11:39,792 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:11:39,792 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:11:39,793 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.8951, 'grad_norm': 7.606267929077148, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n"," 17% 1000/6000 [12:55<1:05:41,  1.27it/s][INFO|trainer.py:3944] 2025-03-10 18:18:12,301 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-10 18:18:12,303 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 18:18:12,304 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 18:18:17,852 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:18:17,856 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:18:17,857 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:18:17,857 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.5859, 'grad_norm': 7.814512252807617, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}\n"," 25% 1500/6000 [19:31<56:49,  1.32it/s][INFO|trainer.py:3944] 2025-03-10 18:24:47,824 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-10 18:24:47,826 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 18:24:47,827 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 18:24:53,373 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:24:53,377 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:24:53,377 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:24:53,378 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.4074, 'grad_norm': 7.271307468414307, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"," 33% 2000/6000 [26:08<50:21,  1.32it/s][INFO|trainer.py:3944] 2025-03-10 18:31:25,350 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-10 18:31:25,352 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 18:31:25,353 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 18:31:30,850 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:31:30,853 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:31:30,853 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:31:30,854 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.8236, 'grad_norm': 7.814577102661133, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}\n"," 42% 2500/6000 [32:44<43:31,  1.34it/s][INFO|trainer.py:3944] 2025-03-10 18:38:00,848 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-10 18:38:00,850 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 18:38:00,851 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 18:38:06,401 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:38:06,404 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:38:06,405 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:38:06,406 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.758, 'grad_norm': 5.690384864807129, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n"," 50% 3000/6000 [39:21<38:08,  1.31it/s][INFO|trainer.py:3944] 2025-03-10 18:44:37,838 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-10 18:44:37,840 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 18:44:37,841 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 18:44:43,390 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:44:43,394 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:44:43,394 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:44:43,394 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.7309, 'grad_norm': 8.432010650634766, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}\n"," 58% 3500/6000 [45:58<31:25,  1.33it/s][INFO|trainer.py:3944] 2025-03-10 18:51:15,660 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500\n","[INFO|configuration_utils.py:423] 2025-03-10 18:51:15,663 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 18:51:15,663 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 18:51:21,209 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:51:21,213 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:51:21,213 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:51:21,214 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.6624, 'grad_norm': 6.384344577789307, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 4000/6000 [52:34<26:04,  1.28it/s][INFO|trainer.py:3944] 2025-03-10 18:57:51,455 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000\n","[INFO|configuration_utils.py:423] 2025-03-10 18:57:51,457 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 18:57:51,458 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 18:57:56,944 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:57:56,947 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:57:56,947 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:57:56,948 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.2164, 'grad_norm': 6.924351692199707, 'learning_rate': 1.25e-05, 'epoch': 2.25}\n"," 75% 4500/6000 [59:11<18:50,  1.33it/s][INFO|trainer.py:3944] 2025-03-10 19:04:28,469 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500\n","[INFO|configuration_utils.py:423] 2025-03-10 19:04:28,471 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 19:04:28,472 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 19:04:34,039 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 19:04:34,042 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 19:04:34,043 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 19:04:34,043 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.2, 'grad_norm': 6.821500778198242, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"," 83% 5000/6000 [1:05:46<12:19,  1.35it/s][INFO|trainer.py:3944] 2025-03-10 19:11:03,349 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000\n","[INFO|configuration_utils.py:423] 2025-03-10 19:11:03,350 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 19:11:03,351 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 19:11:08,451 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 19:11:08,454 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 19:11:08,455 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 19:11:08,455 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.1951, 'grad_norm': 6.860134124755859, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}\n"," 92% 5500/6000 [1:12:19<06:01,  1.38it/s][INFO|trainer.py:3944] 2025-03-10 19:17:36,626 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500\n","[INFO|configuration_utils.py:423] 2025-03-10 19:17:36,628 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 19:17:36,629 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 19:17:41,776 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 19:17:41,779 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 19:17:41,780 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 19:17:41,780 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.1311, 'grad_norm': 6.98347282409668, 'learning_rate': 0.0, 'epoch': 3.0}\n","100% 6000/6000 [1:18:53<00:00,  1.36it/s][INFO|trainer.py:3944] 2025-03-10 19:24:10,314 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000\n","[INFO|configuration_utils.py:423] 2025-03-10 19:24:10,316 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 19:24:10,317 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 19:24:15,588 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 19:24:15,592 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 19:24:15,592 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 19:24:15,593 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-10 19:24:29,306 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 4753.5579, 'train_samples_per_second': 10.098, 'train_steps_per_second': 1.262, 'train_loss': 2.9179393717447915, 'epoch': 3.0}\n","100% 6000/6000 [1:19:12<00:00,  1.26it/s]\n","[INFO|trainer.py:3944] 2025-03-10 19:24:29,311 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000\n","[INFO|configuration_utils.py:423] 2025-03-10 19:24:29,313 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 19:24:29,314 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 19:24:38,219 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 19:24:38,222 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 19:24:38,223 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 19:24:38,223 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  3307792GF\n","  train_loss               =     2.9179\n","  train_runtime            = 1:19:13.55\n","  train_samples            =      16000\n","  train_samples_per_second =     10.098\n","  train_steps_per_second   =      1.262\n","03/10/2025 19:24:38 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-10 19:24:38,375 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-10 19:24:38,375 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-10 19:24:38,375 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:35<00:00,  4.60s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     6.8131\n","  eval_gen_len            =    46.3791\n","  eval_loss               =      2.566\n","  eval_runtime            = 0:09:44.12\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.707\n","  eval_steps_per_second   =      0.214\n","03/10/2025 19:34:22 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-10 19:34:22,502 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-10 19:34:22,503 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-10 19:34:22,503 >>   Batch size = 8\n","100% 127/127 [10:41<00:00,  5.05s/it]\n","***** predict metrics *****\n","  predict_bleu               =     6.3958\n","  predict_gen_len            =    49.4249\n","  predict_loss               =     2.6116\n","  predict_runtime            = 0:10:49.41\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.558\n","  predict_steps_per_second   =      0.196\n","[INFO|modelcard.py:449] 2025-03-10 19:45:27,886 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 6.8131}]}\n"]}]},{"cell_type":"markdown","source":["# Baseline + Africomet"],"metadata":{"id":"mNdDNvlQ9cRn"}},{"cell_type":"code","source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet_plus-1000\")"],"metadata":{"id":"hlcDxJYg9Zzn","colab":{"base_uri":"https://localhost:8080/","height":171},"executionInfo":{"status":"ok","timestamp":1742627425696,"user_tz":-120,"elapsed":10155,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"1993705b-0fc2-440f-db0c-c07f582c4853"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250322_071020-w2kyng73</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/paderborn-university/machine%20translation/runs/w2kyng73' target=\"_blank\">africomet_plus-1000</a></strong> to <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/paderborn-university/machine%20translation/runs/w2kyng73' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation/runs/w2kyng73</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/w2kyng73?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x78e59f5341c0>"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/trainer_1000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-plus-1000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"id":"aIlSRCF9_CpV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e8efa38d-4ebb-431a-c24a-97824d78453a","executionInfo":{"status":"ok","timestamp":1742630948417,"user_tz":-120,"elapsed":852083,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-22 07:11:53.962805: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 07:11:53.983991: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 07:11:53.990794: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-22 07:11:54.006741: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-22 07:11:55.001245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/22/2025 07:12:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/22/2025 07:12:00 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-plus-1000/runs/Mar22_07-12-00_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-plus-1000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-plus-1000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-d09fb30739d7f4ef\n","03/22/2025 07:12:00 - INFO - datasets.builder - Using custom data configuration default-d09fb30739d7f4ef\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/22/2025 07:12:00 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Generating dataset json (/root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/22/2025 07:12:00 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","03/22/2025 07:12:00 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","Downloading took 0.0 min\n","03/22/2025 07:12:00 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","03/22/2025 07:12:00 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Generating train split\n","03/22/2025 07:12:00 - INFO - datasets.builder - Generating train split\n","Generating train split: 6737 examples [00:00, 209154.83 examples/s]\n","Generating validation split\n","03/22/2025 07:12:00 - INFO - datasets.builder - Generating validation split\n","Generating validation split: 997 examples [00:00, 226749.87 examples/s]\n","Generating test split\n","03/22/2025 07:12:00 - INFO - datasets.builder - Generating test split\n","Generating test split: 1012 examples [00:00, 229093.03 examples/s]\n","Unable to verify splits sizes.\n","03/22/2025 07:12:00 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","03/22/2025 07:12:00 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","[INFO|configuration_utils.py:699] 2025-03-22 07:12:00,722 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 07:12:00,724 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-22 07:12:00,812 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 07:12:00,813 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 07:12:00,814 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 07:12:00,814 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 07:12:00,814 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 07:12:00,814 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 07:12:00,815 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 07:12:00,815 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 07:12:00,815 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-22 07:12:00,815 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 07:12:00,816 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-22 07:12:01,808 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|safetensors_conversion.py:61] 2025-03-22 07:12:01,951 >> Attempting to create safetensors variant\n","[INFO|safetensors_conversion.py:74] 2025-03-22 07:12:02,317 >> Safetensors PR exists\n","[INFO|configuration_utils.py:1140] 2025-03-22 07:12:02,377 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-22 07:12:02,559 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-22 07:12:02,559 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-22 07:12:02,648 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-22 07:12:02,648 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","Running tokenizer on train dataset:   0% 0/6737 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-23e62197603ff7b5.arrow\n","03/22/2025 07:12:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-23e62197603ff7b5.arrow\n","Running tokenizer on train dataset: 100% 6737/6737 [00:03<00:00, 1796.46 examples/s]\n","Running tokenizer on validation dataset:   0% 0/997 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0d3debe34b3fe7c6.arrow\n","03/22/2025 07:12:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0d3debe34b3fe7c6.arrow\n","Running tokenizer on validation dataset: 100% 997/997 [00:00<00:00, 1964.47 examples/s]\n","Running tokenizer on prediction dataset:   0% 0/1012 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7608692f880482db.arrow\n","03/22/2025 07:12:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7608692f880482db.arrow\n","Running tokenizer on prediction dataset: 100% 1012/1012 [00:00<00:00, 1856.16 examples/s]\n","[INFO|trainer.py:2407] 2025-03-22 07:12:21,026 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-22 07:12:21,026 >>   Num examples = 6,737\n","[INFO|trainer.py:2409] 2025-03-22 07:12:21,026 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-22 07:12:21,026 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-22 07:12:21,026 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-22 07:12:21,027 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-22 07:12:21,027 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-22 07:12:21,027 >>   Total optimization steps = 2,529\n","[INFO|trainer.py:2416] 2025-03-22 07:12:21,028 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-22 07:12:21,033 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250322_071221-ztsjvqc1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-plus-1000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/ztsjvqc1\u001b[0m\n","  0% 0/2529 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.7562, 'grad_norm': 5.185500144958496, 'learning_rate': 4.011466982997232e-05, 'epoch': 0.59}\n"," 20% 500/2529 [07:00<28:51,  1.17it/s][INFO|trainer.py:3944] 2025-03-22 07:19:22,051 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-22 07:19:22,060 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 07:19:22,061 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 07:19:26,883 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 07:19:26,886 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 07:19:26,886 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 07:19:26,887 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.9712, 'grad_norm': 7.442615509033203, 'learning_rate': 3.0229339659944645e-05, 'epoch': 1.19}\n"," 40% 1000/2529 [14:21<21:57,  1.16it/s][INFO|trainer.py:3944] 2025-03-22 07:26:43,222 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-22 07:26:43,223 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 07:26:43,224 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 07:26:48,131 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 07:26:48,134 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 07:26:48,135 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 07:26:48,135 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4696, 'grad_norm': 5.97845983505249, 'learning_rate': 2.0344009489916967e-05, 'epoch': 1.78}\n"," 59% 1500/2529 [21:44<15:09,  1.13it/s][INFO|trainer.py:3944] 2025-03-22 07:34:06,280 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-22 07:34:06,282 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 07:34:06,283 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 07:34:11,208 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 07:34:11,211 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 07:34:11,212 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 07:34:11,212 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.1316, 'grad_norm': 6.86483907699585, 'learning_rate': 1.0458679319889285e-05, 'epoch': 2.37}\n"," 79% 2000/2529 [29:06<07:21,  1.20it/s][INFO|trainer.py:3944] 2025-03-22 07:41:28,661 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-22 07:41:28,663 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 07:41:28,664 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 07:41:33,629 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 07:41:33,633 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 07:41:33,633 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 07:41:33,634 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9454, 'grad_norm': 5.755749702453613, 'learning_rate': 5.733491498616055e-07, 'epoch': 2.97}\n"," 99% 2500/2529 [36:28<00:23,  1.21it/s][INFO|trainer.py:3944] 2025-03-22 07:48:50,626 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-22 07:48:50,628 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 07:48:50,628 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 07:48:55,622 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 07:48:55,626 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 07:48:55,626 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 07:48:55,627 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 2529/2529 [37:11<00:00,  1.44it/s][INFO|trainer.py:3944] 2025-03-22 07:49:32,953 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529\n","[INFO|configuration_utils.py:423] 2025-03-22 07:49:32,955 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 07:49:32,956 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 07:49:38,041 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 07:49:38,045 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 07:49:38,045 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 07:49:38,045 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-22 07:49:51,860 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2250.8321, 'train_samples_per_second': 8.979, 'train_steps_per_second': 1.124, 'train_loss': 2.6459885412829345, 'epoch': 3.0}\n","100% 2529/2529 [37:29<00:00,  1.12it/s]\n","[INFO|trainer.py:3944] 2025-03-22 07:49:51,870 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000\n","[INFO|configuration_utils.py:423] 2025-03-22 07:49:51,873 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 07:49:51,874 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 07:50:00,840 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 07:50:00,843 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 07:50:00,844 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 07:50:00,845 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  2830543GF\n","  train_loss               =      2.646\n","  train_runtime            = 0:37:30.83\n","  train_samples            =       6737\n","  train_samples_per_second =      8.979\n","  train_steps_per_second   =      1.124\n","03/22/2025 07:50:00 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-22 07:50:00,999 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-22 07:50:00,999 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-22 07:50:00,999 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [08:49<00:00,  4.23s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     7.9219\n","  eval_gen_len            =    46.4905\n","  eval_loss               =     2.4824\n","  eval_runtime            = 0:08:55.49\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.862\n","  eval_steps_per_second   =      0.233\n","03/22/2025 07:58:56 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-22 07:58:56,495 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-22 07:58:56,495 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-22 07:58:56,496 >>   Batch size = 8\n","100% 127/127 [09:45<00:00,  4.61s/it]\n","***** predict metrics *****\n","  predict_bleu               =     7.1617\n","  predict_gen_len            =    48.6818\n","  predict_loss               =     2.5179\n","  predict_runtime            = 0:09:52.18\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.709\n","  predict_steps_per_second   =      0.214\n","[INFO|modelcard.py:449] 2025-03-22 08:09:04,323 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 7.9219}]}\n"]}]},{"cell_type":"code","source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet_plus-2000\")"],"metadata":{"id":"SKfZPpED_kMg","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1742630948611,"user_tz":-120,"elapsed":199,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"109d2c4e-e883-48d1-9994-4e51c3b2ddf7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/w2kyng73?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x78e59f5341c0>"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/trainer_2000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-plus-2000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"id":"W_RO5tzt_ZvK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742631117802,"user_tz":-120,"elapsed":169387,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"0853c396-af70-47f0-a3f4-738f5fb8bb16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-22 08:09:13.747857: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 08:09:13.769628: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 08:09:13.776230: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-22 08:09:13.792813: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-22 08:09:14.922299: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/22/2025 08:09:19 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/22/2025 08:09:19 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-plus-2000/runs/Mar22_08-09-19_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-plus-2000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-plus-2000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-00c1f33ddcf7c443\n","03/22/2025 08:09:19 - INFO - datasets.builder - Using custom data configuration default-00c1f33ddcf7c443\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/22/2025 08:09:19 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Generating dataset json (/root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/22/2025 08:09:19 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","03/22/2025 08:09:19 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","Downloading took 0.0 min\n","03/22/2025 08:09:19 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","03/22/2025 08:09:19 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Generating train split\n","03/22/2025 08:09:19 - INFO - datasets.builder - Generating train split\n","Generating train split: 7737 examples [00:00, 460733.88 examples/s]\n","Generating validation split\n","03/22/2025 08:09:19 - INFO - datasets.builder - Generating validation split\n","Generating validation split: 997 examples [00:00, 145218.82 examples/s]\n","Generating test split\n","03/22/2025 08:09:19 - INFO - datasets.builder - Generating test split\n","Generating test split: 1012 examples [00:00, 217830.01 examples/s]\n","Unable to verify splits sizes.\n","03/22/2025 08:09:19 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","03/22/2025 08:09:19 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","[INFO|configuration_utils.py:699] 2025-03-22 08:09:20,166 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 08:09:20,169 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-22 08:09:20,256 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 08:09:20,257 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 08:09:20,258 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 08:09:20,258 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 08:09:20,258 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 08:09:20,258 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 08:09:20,258 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 08:09:20,258 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 08:09:20,258 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-22 08:09:20,258 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 08:09:20,259 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-22 08:09:21,316 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-22 08:09:21,406 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-22 08:09:21,431 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-22 08:09:21,526 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-22 08:09:21,526 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-22 08:09:21,621 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-22 08:09:21,622 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-22 08:09:21,793 >> Safetensors PR exists\n","Running tokenizer on train dataset:   0% 0/7737 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d9db24e4ea92c1e2.arrow\n","03/22/2025 08:09:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d9db24e4ea92c1e2.arrow\n","Running tokenizer on train dataset: 100% 7737/7737 [00:04<00:00, 1859.63 examples/s]\n","Running tokenizer on validation dataset:   0% 0/997 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9e62119e384f3dba.arrow\n","03/22/2025 08:09:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9e62119e384f3dba.arrow\n","Running tokenizer on validation dataset: 100% 997/997 [00:00<00:00, 1824.62 examples/s]\n","Running tokenizer on prediction dataset:   0% 0/1012 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2488e0046852a6d9.arrow\n","03/22/2025 08:09:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2488e0046852a6d9.arrow\n","Running tokenizer on prediction dataset: 100% 1012/1012 [00:00<00:00, 1768.38 examples/s]\n","[INFO|trainer.py:2407] 2025-03-22 08:09:43,712 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-22 08:09:43,712 >>   Num examples = 7,737\n","[INFO|trainer.py:2409] 2025-03-22 08:09:43,712 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-22 08:09:43,712 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-22 08:09:43,712 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-22 08:09:43,712 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-22 08:09:43,712 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-22 08:09:43,712 >>   Total optimization steps = 2,904\n","[INFO|trainer.py:2416] 2025-03-22 08:09:43,713 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-22 08:09:43,718 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250322_080943-ejjgbgtt\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-plus-2000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/ejjgbgtt\u001b[0m\n","  0% 0/2904 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","  5% 153/2904 [02:10<37:14,  1.23it/s]Traceback (most recent call last):\n","  File \"/content/transformers/examples/pytorch/translation/run_translation.py\", line 697, in <module>\n","    main()\n","  File \"/content/transformers/examples/pytorch/translation/run_translation.py\", line 612, in main\n","    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2243, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2555, in _inner_training_loop\n","    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n","KeyboardInterrupt\n","  5% 153/2904 [02:11<39:20,  1.17it/s]\n","^C\n"]}]},{"cell_type":"code","source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet_plus-4000\")"],"metadata":{"id":"-sq2Lrua_oqR","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1742631117934,"user_tz":-120,"elapsed":135,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"20e9d9d9-35a7-4045-f3ef-133ff2f9a1e6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/w2kyng73?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x78e59f5341c0>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/trainer_4000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-plus-4000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"id":"A9KnDDS6_ynN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742631122918,"user_tz":-120,"elapsed":5117,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"b843cae1-9929-426b-8edb-30bf9455057c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-22 08:12:01.770855: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 08:12:01.793986: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 08:12:01.800937: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","Traceback (most recent call last):\n","  File \"/content/transformers/examples/pytorch/translation/run_translation.py\", line 28, in <module>\n","    import evaluate\n","  File \"/usr/local/lib/python3.10/dist-packages/evaluate/__init__.py\", line 29, in <module>\n","    from .evaluation_suite import EvaluationSuite\n","  File \"/usr/local/lib/python3.10/dist-packages/evaluate/evaluation_suite/__init__.py\", line 10, in <module>\n","    from ..evaluator import evaluator\n","  File \"/usr/local/lib/python3.10/dist-packages/evaluate/evaluator/__init__.py\", line 17, in <module>\n","    from transformers.pipelines import SUPPORTED_TASKS as SUPPORTED_PIPELINE_TASKS\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\", line 26, in <module>\n","    from ..image_processing_utils import BaseImageProcessor\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py\", line 22, in <module>\n","    from .image_transforms import center_crop, normalize, rescale\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\", line 48, in <module>\n","    import tensorflow as tf\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\", line 38, in <module>\n","    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 34, in <module>\n","    self_check.preload_check()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/platform/self_check.py\", line 63, in preload_check\n","    from tensorflow.python.platform import _pywrap_cpu_feature_guard\n","KeyboardInterrupt\n"]}]},{"cell_type":"code","source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"comet-8000\")"],"metadata":{"id":"zgBkTtEQ_qHT","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1741677124497,"user_tz":-120,"elapsed":335,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"cefc32f0-a820-4c4b-b2f7-0483f139a3b9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/fax4wneh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x78058efec220>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_8000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-8000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"id":"XnLgBK_Q_0Nt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"743a55c6-529b-4968-b687-ec5c450364c9","executionInfo":{"status":"ok","timestamp":1741680683851,"user_tz":-120,"elapsed":2226696,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-11 07:12:15.544791: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-11 07:12:15.566555: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-11 07:12:15.573051: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-11 07:12:15.589312: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-11 07:12:16.661912: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/11/2025 07:12:21 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/11/2025 07:12:21 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-8000/runs/Mar11_07-12-21_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-8000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-8000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-5bc63a9eaf840ac0\n","03/11/2025 07:12:21 - INFO - datasets.builder - Using custom data configuration default-5bc63a9eaf840ac0\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/11/2025 07:12:21 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/11/2025 07:12:21 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/11/2025 07:12:21 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/11/2025 07:12:21 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/11/2025 07:12:21 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-11 07:12:22,204 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-11 07:12:22,206 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-11 07:12:22,312 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-11 07:12:22,313 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-11 07:12:22,315 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-11 07:12:22,315 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-11 07:12:23,259 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-11 07:12:23,339 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-11 07:12:23,447 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-11 07:12:23,447 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|safetensors_conversion.py:61] 2025-03-11 07:12:23,553 >> Attempting to create safetensors variant\n","[INFO|configuration_utils.py:1095] 2025-03-11 07:12:23,732 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-11 07:12:23,733 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-11 07:12:24,483 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-805d80dbaf682edc.arrow\n","03/11/2025 07:12:25 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-805d80dbaf682edc.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-8d35743ea3961495.arrow\n","03/11/2025 07:12:26 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-8d35743ea3961495.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a687ca1a7df8988f.arrow\n","03/11/2025 07:12:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a687ca1a7df8988f.arrow\n","[INFO|trainer.py:2407] 2025-03-11 07:12:37,293 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-11 07:12:37,293 >>   Num examples = 8,000\n","[INFO|trainer.py:2409] 2025-03-11 07:12:37,293 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-11 07:12:37,293 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-11 07:12:37,293 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-11 07:12:37,293 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-11 07:12:37,293 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-11 07:12:37,293 >>   Total optimization steps = 3,000\n","[INFO|trainer.py:2416] 2025-03-11 07:12:37,294 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-11 07:12:37,301 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250311_071237-d7dmi3j0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-8000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/d7dmi3j0\u001b[0m\n","  0% 0/3000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.3577, 'grad_norm': 8.726241111755371, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n"," 17% 500/3000 [05:59<29:48,  1.40it/s][INFO|trainer.py:3944] 2025-03-11 07:18:38,000 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-11 07:18:38,012 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 07:18:38,012 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 07:18:49,502 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:18:49,505 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:18:49,506 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:18:49,506 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.8395, 'grad_norm': 8.096263885498047, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"," 33% 1000/3000 [12:30<24:33,  1.36it/s][INFO|trainer.py:3944] 2025-03-11 07:25:08,462 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-11 07:25:08,464 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 07:25:08,465 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 07:25:13,388 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:25:13,391 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:25:13,392 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:25:13,392 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.045, 'grad_norm': 7.830597400665283, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n"," 50% 1500/3000 [18:47<18:04,  1.38it/s][INFO|trainer.py:3944] 2025-03-11 07:31:25,260 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-11 07:31:25,262 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 07:31:25,263 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 07:31:30,226 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:31:30,229 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:31:30,230 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:31:30,230 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.9503, 'grad_norm': 7.9375739097595215, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 2000/3000 [25:05<11:27,  1.45it/s][INFO|trainer.py:3944] 2025-03-11 07:37:44,118 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-11 07:37:44,120 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 07:37:44,121 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 07:37:49,103 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:37:49,107 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:37:49,107 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:37:49,108 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4077, 'grad_norm': 8.3837308883667, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"," 83% 2500/3000 [31:22<05:56,  1.40it/s][INFO|trainer.py:3944] 2025-03-11 07:44:00,670 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-11 07:44:00,673 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 07:44:00,674 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 07:44:05,567 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:44:05,570 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:44:05,570 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:44:05,571 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.359, 'grad_norm': 7.027705192565918, 'learning_rate': 0.0, 'epoch': 3.0}\n","100% 3000/3000 [37:42<00:00,  1.38it/s][INFO|trainer.py:3944] 2025-03-11 07:50:20,600 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-11 07:50:20,602 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 07:50:20,602 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 07:50:25,487 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:50:25,489 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:50:25,490 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:50:25,490 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-11 07:50:39,111 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2281.8171, 'train_samples_per_second': 10.518, 'train_steps_per_second': 1.315, 'train_loss': 3.159888224283854, 'epoch': 3.0}\n","100% 3000/3000 [38:00<00:00,  1.32it/s]\n","[INFO|trainer.py:3944] 2025-03-11 07:50:39,116 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000\n","[INFO|configuration_utils.py:423] 2025-03-11 07:50:39,120 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 07:50:39,169 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 07:50:54,302 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:50:54,306 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:50:54,307 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:50:54,309 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  1672691GF\n","  train_loss               =     3.1599\n","  train_runtime            = 0:38:01.81\n","  train_samples            =       8000\n","  train_samples_per_second =     10.518\n","  train_steps_per_second   =      1.315\n","03/11/2025 07:50:54 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-11 07:50:54,447 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-11 07:50:54,447 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-11 07:50:54,447 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:24<00:00,  4.52s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     5.5443\n","  eval_gen_len            =    47.0752\n","  eval_loss               =     2.9433\n","  eval_runtime            = 0:09:30.03\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.749\n","  eval_steps_per_second   =      0.219\n","03/11/2025 08:00:24 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-11 08:00:24,491 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-11 08:00:24,492 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-11 08:00:24,492 >>   Batch size = 8\n","100% 127/127 [10:32<00:00,  4.98s/it]\n","***** predict metrics *****\n","  predict_bleu               =     4.8857\n","  predict_gen_len            =    49.6897\n","  predict_loss               =     3.0054\n","  predict_runtime            = 0:10:40.04\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.581\n","  predict_steps_per_second   =      0.198\n","[INFO|modelcard.py:449] 2025-03-11 08:11:20,174 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 5.5443}]}\n"]}]},{"cell_type":"code","source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"comet-16000\")"],"metadata":{"id":"38xgPP_G_rjq","colab":{"base_uri":"https://localhost:8080/","height":171},"executionInfo":{"status":"ok","timestamp":1741682663701,"user_tz":-120,"elapsed":11410,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"c92baf51-4e22-401b-b4ab-ef5f48f8bb19"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250311_084418-vjxkfu79</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/paderborn-university/machine%20translation/runs/vjxkfu79' target=\"_blank\">comet-16000</a></strong> to <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/paderborn-university/machine%20translation/runs/vjxkfu79' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation/runs/vjxkfu79</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/vjxkfu79?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x79baa0d2c430>"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_16000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-16000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"id":"a8A_KBlC_2Q_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"16d9b74c-8102-4bf9-df55-eaf28a670771","executionInfo":{"status":"ok","timestamp":1741688608186,"user_tz":-120,"elapsed":1983098,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-11 08:45:17.490707: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-11 08:45:17.512410: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-11 08:45:17.519113: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-11 08:45:17.535362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-11 08:45:18.555378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/11/2025 08:45:23 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/11/2025 08:45:23 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-16000/runs/Mar11_08-45-23_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-16000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-16000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-8dd97eb64194ed63\n","03/11/2025 08:45:24 - INFO - datasets.builder - Using custom data configuration default-8dd97eb64194ed63\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/11/2025 08:45:24 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/11/2025 08:45:24 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/11/2025 08:45:24 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/11/2025 08:45:24 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/11/2025 08:45:24 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-11 08:45:24,413 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-11 08:45:24,416 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-11 08:45:24,503 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-11 08:45:24,504 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-11 08:45:24,506 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-11 08:45:24,507 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-11 08:45:25,474 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|safetensors_conversion.py:61] 2025-03-11 08:45:25,779 >> Attempting to create safetensors variant\n","[INFO|configuration_utils.py:1140] 2025-03-11 08:45:25,983 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-11 08:45:26,161 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-11 08:45:26,161 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-11 08:45:26,289 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-11 08:45:26,289 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-11 08:45:26,435 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4f73fd23cbbc2044.arrow\n","03/11/2025 08:45:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4f73fd23cbbc2044.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-61f573cc25eca874.arrow\n","03/11/2025 08:45:28 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-61f573cc25eca874.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-29c03a83c2242779.arrow\n","03/11/2025 08:45:30 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-29c03a83c2242779.arrow\n","[INFO|trainer.py:2407] 2025-03-11 08:45:39,481 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-11 08:45:39,481 >>   Num examples = 16,000\n","[INFO|trainer.py:2409] 2025-03-11 08:45:39,481 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-11 08:45:39,481 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-11 08:45:39,481 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-11 08:45:39,481 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-11 08:45:39,481 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-11 08:45:39,482 >>   Total optimization steps = 6,000\n","[INFO|trainer.py:2416] 2025-03-11 08:45:39,483 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-11 08:45:39,490 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250311_084539-hctutu3t\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-16000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/hctutu3t\u001b[0m\n","  0% 0/6000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.4124, 'grad_norm': 7.936241149902344, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}\n","  8% 500/6000 [06:05<1:05:10,  1.41it/s][INFO|trainer.py:3944] 2025-03-11 08:51:45,992 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-11 08:51:45,998 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 08:51:45,999 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 08:51:57,453 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 08:51:57,457 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 08:51:57,457 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 08:51:57,458 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.8947, 'grad_norm': 6.913146018981934, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n"," 17% 1000/6000 [12:42<1:03:27,  1.31it/s][INFO|trainer.py:3944] 2025-03-11 08:58:22,858 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-11 08:58:22,860 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 08:58:22,861 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 08:58:27,705 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 08:58:27,709 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 08:58:27,709 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 08:58:27,710 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.5953, 'grad_norm': 7.811131954193115, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}\n"," 25% 1500/6000 [19:05<55:15,  1.36it/s][INFO|trainer.py:3944] 2025-03-11 09:04:46,081 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-11 09:04:46,083 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:04:46,084 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:04:50,966 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:04:50,969 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:04:50,969 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:04:50,970 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.4118, 'grad_norm': 6.7470479011535645, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"," 33% 2000/6000 [25:30<48:39,  1.37it/s][INFO|trainer.py:3944] 2025-03-11 09:11:11,353 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-11 09:11:11,354 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:11:11,355 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:11:16,272 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:11:16,276 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:11:16,276 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:11:16,276 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.8207, 'grad_norm': 7.763012409210205, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}\n"," 42% 2500/6000 [31:54<42:08,  1.38it/s][INFO|trainer.py:3944] 2025-03-11 09:17:34,481 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-11 09:17:34,482 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:17:34,483 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:17:39,362 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:17:39,365 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:17:39,365 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:17:39,366 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.7635, 'grad_norm': 6.0602593421936035, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n"," 50% 3000/6000 [38:18<36:51,  1.36it/s][INFO|trainer.py:3944] 2025-03-11 09:23:59,149 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-11 09:23:59,151 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:23:59,151 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:24:04,056 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:24:04,059 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:24:04,059 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:24:04,060 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.7337, 'grad_norm': 7.55765962600708, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}\n"," 58% 3500/6000 [44:44<30:26,  1.37it/s][INFO|trainer.py:3944] 2025-03-11 09:30:24,508 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500\n","[INFO|configuration_utils.py:423] 2025-03-11 09:30:24,509 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:30:24,510 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:30:29,367 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:30:29,370 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:30:29,371 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:30:29,371 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.6668, 'grad_norm': 6.23962926864624, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 4000/6000 [51:07<25:24,  1.31it/s][INFO|trainer.py:3944] 2025-03-11 09:36:47,496 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000\n","[INFO|configuration_utils.py:423] 2025-03-11 09:36:47,498 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:36:47,498 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:36:52,406 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:36:52,409 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:36:52,409 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:36:52,410 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.2207, 'grad_norm': 6.355872631072998, 'learning_rate': 1.25e-05, 'epoch': 2.25}\n"," 75% 4500/6000 [57:31<18:13,  1.37it/s][INFO|trainer.py:3944] 2025-03-11 09:43:12,085 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500\n","[INFO|configuration_utils.py:423] 2025-03-11 09:43:12,087 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:43:12,088 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:43:17,005 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:43:17,008 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:43:17,009 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:43:17,009 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.2021, 'grad_norm': 7.565613746643066, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"," 83% 5000/6000 [1:03:55<12:06,  1.38it/s][INFO|trainer.py:3944] 2025-03-11 09:49:36,015 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000\n","[INFO|configuration_utils.py:423] 2025-03-11 09:49:36,017 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:49:36,017 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:49:40,929 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:49:40,932 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:49:40,933 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:49:40,933 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.1973, 'grad_norm': 14.712209701538086, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}\n"," 92% 5500/6000 [1:10:18<05:52,  1.42it/s][INFO|trainer.py:3944] 2025-03-11 09:55:59,282 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500\n","[INFO|configuration_utils.py:423] 2025-03-11 09:55:59,284 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:55:59,285 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:56:04,201 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:56:04,204 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:56:04,204 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:56:04,205 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.1321, 'grad_norm': 7.183525085449219, 'learning_rate': 0.0, 'epoch': 3.0}\n","100% 6000/6000 [1:16:43<00:00,  1.38it/s][INFO|trainer.py:3944] 2025-03-11 10:02:23,522 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000\n","[INFO|configuration_utils.py:423] 2025-03-11 10:02:23,525 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 10:02:23,526 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 10:02:28,451 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 10:02:28,454 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 10:02:28,454 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 10:02:28,455 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-11 10:02:42,100 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 4622.6168, 'train_samples_per_second': 10.384, 'train_steps_per_second': 1.298, 'train_loss': 2.920919514973958, 'epoch': 3.0}\n","100% 6000/6000 [1:17:01<00:00,  1.30it/s]\n","[INFO|trainer.py:3944] 2025-03-11 10:02:42,110 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000\n","[INFO|configuration_utils.py:423] 2025-03-11 10:02:42,137 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 10:02:42,138 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 10:02:57,056 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 10:02:57,060 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 10:02:57,061 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 10:02:57,061 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  3307792GF\n","  train_loss               =     2.9209\n","  train_runtime            = 1:17:02.61\n","  train_samples            =      16000\n","  train_samples_per_second =     10.384\n","  train_steps_per_second   =      1.298\n","03/11/2025 10:02:57 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-11 10:02:57,210 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-11 10:02:57,211 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-11 10:02:57,211 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:38<00:00,  4.63s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     6.7478\n","  eval_gen_len            =    46.4132\n","  eval_loss               =     2.5626\n","  eval_runtime            = 0:09:46.79\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.699\n","  eval_steps_per_second   =      0.213\n","03/11/2025 10:12:44 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-11 10:12:44,006 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-11 10:12:44,006 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-11 10:12:44,006 >>   Batch size = 8\n","100% 127/127 [10:18<00:00,  4.87s/it]\n","***** predict metrics *****\n","  predict_bleu               =     6.2998\n","  predict_gen_len            =    48.8399\n","  predict_loss               =     2.6004\n","  predict_runtime            = 0:10:25.58\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.618\n","  predict_steps_per_second   =      0.203\n","[INFO|modelcard.py:449] 2025-03-11 10:23:25,008 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 6.7478}]}\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wslf2Suxpp-R"},"execution_count":null,"outputs":[]}]}