<<<<<<< HEAD
{"cells":[{"cell_type":"markdown","metadata":{"id":"SAvOFiCfspNZ"},"source":["# Leveraging Bitext mining and COMET-QE for improving parallel data selection of low-resource machine translation\n","<a href=\"https://colab.research.google.com/github/emmanuelayanful/AIMS-NLP-Project/blob/main/M2M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqsa3QW1weGy"},"outputs":[],"source":["import wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ICCsdcJZpub"},"outputs":[],"source":["# !rm -rf M2M-100/ wandb/"]},{"cell_type":"markdown","metadata":{"id":"qZQLQPZjy2QO"},"source":["# Baseline Run"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"executionInfo":{"elapsed":3504,"status":"ok","timestamp":1741616291599,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"aggyfHIUR2zp","outputId":"2d6920f7-eda9-447a-ef8f-12ed3c667da5"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.19.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20250310_141809-2aublbbj</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj' target=\"_blank\">baseline</a></strong> to <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Log in with your API key\n","wandb.login(key=\"7f13d9fe09d0856f7c12099a27ccda7aa15c8afd\")\n","\n","# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"baseline\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3236734,"status":"ok","timestamp":1741619528327,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"VvM-Zf4jUO81","outputId":"f0790cb9-ef48-43a0-9f4a-89e380020650"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-10 14:18:16.490271: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-10 14:18:16.513278: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-10 14:18:16.520425: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-10 14:18:16.537626: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-10 14:18:17.613945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/10/2025 14:18:20 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/10/2025 14:18:20 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/baseline/runs/Mar10_14-18-19_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/baseline,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/baseline,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-fcf11d994dda434e\n","03/10/2025 14:18:20 - INFO - datasets.builder - Using custom data configuration default-fcf11d994dda434e\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/10/2025 14:18:20 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/10/2025 14:18:20 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 14:18:20 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/10/2025 14:18:20 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 14:18:20 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-10 14:18:20,481 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 14:18:20,483 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-10 14:18:20,581 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 14:18:20,582 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 14:18:20,583 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 14:18:20,583 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 14:18:20,583 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 14:18:20,583 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 14:18:20,583 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 14:18:20,583 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 14:18:20,583 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-10 14:18:20,584 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 14:18:20,584 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-10 14:18:21,486 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-10 14:18:21,552 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-10 14:18:21,597 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-10 14:18:21,661 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-10 14:18:21,661 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-10 14:18:21,756 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-10 14:18:21,756 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-10 14:18:21,975 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-500813e1b41c1f29.arrow\n","03/10/2025 14:18:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-500813e1b41c1f29.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-24d83da570939502.arrow\n","03/10/2025 14:18:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-24d83da570939502.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c4510aa2c36da9e3.arrow\n","03/10/2025 14:18:25 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c4510aa2c36da9e3.arrow\n","[INFO|trainer.py:2407] 2025-03-10 14:18:28,106 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-10 14:18:28,106 >>   Num examples = 5,737\n","[INFO|trainer.py:2409] 2025-03-10 14:18:28,106 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-10 14:18:28,106 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-10 14:18:28,106 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-10 14:18:28,106 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-10 14:18:28,106 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-10 14:18:28,106 >>   Total optimization steps = 2,154\n","[INFO|trainer.py:2416] 2025-03-10 14:18:28,107 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-10 14:18:28,113 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_141828-ulsf7cg9\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/baseline\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/ulsf7cg9\u001b[0m\n","  0% 0/2154 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.642, 'grad_norm': 6.3416852951049805, 'learning_rate': 3.8393686165273915e-05, 'epoch': 0.7}\n"," 23% 500/2154 [07:30<23:59,  1.15it/s][INFO|trainer.py:3944] 2025-03-10 14:25:59,660 >> Saving model checkpoint to M2M-100/baseline/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-10 14:25:59,664 >> Configuration saved in M2M-100/baseline/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 14:25:59,664 >> Configuration saved in M2M-100/baseline/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 14:26:04,580 >> Model weights saved in M2M-100/baseline/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 14:26:04,583 >> tokenizer config file saved in M2M-100/baseline/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 14:26:04,584 >> Special tokens file saved in M2M-100/baseline/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 14:26:04,584 >> added tokens file saved in M2M-100/baseline/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.781, 'grad_norm': 6.211764812469482, 'learning_rate': 2.678737233054782e-05, 'epoch': 1.39}\n"," 46% 1000/2154 [15:22<16:02,  1.20it/s][INFO|trainer.py:3944] 2025-03-10 14:33:51,805 >> Saving model checkpoint to M2M-100/baseline/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-10 14:33:51,807 >> Configuration saved in M2M-100/baseline/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 14:33:51,807 >> Configuration saved in M2M-100/baseline/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 14:33:56,812 >> Model weights saved in M2M-100/baseline/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 14:33:56,818 >> tokenizer config file saved in M2M-100/baseline/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 14:33:56,819 >> Special tokens file saved in M2M-100/baseline/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 14:33:56,819 >> added tokens file saved in M2M-100/baseline/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3355, 'grad_norm': 5.986948490142822, 'learning_rate': 1.518105849582173e-05, 'epoch': 2.09}\n"," 70% 1500/2154 [23:16<09:25,  1.16it/s][INFO|trainer.py:3944] 2025-03-10 14:41:44,981 >> Saving model checkpoint to M2M-100/baseline/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-10 14:41:44,982 >> Configuration saved in M2M-100/baseline/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 14:41:44,983 >> Configuration saved in M2M-100/baseline/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 14:41:49,973 >> Model weights saved in M2M-100/baseline/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 14:41:49,977 >> tokenizer config file saved in M2M-100/baseline/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 14:41:49,977 >> Special tokens file saved in M2M-100/baseline/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 14:41:49,978 >> added tokens file saved in M2M-100/baseline/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9662, 'grad_norm': 6.2669148445129395, 'learning_rate': 3.574744661095636e-06, 'epoch': 2.79}\n"," 93% 2000/2154 [31:07<02:20,  1.10it/s][INFO|trainer.py:3944] 2025-03-10 14:49:36,623 >> Saving model checkpoint to M2M-100/baseline/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-10 14:49:36,624 >> Configuration saved in M2M-100/baseline/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 14:49:36,625 >> Configuration saved in M2M-100/baseline/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 14:49:41,557 >> Model weights saved in M2M-100/baseline/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 14:49:41,561 >> tokenizer config file saved in M2M-100/baseline/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 14:49:41,561 >> Special tokens file saved in M2M-100/baseline/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 14:49:41,562 >> added tokens file saved in M2M-100/baseline/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 2154/2154 [33:44<00:00,  1.50it/s][INFO|trainer.py:3944] 2025-03-10 14:52:13,206 >> Saving model checkpoint to M2M-100/baseline/checkpoint-2154\n","[INFO|configuration_utils.py:423] 2025-03-10 14:52:13,208 >> Configuration saved in M2M-100/baseline/checkpoint-2154/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 14:52:13,208 >> Configuration saved in M2M-100/baseline/checkpoint-2154/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 14:52:18,268 >> Model weights saved in M2M-100/baseline/checkpoint-2154/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 14:52:18,270 >> tokenizer config file saved in M2M-100/baseline/checkpoint-2154/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 14:52:18,271 >> Special tokens file saved in M2M-100/baseline/checkpoint-2154/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 14:52:18,271 >> added tokens file saved in M2M-100/baseline/checkpoint-2154/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-10 14:52:31,891 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2043.7839, 'train_samples_per_second': 8.421, 'train_steps_per_second': 1.054, 'train_loss': 2.626034424054811, 'epoch': 3.0}\n","100% 2154/2154 [34:02<00:00,  1.05it/s]\n","[INFO|trainer.py:3944] 2025-03-10 14:52:31,901 >> Saving model checkpoint to M2M-100/baseline\n","[INFO|configuration_utils.py:423] 2025-03-10 14:52:31,902 >> Configuration saved in M2M-100/baseline/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 14:52:31,903 >> Configuration saved in M2M-100/baseline/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 14:52:40,831 >> Model weights saved in M2M-100/baseline/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 14:52:40,833 >> tokenizer config file saved in M2M-100/baseline/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 14:52:40,834 >> Special tokens file saved in M2M-100/baseline/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 14:52:40,834 >> added tokens file saved in M2M-100/baseline/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  2504202GF\n","  train_loss               =      2.626\n","  train_runtime            = 0:34:03.78\n","  train_samples            =       5737\n","  train_samples_per_second =      8.421\n","  train_steps_per_second   =      1.054\n","03/10/2025 14:52:40 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-10 14:52:40,981 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-10 14:52:40,981 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-10 14:52:40,981 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:06<00:00,  4.37s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     7.2001\n","  eval_gen_len            =    47.1946\n","  eval_loss               =     2.5306\n","  eval_runtime            = 0:09:14.70\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.797\n","  eval_steps_per_second   =      0.225\n","03/10/2025 15:01:55 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-10 15:01:55,694 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-10 15:01:55,695 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-10 15:01:55,695 >>   Batch size = 8\n","100% 127/127 [09:47<00:00,  4.63s/it]\n","***** predict metrics *****\n","  predict_bleu               =     7.3419\n","  predict_gen_len            =    49.1245\n","  predict_loss               =     2.5737\n","  predict_runtime            = 0:09:54.65\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.702\n","  predict_steps_per_second   =      0.214\n","[INFO|modelcard.py:449] 2025-03-10 15:12:05,599 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 7.2001}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/mafand/en-zu/merged.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/baseline \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"markdown","metadata":{"id":"1OEaIczVafcf"},"source":["# Africomet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"elapsed":235,"status":"ok","timestamp":1741619528328,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"Mp6AahGfufdK","outputId":"8fd256aa-0f29-4dcb-ef13-06a227a1f225"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet-1000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1850320,"status":"ok","timestamp":1741621378424,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"hbNrB5boVIgU","outputId":"940328a7-f8fb-4a5f-939c-7508d167fd19"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-10 15:12:13.555077: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-10 15:12:13.576922: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-10 15:12:13.583428: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-10 15:12:13.599610: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-10 15:12:14.655466: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/10/2025 15:12:18 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/10/2025 15:12:18 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-1000/runs/Mar10_15-12-18_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-1000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-1000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-d22542853ecd2369\n","03/10/2025 15:12:19 - INFO - datasets.builder - Using custom data configuration default-d22542853ecd2369\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/10/2025 15:12:19 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/10/2025 15:12:19 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 15:12:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/10/2025 15:12:19 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 15:12:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-10 15:12:19,381 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 15:12:19,383 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-10 15:12:19,615 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 15:12:19,615 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:12:19,616 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:12:19,616 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:12:19,616 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:12:19,616 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:12:19,616 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:12:19,616 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:12:19,616 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-10 15:12:19,617 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 15:12:19,617 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-10 15:12:20,849 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-10 15:12:20,919 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-10 15:12:20,966 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-10 15:12:21,024 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-10 15:12:21,024 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-10 15:12:21,119 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-10 15:12:21,119 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-10 15:12:21,327 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e70b427f4acd1a90.arrow\n","03/10/2025 15:12:22 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e70b427f4acd1a90.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1e1e9cb1ba5e0af7.arrow\n","03/10/2025 15:12:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1e1e9cb1ba5e0af7.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b37e93e0dc9f95ce.arrow\n","03/10/2025 15:12:25 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b37e93e0dc9f95ce.arrow\n","[INFO|trainer.py:2407] 2025-03-10 15:12:27,279 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-10 15:12:27,279 >>   Num examples = 1,000\n","[INFO|trainer.py:2409] 2025-03-10 15:12:27,279 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-10 15:12:27,280 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-10 15:12:27,280 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-10 15:12:27,280 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-10 15:12:27,280 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-10 15:12:27,280 >>   Total optimization steps = 375\n","[INFO|trainer.py:2416] 2025-03-10 15:12:27,281 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-10 15:12:27,286 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_151227-bthnr4uy\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-1000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/bthnr4uy\u001b[0m\n","  0% 0/375 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 375/375 [04:42<00:00,  1.35it/s][INFO|trainer.py:3944] 2025-03-10 15:17:10,484 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-10 15:17:10,487 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 15:17:10,488 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 15:17:15,538 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 15:17:15,541 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 15:17:15,542 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 15:17:15,543 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-10 15:17:29,121 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 301.8402, 'train_samples_per_second': 9.939, 'train_steps_per_second': 1.242, 'train_loss': 3.7139088541666667, 'epoch': 3.0}\n","100% 375/375 [05:00<00:00,  1.25it/s]\n","[INFO|trainer.py:3944] 2025-03-10 15:17:29,132 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-1000\n","[INFO|configuration_utils.py:423] 2025-03-10 15:17:29,134 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 15:17:29,134 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 15:17:38,072 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 15:17:38,075 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 15:17:38,075 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 15:17:38,076 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =   205627GF\n","  train_loss               =     3.7139\n","  train_runtime            = 0:05:01.84\n","  train_samples            =       1000\n","  train_samples_per_second =      9.939\n","  train_steps_per_second   =      1.242\n","03/10/2025 15:17:38 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-10 15:17:38,225 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-10 15:17:38,226 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-10 15:17:38,226 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [11:36<00:00,  5.57s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     2.2044\n","  eval_gen_len            =    54.6329\n","  eval_loss               =     3.9725\n","  eval_runtime            = 0:11:43.50\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.417\n","  eval_steps_per_second   =      0.178\n","03/10/2025 15:29:21 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-10 15:29:21,737 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-10 15:29:21,737 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-10 15:29:21,737 >>   Batch size = 8\n","100% 127/127 [13:07<00:00,  6.20s/it]\n","***** predict metrics *****\n","  predict_bleu               =     1.9318\n","  predict_gen_len            =    56.9022\n","  predict_loss               =     4.0076\n","  predict_runtime            = 0:13:16.19\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.271\n","  predict_steps_per_second   =       0.16\n","[INFO|modelcard.py:449] 2025-03-10 15:42:55,361 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 2.2044}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_1000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-1000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"elapsed":242,"status":"ok","timestamp":1741621378425,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"oAhoqvCbuk06","outputId":"bf367f98-0f7e-4ae5-dd85-8a721d4a908a"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet-2000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2180767,"status":"ok","timestamp":1741623558963,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"S0Vip1YNtpQn","outputId":"1479f6f2-e77a-4289-ea6a-281bf29bc7fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-10 15:43:04.181598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-10 15:43:04.206516: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-10 15:43:04.213821: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-10 15:43:04.232535: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-10 15:43:05.563373: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/10/2025 15:43:10 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/10/2025 15:43:10 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-2000/runs/Mar10_15-43-10_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-2000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-2000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-63fa3ea70d279152\n","03/10/2025 15:43:10 - INFO - datasets.builder - Using custom data configuration default-63fa3ea70d279152\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/10/2025 15:43:10 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/10/2025 15:43:10 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 15:43:10 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/10/2025 15:43:10 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 15:43:10 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-10 15:43:10,630 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 15:43:10,632 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-10 15:43:10,719 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 15:43:10,720 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:43:10,721 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:43:10,721 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:43:10,721 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:43:10,721 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:43:10,721 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:43:10,721 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:43:10,721 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-10 15:43:10,722 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 15:43:10,723 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-10 15:43:11,819 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-10 15:43:11,898 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-10 15:43:11,964 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-10 15:43:12,040 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-10 15:43:12,040 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-10 15:43:12,131 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-10 15:43:12,132 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-10 15:43:12,387 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-239a0c8168036379.arrow\n","03/10/2025 15:43:13 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-239a0c8168036379.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4e37c8bc3d2bbba6.arrow\n","03/10/2025 15:43:15 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4e37c8bc3d2bbba6.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-283cd450680fa67d.arrow\n","03/10/2025 15:43:16 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-283cd450680fa67d.arrow\n","[INFO|trainer.py:2407] 2025-03-10 15:43:19,332 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-10 15:43:19,332 >>   Num examples = 2,000\n","[INFO|trainer.py:2409] 2025-03-10 15:43:19,332 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-10 15:43:19,332 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-10 15:43:19,332 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-10 15:43:19,332 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-10 15:43:19,332 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-10 15:43:19,332 >>   Total optimization steps = 750\n","[INFO|trainer.py:2416] 2025-03-10 15:43:19,333 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-10 15:43:19,339 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_154319-6o0957if\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-2000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/6o0957if\u001b[0m\n","  0% 0/750 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.0076, 'grad_norm': 9.165338516235352, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 500/750 [06:18<03:06,  1.34it/s][INFO|trainer.py:3944] 2025-03-10 15:49:38,632 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-10 15:49:38,635 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 15:49:38,635 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 15:49:43,833 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 15:49:43,837 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 15:49:43,838 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 15:49:43,838 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 750/750 [09:48<00:00,  1.32it/s][INFO|trainer.py:3944] 2025-03-10 15:53:08,314 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750\n","[INFO|configuration_utils.py:423] 2025-03-10 15:53:08,316 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 15:53:08,317 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 15:53:14,152 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 15:53:14,157 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 15:53:14,157 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 15:53:14,158 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-10 15:53:27,615 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 608.2826, 'train_samples_per_second': 9.864, 'train_steps_per_second': 1.233, 'train_loss': 3.6032588704427084, 'epoch': 3.0}\n","100% 750/750 [10:07<00:00,  1.23it/s]\n","[INFO|trainer.py:3944] 2025-03-10 15:53:27,626 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-2000\n","[INFO|configuration_utils.py:423] 2025-03-10 15:53:27,629 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 15:53:27,630 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 15:53:36,540 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 15:53:36,544 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 15:53:36,545 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 15:53:36,545 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =   409031GF\n","  train_loss               =     3.6033\n","  train_runtime            = 0:10:08.28\n","  train_samples            =       2000\n","  train_samples_per_second =      9.864\n","  train_steps_per_second   =      1.233\n","03/10/2025 15:53:36 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-10 15:53:36,707 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-10 15:53:36,707 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-10 15:53:36,708 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [12:21<00:00,  5.93s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     2.4532\n","  eval_gen_len            =     54.345\n","  eval_loss               =     3.7164\n","  eval_runtime            = 0:12:27.98\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.333\n","  eval_steps_per_second   =      0.167\n","03/10/2025 16:06:04 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-10 16:06:04,703 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-10 16:06:04,704 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-10 16:06:04,704 >>   Batch size = 8\n","100% 127/127 [12:43<00:00,  6.01s/it]\n","***** predict metrics *****\n","  predict_bleu               =     2.4691\n","  predict_gen_len            =    55.2362\n","  predict_loss               =     3.7659\n","  predict_runtime            = 0:12:51.87\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.311\n","  predict_steps_per_second   =      0.165\n","[INFO|modelcard.py:449] 2025-03-10 16:19:15,368 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 2.4532}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_2000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-2000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"elapsed":225,"status":"ok","timestamp":1741623558963,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"0v4YeYRgumqu","outputId":"a73abb8e-6f01-4eb2-a1f6-a68e504358a5"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet-4000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2656006,"status":"ok","timestamp":1741626214754,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"yGUYnc2wtrby","outputId":"f6263987-ce01-4377-8bd5-c34a91612576"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-10 16:19:23.692470: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-10 16:19:23.717422: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-10 16:19:23.724751: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-10 16:19:23.744686: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-10 16:19:25.130002: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/10/2025 16:19:28 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/10/2025 16:19:28 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-4000/runs/Mar10_16-19-28_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-4000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-4000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-2c18f5caf6d15f80\n","03/10/2025 16:19:29 - INFO - datasets.builder - Using custom data configuration default-2c18f5caf6d15f80\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/10/2025 16:19:29 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/10/2025 16:19:29 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 16:19:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/10/2025 16:19:29 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 16:19:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-10 16:19:29,202 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 16:19:29,204 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-10 16:19:29,319 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 16:19:29,320 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 16:19:29,321 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 16:19:29,321 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 16:19:29,321 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 16:19:29,321 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 16:19:29,321 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 16:19:29,321 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 16:19:29,321 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-10 16:19:29,322 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 16:19:29,323 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-10 16:19:30,440 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-10 16:19:30,519 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-10 16:19:30,556 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-10 16:19:30,655 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-10 16:19:30,656 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-10 16:19:30,752 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-10 16:19:30,752 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-10 16:19:31,115 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b3e5c8b83b5f5d81.arrow\n","03/10/2025 16:19:32 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b3e5c8b83b5f5d81.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d80ea2e702926f3c.arrow\n","03/10/2025 16:19:34 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d80ea2e702926f3c.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-94a06ef0cd1537df.arrow\n","03/10/2025 16:19:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-94a06ef0cd1537df.arrow\n","[INFO|trainer.py:2407] 2025-03-10 16:19:38,157 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-10 16:19:38,158 >>   Num examples = 4,000\n","[INFO|trainer.py:2409] 2025-03-10 16:19:38,158 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-10 16:19:38,158 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-10 16:19:38,158 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-10 16:19:38,158 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-10 16:19:38,158 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-10 16:19:38,158 >>   Total optimization steps = 1,500\n","[INFO|trainer.py:2416] 2025-03-10 16:19:38,159 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-10 16:19:38,165 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_161938-86eywks3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-4000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/86eywks3\u001b[0m\n","  0% 0/1500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.3848, 'grad_norm': 6.340878486633301, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"," 33% 500/1500 [06:22<12:49,  1.30it/s][INFO|trainer.py:3944] 2025-03-10 16:26:01,177 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-10 16:26:01,181 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 16:26:01,182 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 16:26:06,218 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 16:26:06,221 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 16:26:06,222 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 16:26:06,222 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.2917, 'grad_norm': 8.45515251159668, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 1000/1500 [12:58<06:52,  1.21it/s][INFO|trainer.py:3944] 2025-03-10 16:32:37,705 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-10 16:32:37,707 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 16:32:37,708 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 16:32:43,422 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 16:32:43,425 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 16:32:43,426 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 16:32:43,426 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.64, 'grad_norm': 10.272100448608398, 'learning_rate': 0.0, 'epoch': 3.0}\n","100% 1500/1500 [19:36<00:00,  1.35it/s][INFO|trainer.py:3944] 2025-03-10 16:39:15,183 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-10 16:39:15,184 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 16:39:15,185 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 16:39:20,610 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 16:39:20,614 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 16:39:20,615 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 16:39:20,615 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-10 16:39:34,161 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 1196.0024, 'train_samples_per_second': 10.033, 'train_steps_per_second': 1.254, 'train_loss': 3.438828369140625, 'epoch': 3.0}\n","100% 1500/1500 [19:55<00:00,  1.26it/s]\n","[INFO|trainer.py:3944] 2025-03-10 16:39:34,173 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-4000\n","[INFO|configuration_utils.py:423] 2025-03-10 16:39:34,174 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 16:39:34,175 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 16:39:43,103 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 16:39:43,108 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 16:39:43,109 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 16:39:43,109 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =   849267GF\n","  train_loss               =     3.4388\n","  train_runtime            = 0:19:56.00\n","  train_samples            =       4000\n","  train_samples_per_second =     10.033\n","  train_steps_per_second   =      1.254\n","03/10/2025 16:39:43 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-10 16:39:43,265 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-10 16:39:43,265 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-10 16:39:43,265 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [11:11<00:00,  5.37s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     3.6671\n","  eval_gen_len            =    49.7061\n","  eval_loss               =      3.353\n","  eval_runtime            = 0:11:19.28\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.468\n","  eval_steps_per_second   =      0.184\n","03/10/2025 16:51:02 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-10 16:51:02,554 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-10 16:51:02,554 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-10 16:51:02,554 >>   Batch size = 8\n","100% 127/127 [12:03<00:00,  5.70s/it]\n","***** predict metrics *****\n","  predict_bleu               =     3.0816\n","  predict_gen_len            =    52.0968\n","  predict_loss               =     3.4086\n","  predict_runtime            = 0:12:09.82\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.387\n","  predict_steps_per_second   =      0.174\n","[INFO|modelcard.py:449] 2025-03-10 17:03:31,361 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 3.6671}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_4000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-4000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"elapsed":351,"status":"ok","timestamp":1741626214755,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"hKmXZ3xLuoYh","outputId":"ce91efb5-bca3-48d9-bb5e-be0800661186"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet-8000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59743,"status":"ok","timestamp":1741634746236,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"TZSOQFxvttWv","outputId":"747535e8-c367-45a9-d48d-fc14c8439854"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-10 17:03:38.550278: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-10 17:03:38.583109: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-10 17:03:38.590670: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-10 17:03:38.609732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-10 17:03:39.832340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/10/2025 17:03:42 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/10/2025 17:03:42 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-8000/runs/Mar10_17-03-42_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-8000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-8000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-5bc63a9eaf840ac0\n","03/10/2025 17:03:43 - INFO - datasets.builder - Using custom data configuration default-5bc63a9eaf840ac0\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/10/2025 17:03:43 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/10/2025 17:03:43 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 17:03:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/10/2025 17:03:43 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 17:03:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-10 17:03:43,240 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 17:03:43,242 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-10 17:03:43,395 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 17:03:43,396 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,397 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,397 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,397 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,398 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,398 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,398 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,398 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-10 17:03:43,398 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 17:03:43,399 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-10 17:03:44,414 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-10 17:03:44,491 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-10 17:03:44,531 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-10 17:03:44,617 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-10 17:03:44,617 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-10 17:03:44,715 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-10 17:03:44,715 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-10 17:03:44,904 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-805d80dbaf682edc.arrow\n","03/10/2025 17:03:46 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-805d80dbaf682edc.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-8d35743ea3961495.arrow\n","03/10/2025 17:03:47 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-8d35743ea3961495.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a687ca1a7df8988f.arrow\n","03/10/2025 17:03:49 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a687ca1a7df8988f.arrow\n","[INFO|trainer.py:2407] 2025-03-10 17:03:51,848 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-10 17:03:51,848 >>   Num examples = 8,000\n","[INFO|trainer.py:2409] 2025-03-10 17:03:51,848 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-10 17:03:51,848 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-10 17:03:51,848 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-10 17:03:51,848 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-10 17:03:51,848 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-10 17:03:51,848 >>   Total optimization steps = 3,000\n","[INFO|trainer.py:2416] 2025-03-10 17:03:51,849 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-10 17:03:51,855 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_170351-7zajcprb\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-8000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/7zajcprb\u001b[0m\n","  0% 0/3000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.3678, 'grad_norm': 8.133068084716797, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n"," 17% 500/3000 [06:18<31:10,  1.34it/s][INFO|trainer.py:3944] 2025-03-10 17:10:11,245 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-10 17:10:11,248 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 17:10:11,249 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 17:10:16,754 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:10:16,757 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:10:16,758 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:10:16,758 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.8433, 'grad_norm': 8.525639533996582, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"," 33% 1000/3000 [12:57<25:38,  1.30it/s][INFO|trainer.py:3944] 2025-03-10 17:16:50,074 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-10 17:16:50,076 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 17:16:50,077 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 17:16:55,629 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:16:55,632 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:16:55,632 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:16:55,633 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.0452, 'grad_norm': 7.349609851837158, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n"," 50% 1500/3000 [19:33<18:57,  1.32it/s][INFO|trainer.py:3944] 2025-03-10 17:23:26,116 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-10 17:23:26,118 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 17:23:26,119 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 17:23:31,635 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:23:31,638 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:23:31,639 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:23:31,639 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.95, 'grad_norm': 8.351008415222168, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 2000/3000 [26:11<12:11,  1.37it/s][INFO|trainer.py:3944] 2025-03-10 17:30:03,819 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-10 17:30:03,821 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 17:30:03,822 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 17:30:09,382 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:30:09,386 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:30:09,387 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:30:09,388 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4116, 'grad_norm': 9.160988807678223, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"," 83% 2500/3000 [32:46<06:13,  1.34it/s][INFO|trainer.py:3944] 2025-03-10 17:36:39,364 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-10 17:36:39,366 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 17:36:39,367 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 17:36:44,901 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:36:44,908 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:36:44,909 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:36:44,909 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3545, 'grad_norm': 6.916102886199951, 'learning_rate': 0.0, 'epoch': 3.0}\n","100% 3000/3000 [39:25<00:00,  1.31it/s][INFO|trainer.py:3944] 2025-03-10 17:43:18,656 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-10 17:43:18,658 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 17:43:18,659 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 17:43:24,214 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:43:24,217 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:43:24,217 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:43:24,218 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-10 17:43:37,703 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2385.8542, 'train_samples_per_second': 10.059, 'train_steps_per_second': 1.257, 'train_loss': 3.1620497639973957, 'epoch': 3.0}\n","100% 3000/3000 [39:44<00:00,  1.26it/s]\n","[INFO|trainer.py:3944] 2025-03-10 17:43:37,707 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000\n","[INFO|configuration_utils.py:423] 2025-03-10 17:43:37,709 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 17:43:37,710 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 17:43:46,668 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:43:46,672 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:43:46,672 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:43:46,673 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  1672691GF\n","  train_loss               =      3.162\n","  train_runtime            = 0:39:45.85\n","  train_samples            =       8000\n","  train_samples_per_second =     10.059\n","  train_steps_per_second   =      1.257\n","03/10/2025 17:43:46 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-10 17:43:46,838 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-10 17:43:46,838 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-10 17:43:46,839 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:45<00:00,  4.68s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     5.3524\n","  eval_gen_len            =    46.8586\n","  eval_loss               =     2.9557\n","  eval_runtime            = 0:09:51.81\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.685\n","  eval_steps_per_second   =      0.211\n","03/10/2025 17:53:38 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-10 17:53:38,657 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-10 17:53:38,657 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-10 17:53:38,657 >>   Batch size = 8\n","100% 127/127 [10:52<00:00,  5.14s/it]\n","***** predict metrics *****\n","  predict_bleu               =     4.9433\n","  predict_gen_len            =    49.2668\n","  predict_loss               =     3.0128\n","  predict_runtime            = 0:10:58.87\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.536\n","  predict_steps_per_second   =      0.193\n","[INFO|modelcard.py:449] 2025-03-10 18:04:55,614 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 5.3524}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_8000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-8000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":37197,"status":"ok","timestamp":1741634746530,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"50vt5hMgup68","outputId":"63b5de95-854a-42da-ca9c-2117e0317b58"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet-16000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1221111,"status":"ok","timestamp":1741635930446,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"mlfAMTxMtvOi","outputId":"5aa226a1-9582-4fb7-80bd-d80b5cf5b37f"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-10 18:05:02.627668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-10 18:05:02.653295: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-10 18:05:02.660907: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-10 18:05:02.679637: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-10 18:05:03.876059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/10/2025 18:05:06 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/10/2025 18:05:06 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-16000/runs/Mar10_18-05-06_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-16000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-16000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-8dd97eb64194ed63\n","03/10/2025 18:05:06 - INFO - datasets.builder - Using custom data configuration default-8dd97eb64194ed63\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/10/2025 18:05:06 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/10/2025 18:05:06 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 18:05:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/10/2025 18:05:06 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 18:05:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-10 18:05:07,170 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 18:05:07,172 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-10 18:05:07,258 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 18:05:07,259 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,260 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,260 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,260 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,260 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,260 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,261 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,261 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-10 18:05:07,261 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 18:05:07,262 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-10 18:05:08,314 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-10 18:05:08,391 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-10 18:05:08,426 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-10 18:05:08,524 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-10 18:05:08,524 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-10 18:05:08,671 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-10 18:05:08,671 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4f73fd23cbbc2044.arrow\n","03/10/2025 18:05:10 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4f73fd23cbbc2044.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-61f573cc25eca874.arrow\n","03/10/2025 18:05:11 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-61f573cc25eca874.arrow\n","[INFO|safetensors_conversion.py:74] 2025-03-10 18:05:12,713 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-29c03a83c2242779.arrow\n","03/10/2025 18:05:13 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-29c03a83c2242779.arrow\n","[INFO|trainer.py:2407] 2025-03-10 18:05:15,747 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-10 18:05:15,747 >>   Num examples = 16,000\n","[INFO|trainer.py:2409] 2025-03-10 18:05:15,747 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-10 18:05:15,747 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-10 18:05:15,747 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-10 18:05:15,747 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-10 18:05:15,747 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-10 18:05:15,747 >>   Total optimization steps = 6,000\n","[INFO|trainer.py:2416] 2025-03-10 18:05:15,749 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-10 18:05:15,755 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_180515-53flw2ac\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-16000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/53flw2ac\u001b[0m\n","  0% 0/6000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.4093, 'grad_norm': 8.515572547912598, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}\n","  8% 500/6000 [06:17<1:07:03,  1.37it/s][INFO|trainer.py:3944] 2025-03-10 18:11:34,382 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-10 18:11:34,386 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 18:11:34,387 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 18:11:39,789 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:11:39,792 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:11:39,792 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:11:39,793 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.8951, 'grad_norm': 7.606267929077148, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n"," 17% 1000/6000 [12:55<1:05:41,  1.27it/s][INFO|trainer.py:3944] 2025-03-10 18:18:12,301 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-10 18:18:12,303 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 18:18:12,304 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 18:18:17,852 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:18:17,856 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:18:17,857 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:18:17,857 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.5859, 'grad_norm': 7.814512252807617, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}\n"," 25% 1500/6000 [19:31<56:49,  1.32it/s][INFO|trainer.py:3944] 2025-03-10 18:24:47,824 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-10 18:24:47,826 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 18:24:47,827 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 18:24:53,373 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:24:53,377 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:24:53,377 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:24:53,378 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.4074, 'grad_norm': 7.271307468414307, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"," 33% 2000/6000 [26:08<50:21,  1.32it/s][INFO|trainer.py:3944] 2025-03-10 18:31:25,350 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-10 18:31:25,352 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 18:31:25,353 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 18:31:30,850 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:31:30,853 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:31:30,853 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:31:30,854 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.8236, 'grad_norm': 7.814577102661133, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}\n"," 42% 2500/6000 [32:44<43:31,  1.34it/s][INFO|trainer.py:3944] 2025-03-10 18:38:00,848 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-10 18:38:00,850 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 18:38:00,851 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 18:38:06,401 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:38:06,404 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:38:06,405 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:38:06,406 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.758, 'grad_norm': 5.690384864807129, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n"," 50% 3000/6000 [39:21<38:08,  1.31it/s][INFO|trainer.py:3944] 2025-03-10 18:44:37,838 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-10 18:44:37,840 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 18:44:37,841 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 18:44:43,390 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:44:43,394 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:44:43,394 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:44:43,394 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.7309, 'grad_norm': 8.432010650634766, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}\n"," 58% 3500/6000 [45:58<31:25,  1.33it/s][INFO|trainer.py:3944] 2025-03-10 18:51:15,660 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500\n","[INFO|configuration_utils.py:423] 2025-03-10 18:51:15,663 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 18:51:15,663 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 18:51:21,209 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:51:21,213 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:51:21,213 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:51:21,214 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.6624, 'grad_norm': 6.384344577789307, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 4000/6000 [52:34<26:04,  1.28it/s][INFO|trainer.py:3944] 2025-03-10 18:57:51,455 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000\n","[INFO|configuration_utils.py:423] 2025-03-10 18:57:51,457 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 18:57:51,458 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 18:57:56,944 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:57:56,947 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:57:56,947 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:57:56,948 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.2164, 'grad_norm': 6.924351692199707, 'learning_rate': 1.25e-05, 'epoch': 2.25}\n"," 75% 4500/6000 [59:11<18:50,  1.33it/s][INFO|trainer.py:3944] 2025-03-10 19:04:28,469 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500\n","[INFO|configuration_utils.py:423] 2025-03-10 19:04:28,471 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 19:04:28,472 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 19:04:34,039 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 19:04:34,042 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 19:04:34,043 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 19:04:34,043 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.2, 'grad_norm': 6.821500778198242, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"," 83% 5000/6000 [1:05:46<12:19,  1.35it/s][INFO|trainer.py:3944] 2025-03-10 19:11:03,349 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000\n","[INFO|configuration_utils.py:423] 2025-03-10 19:11:03,350 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 19:11:03,351 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 19:11:08,451 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 19:11:08,454 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 19:11:08,455 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 19:11:08,455 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.1951, 'grad_norm': 6.860134124755859, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}\n"," 92% 5500/6000 [1:12:19<06:01,  1.38it/s][INFO|trainer.py:3944] 2025-03-10 19:17:36,626 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500\n","[INFO|configuration_utils.py:423] 2025-03-10 19:17:36,628 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 19:17:36,629 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 19:17:41,776 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 19:17:41,779 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 19:17:41,780 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 19:17:41,780 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.1311, 'grad_norm': 6.98347282409668, 'learning_rate': 0.0, 'epoch': 3.0}\n","100% 6000/6000 [1:18:53<00:00,  1.36it/s][INFO|trainer.py:3944] 2025-03-10 19:24:10,314 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000\n","[INFO|configuration_utils.py:423] 2025-03-10 19:24:10,316 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 19:24:10,317 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 19:24:15,588 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 19:24:15,592 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 19:24:15,592 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 19:24:15,593 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-10 19:24:29,306 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 4753.5579, 'train_samples_per_second': 10.098, 'train_steps_per_second': 1.262, 'train_loss': 2.9179393717447915, 'epoch': 3.0}\n","100% 6000/6000 [1:19:12<00:00,  1.26it/s]\n","[INFO|trainer.py:3944] 2025-03-10 19:24:29,311 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000\n","[INFO|configuration_utils.py:423] 2025-03-10 19:24:29,313 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 19:24:29,314 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 19:24:38,219 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 19:24:38,222 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 19:24:38,223 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 19:24:38,223 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  3307792GF\n","  train_loss               =     2.9179\n","  train_runtime            = 1:19:13.55\n","  train_samples            =      16000\n","  train_samples_per_second =     10.098\n","  train_steps_per_second   =      1.262\n","03/10/2025 19:24:38 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-10 19:24:38,375 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-10 19:24:38,375 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-10 19:24:38,375 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:35<00:00,  4.60s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     6.8131\n","  eval_gen_len            =    46.3791\n","  eval_loss               =      2.566\n","  eval_runtime            = 0:09:44.12\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.707\n","  eval_steps_per_second   =      0.214\n","03/10/2025 19:34:22 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-10 19:34:22,502 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-10 19:34:22,503 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-10 19:34:22,503 >>   Batch size = 8\n","100% 127/127 [10:41<00:00,  5.05s/it]\n","***** predict metrics *****\n","  predict_bleu               =     6.3958\n","  predict_gen_len            =    49.4249\n","  predict_loss               =     2.6116\n","  predict_runtime            = 0:10:49.41\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.558\n","  predict_steps_per_second   =      0.196\n","[INFO|modelcard.py:449] 2025-03-10 19:45:27,886 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 6.8131}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_16000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-16000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet-32000\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"id":"UD9PUM4ysy9i","executionInfo":{"status":"ok","timestamp":1742394289165,"user_tz":-120,"elapsed":10973,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"122b6e06-2e2f-46f5-d94a-37674d1af1ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250319_142443-anyv4lg7</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/paderborn-university/machine%20translation/runs/anyv4lg7' target=\"_blank\">africomet-32000</a></strong> to <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/paderborn-university/machine%20translation/runs/anyv4lg7' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation/runs/anyv4lg7</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/anyv4lg7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x78d964654340>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_32000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-32000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E3PZdrUHtAaa","outputId":"9792e4b1-023b-439e-84e4-5df879b84e96","executionInfo":{"status":"ok","timestamp":1742407220463,"user_tz":-120,"elapsed":173800,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-19 14:24:58.849265: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-19 14:24:58.871520: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-19 14:24:58.878105: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-19 14:24:58.894592: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-19 14:24:59.918715: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/19/2025 14:25:02 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/19/2025 14:25:02 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-32000/runs/Mar19_14-25-02_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-32000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-32000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-93c76eb23e51e985\n","03/19/2025 14:25:02 - INFO - datasets.builder - Using custom data configuration default-93c76eb23e51e985\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/19/2025 14:25:02 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/19/2025 14:25:02 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/19/2025 14:25:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/19/2025 14:25:02 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/19/2025 14:25:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-19 14:25:02,679 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-19 14:25:02,680 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-19 14:25:02,762 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-19 14:25:02,763 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-19 14:25:02,764 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-19 14:25:02,764 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-19 14:25:02,764 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-19 14:25:02,764 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-19 14:25:02,764 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-19 14:25:02,764 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-19 14:25:02,764 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-19 14:25:02,764 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-19 14:25:02,765 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-19 14:25:03,630 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-19 14:25:03,700 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-19 14:25:03,748 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-19 14:25:03,805 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-19 14:25:03,806 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-19 14:25:03,892 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-19 14:25:03,892 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-19 14:25:04,092 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e7e4ea04dc2a8f5f.arrow\n","03/19/2025 14:25:05 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e7e4ea04dc2a8f5f.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-caa473c3d2478b37.arrow\n","03/19/2025 14:25:06 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-caa473c3d2478b37.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1700a6a31f02dd2e.arrow\n","03/19/2025 14:25:07 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1700a6a31f02dd2e.arrow\n","[INFO|trainer.py:2407] 2025-03-19 14:25:09,941 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-19 14:25:09,941 >>   Num examples = 32,000\n","[INFO|trainer.py:2409] 2025-03-19 14:25:09,941 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-19 14:25:09,941 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-19 14:25:09,941 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-19 14:25:09,941 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-19 14:25:09,941 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-19 14:25:09,941 >>   Total optimization steps = 12,000\n","[INFO|trainer.py:2416] 2025-03-19 14:25:09,943 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-19 14:25:09,948 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250319_142510-m63a8fd7\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-32000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/m63a8fd7\u001b[0m\n","  0% 0/12000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.3918, 'grad_norm': 6.957307815551758, 'learning_rate': 4.791666666666667e-05, 'epoch': 0.12}\n","  4% 500/12000 [05:48<2:13:39,  1.43it/s][INFO|trainer.py:3944] 2025-03-19 14:30:58,945 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-19 14:30:58,953 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 14:30:58,953 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 14:31:03,885 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 14:31:03,887 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 14:31:03,888 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 14:31:03,888 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.8956, 'grad_norm': 7.329458713531494, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}\n","  8% 1000/12000 [11:54<2:11:26,  1.39it/s][INFO|trainer.py:3944] 2025-03-19 14:37:05,210 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-19 14:37:05,212 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 14:37:05,213 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 14:37:16,980 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 14:37:16,984 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 14:37:16,985 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 14:37:16,985 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.6333, 'grad_norm': 6.621172904968262, 'learning_rate': 4.375e-05, 'epoch': 0.38}\n"," 12% 1500/12000 [18:14<1:59:19,  1.47it/s][INFO|trainer.py:3944] 2025-03-19 14:43:25,329 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-19 14:43:25,331 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 14:43:25,332 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 14:43:37,281 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 14:43:37,286 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 14:43:37,286 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 14:43:37,286 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.4492, 'grad_norm': 7.442023754119873, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n"," 17% 2000/12000 [24:34<1:54:11,  1.46it/s][INFO|trainer.py:3944] 2025-03-19 14:49:45,396 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-19 14:49:45,397 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 14:49:45,398 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 14:49:57,082 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 14:49:57,086 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 14:49:57,086 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 14:49:57,086 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.3131, 'grad_norm': 7.498073101043701, 'learning_rate': 3.958333333333333e-05, 'epoch': 0.62}\n"," 21% 2500/12000 [30:49<1:44:33,  1.51it/s][INFO|trainer.py:3944] 2025-03-19 14:56:00,592 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-19 14:56:00,594 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 14:56:00,595 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 14:56:05,606 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 14:56:05,609 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 14:56:05,610 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 14:56:05,610 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.217, 'grad_norm': 5.453154563903809, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}\n"," 25% 3000/12000 [36:54<1:44:39,  1.43it/s][INFO|trainer.py:3944] 2025-03-19 15:02:05,254 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-19 15:02:05,256 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 15:02:05,256 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 15:02:10,238 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 15:02:10,240 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 15:02:10,241 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 15:02:10,241 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.1427, 'grad_norm': 6.058298587799072, 'learning_rate': 3.541666666666667e-05, 'epoch': 0.88}\n"," 29% 3500/12000 [42:59<1:42:59,  1.38it/s][INFO|trainer.py:3944] 2025-03-19 15:08:10,750 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3500\n","[INFO|configuration_utils.py:423] 2025-03-19 15:08:10,752 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 15:08:10,752 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 15:08:15,663 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 15:08:15,667 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 15:08:15,668 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 15:08:15,668 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.0512, 'grad_norm': 6.179907321929932, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"," 33% 4000/12000 [49:04<1:36:06,  1.39it/s][INFO|trainer.py:3944] 2025-03-19 15:14:15,267 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4000\n","[INFO|configuration_utils.py:423] 2025-03-19 15:14:15,269 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 15:14:15,270 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 15:14:20,209 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 15:14:20,212 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 15:14:20,212 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 15:14:20,213 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.5636, 'grad_norm': 6.186031341552734, 'learning_rate': 3.125e-05, 'epoch': 1.12}\n"," 38% 4500/12000 [55:08<1:25:03,  1.47it/s][INFO|trainer.py:3944] 2025-03-19 15:20:19,422 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4500\n","[INFO|configuration_utils.py:423] 2025-03-19 15:20:19,424 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 15:20:19,425 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 15:20:24,328 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 15:20:24,332 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 15:20:24,333 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 15:20:24,333 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.5901, 'grad_norm': 6.246288299560547, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}\n"," 42% 5000/12000 [1:01:12<1:21:15,  1.44it/s][INFO|trainer.py:3944] 2025-03-19 15:26:22,940 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5000\n","[INFO|configuration_utils.py:423] 2025-03-19 15:26:22,942 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 15:26:22,943 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 15:26:27,883 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 15:26:27,886 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 15:26:27,886 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 15:26:27,887 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.575, 'grad_norm': 5.728704452514648, 'learning_rate': 2.7083333333333332e-05, 'epoch': 1.38}\n"," 46% 5500/12000 [1:07:16<1:19:14,  1.37it/s][INFO|trainer.py:3944] 2025-03-19 15:32:27,328 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5500\n","[INFO|configuration_utils.py:423] 2025-03-19 15:32:27,330 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 15:32:27,331 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 15:32:32,335 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 15:32:32,338 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 15:32:32,339 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 15:32:32,339 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.5084, 'grad_norm': 7.6965813636779785, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n"," 50% 6000/12000 [1:13:21<1:14:16,  1.35it/s][INFO|trainer.py:3944] 2025-03-19 15:38:32,208 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6000\n","[INFO|configuration_utils.py:423] 2025-03-19 15:38:32,211 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 15:38:32,211 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 15:38:37,196 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 15:38:37,200 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 15:38:37,200 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 15:38:37,201 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4954, 'grad_norm': 8.369145393371582, 'learning_rate': 2.2916666666666667e-05, 'epoch': 1.62}\n"," 54% 6500/12000 [1:19:26<1:00:22,  1.52it/s][INFO|trainer.py:3944] 2025-03-19 15:44:37,654 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6500\n","[INFO|configuration_utils.py:423] 2025-03-19 15:44:37,656 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 15:44:37,656 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 15:44:42,552 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 15:44:42,556 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 15:44:42,557 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 15:44:42,558 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4873, 'grad_norm': 5.348365783691406, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}\n"," 58% 7000/12000 [1:25:31<58:04,  1.43it/s][INFO|trainer.py:3944] 2025-03-19 15:50:42,732 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7000\n","[INFO|configuration_utils.py:423] 2025-03-19 15:50:42,734 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 15:50:42,734 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 15:50:47,678 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 15:50:47,683 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 15:50:47,683 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 15:50:47,684 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4512, 'grad_norm': 7.284200668334961, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.88}\n"," 62% 7500/12000 [1:31:38<53:04,  1.41it/s][INFO|trainer.py:3944] 2025-03-19 15:56:48,989 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7500\n","[INFO|configuration_utils.py:423] 2025-03-19 15:56:48,991 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 15:56:48,992 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 15:56:53,954 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 15:56:53,957 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 15:56:53,957 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 15:56:53,958 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.41, 'grad_norm': 6.396664142608643, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 8000/12000 [1:37:45<44:15,  1.51it/s][INFO|trainer.py:3944] 2025-03-19 16:02:56,037 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8000\n","[INFO|configuration_utils.py:423] 2025-03-19 16:02:56,039 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 16:02:56,040 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 16:03:00,956 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 16:03:00,959 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 16:03:00,959 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 16:03:00,960 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.0316, 'grad_norm': 7.27886438369751, 'learning_rate': 1.4583333333333335e-05, 'epoch': 2.12}\n"," 71% 8500/12000 [1:43:51<40:22,  1.44it/s][INFO|trainer.py:3944] 2025-03-19 16:09:02,315 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8500\n","[INFO|configuration_utils.py:423] 2025-03-19 16:09:02,317 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 16:09:02,317 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 16:09:07,372 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 16:09:07,375 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 16:09:07,376 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 16:09:07,376 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.0142, 'grad_norm': 5.559528350830078, 'learning_rate': 1.25e-05, 'epoch': 2.25}\n"," 75% 9000/12000 [1:49:56<33:37,  1.49it/s][INFO|trainer.py:3944] 2025-03-19 16:15:07,427 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9000\n","[INFO|configuration_utils.py:423] 2025-03-19 16:15:07,428 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 16:15:07,429 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 16:15:12,450 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 16:15:12,453 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 16:15:12,454 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 16:15:12,454 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.0344, 'grad_norm': 5.866454601287842, 'learning_rate': 1.0416666666666668e-05, 'epoch': 2.38}\n"," 79% 9500/12000 [1:56:02<29:02,  1.44it/s][INFO|trainer.py:3944] 2025-03-19 16:21:12,951 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9500\n","[INFO|configuration_utils.py:423] 2025-03-19 16:21:12,953 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 16:21:12,953 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 16:21:17,853 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 16:21:17,856 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 16:21:17,857 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 16:21:17,857 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.0277, 'grad_norm': 6.822623252868652, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"," 83% 10000/12000 [2:02:07<23:30,  1.42it/s][INFO|trainer.py:3944] 2025-03-19 16:27:17,892 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10000\n","[INFO|configuration_utils.py:423] 2025-03-19 16:27:17,894 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 16:27:17,894 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 16:27:22,865 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 16:27:22,868 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 16:27:22,869 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 16:27:22,869 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.988, 'grad_norm': 6.017353534698486, 'learning_rate': 6.25e-06, 'epoch': 2.62}\n"," 88% 10500/12000 [2:08:11<17:19,  1.44it/s][INFO|trainer.py:3944] 2025-03-19 16:33:22,576 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10500\n","[INFO|configuration_utils.py:423] 2025-03-19 16:33:22,578 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 16:33:22,578 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 16:33:27,543 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 16:33:27,546 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 16:33:27,546 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 16:33:27,547 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.0101, 'grad_norm': 5.1718244552612305, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}\n"," 92% 11000/12000 [2:14:17<12:08,  1.37it/s][INFO|trainer.py:3944] 2025-03-19 16:39:28,227 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11000\n","[INFO|configuration_utils.py:423] 2025-03-19 16:39:28,229 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 16:39:28,230 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 16:39:33,172 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 16:39:33,175 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 16:39:33,175 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 16:39:33,176 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9616, 'grad_norm': 7.736513137817383, 'learning_rate': 2.0833333333333334e-06, 'epoch': 2.88}\n"," 96% 11500/12000 [2:20:22<05:34,  1.49it/s][INFO|trainer.py:3944] 2025-03-19 16:45:33,115 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11500\n","[INFO|configuration_utils.py:423] 2025-03-19 16:45:33,117 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 16:45:33,117 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 16:45:38,063 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 16:45:38,066 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 16:45:38,066 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 16:45:38,067 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9758, 'grad_norm': 6.556053638458252, 'learning_rate': 0.0, 'epoch': 3.0}\n","100% 12000/12000 [2:26:28<00:00,  1.44it/s][INFO|trainer.py:3944] 2025-03-19 16:51:39,306 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-12000\n","[INFO|configuration_utils.py:423] 2025-03-19 16:51:39,308 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-12000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 16:51:39,309 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-12000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 16:51:44,858 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-12000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 16:51:44,861 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 16:51:44,862 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-12000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 16:51:44,863 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-12000/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-19 16:51:58,323 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 8808.3808, 'train_samples_per_second': 10.899, 'train_steps_per_second': 1.362, 'train_loss': 2.675770970662435, 'epoch': 3.0}\n","100% 12000/12000 [2:26:47<00:00,  1.36it/s]\n","[INFO|trainer.py:3944] 2025-03-19 16:51:58,333 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000\n","[INFO|configuration_utils.py:423] 2025-03-19 16:51:58,335 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-19 16:51:58,336 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-19 16:52:07,334 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-19 16:52:07,338 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-19 16:52:07,338 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-19 16:52:07,339 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  6605052GF\n","  train_loss               =     2.6758\n","  train_runtime            = 2:26:48.38\n","  train_samples            =      32000\n","  train_samples_per_second =     10.899\n","  train_steps_per_second   =      1.362\n","03/19/2025 16:52:07 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-19 16:52:07,509 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-19 16:52:07,509 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-19 16:52:07,510 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [08:55<00:00,  4.28s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     8.7598\n","  eval_gen_len            =    44.8044\n","  eval_loss               =     2.2391\n","  eval_runtime            = 0:09:03.58\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.834\n","  eval_steps_per_second   =       0.23\n","03/19/2025 17:01:11 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-19 17:01:11,095 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-19 17:01:11,095 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-19 17:01:11,095 >>   Batch size = 8\n","100% 127/127 [09:42<00:00,  4.59s/it]\n","***** predict metrics *****\n","  predict_bleu               =     8.3252\n","  predict_gen_len            =    46.4921\n","  predict_loss               =     2.2778\n","  predict_runtime            = 0:09:48.89\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.718\n","  predict_steps_per_second   =      0.216\n","[INFO|modelcard.py:449] 2025-03-19 17:11:18,287 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 8.7598}]}\n"]}]},{"cell_type":"markdown","metadata":{"id":"mNdDNvlQ9cRn"},"source":["# Comet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":633,"status":"ok","timestamp":1741636818439,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"hlcDxJYg9Zzn","outputId":"b5c36dd5-13ba-4680-fb50-b8687c9cee23"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"comet-1000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1772093,"status":"ok","timestamp":1741638594578,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"aIlSRCF9_CpV","outputId":"f39857c3-3a83-405d-9ac4-ea1c5d1d7fc4"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-10 20:00:25.748656: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-10 20:00:25.771067: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-10 20:00:25.778534: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-10 20:00:25.798099: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-10 20:00:26.864180: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/10/2025 20:00:29 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/10/2025 20:00:29 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-1000/runs/Mar10_20-00-29_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-1000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-1000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-d22542853ecd2369\n","03/10/2025 20:00:29 - INFO - datasets.builder - Using custom data configuration default-d22542853ecd2369\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/10/2025 20:00:29 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/10/2025 20:00:29 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 20:00:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/10/2025 20:00:29 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 20:00:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-10 20:00:29,901 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 20:00:29,902 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-10 20:00:30,221 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 20:00:30,221 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:00:30,222 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:00:30,222 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:00:30,222 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:00:30,222 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:00:30,222 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:00:30,222 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:00:30,222 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-10 20:00:30,223 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 20:00:30,224 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-10 20:00:31,399 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-10 20:00:31,468 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-10 20:00:31,579 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-10 20:00:31,580 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|safetensors_conversion.py:61] 2025-03-10 20:00:31,642 >> Attempting to create safetensors variant\n","[INFO|configuration_utils.py:1095] 2025-03-10 20:00:31,706 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-10 20:00:31,707 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-10 20:00:32,193 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e70b427f4acd1a90.arrow\n","03/10/2025 20:00:33 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e70b427f4acd1a90.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1e1e9cb1ba5e0af7.arrow\n","03/10/2025 20:00:34 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1e1e9cb1ba5e0af7.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b37e93e0dc9f95ce.arrow\n","03/10/2025 20:00:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b37e93e0dc9f95ce.arrow\n","[INFO|trainer.py:2407] 2025-03-10 20:00:38,315 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-10 20:00:38,315 >>   Num examples = 1,000\n","[INFO|trainer.py:2409] 2025-03-10 20:00:38,315 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-10 20:00:38,315 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-10 20:00:38,315 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-10 20:00:38,315 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-10 20:00:38,315 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-10 20:00:38,315 >>   Total optimization steps = 375\n","[INFO|trainer.py:2416] 2025-03-10 20:00:38,317 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-10 20:00:38,322 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_200038-n45fc45w\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-1000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/n45fc45w\u001b[0m\n","  0% 0/375 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 375/375 [04:41<00:00,  1.35it/s][INFO|trainer.py:3944] 2025-03-10 20:05:20,655 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-1000/checkpoint-375\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-10 20:05:20,658 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-1000/checkpoint-375/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 20:05:20,659 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-1000/checkpoint-375/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 20:05:25,922 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-1000/checkpoint-375/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 20:05:25,926 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-1000/checkpoint-375/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 20:05:25,927 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-1000/checkpoint-375/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 20:05:25,927 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-1000/checkpoint-375/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-10 20:05:39,686 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 301.3697, 'train_samples_per_second': 9.955, 'train_steps_per_second': 1.244, 'train_loss': 3.713399088541667, 'epoch': 3.0}\n","100% 375/375 [05:00<00:00,  1.25it/s]\n","[INFO|trainer.py:3944] 2025-03-10 20:05:39,689 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-1000\n","[INFO|configuration_utils.py:423] 2025-03-10 20:05:39,691 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 20:05:39,692 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 20:05:48,659 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 20:05:48,662 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 20:05:48,662 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 20:05:48,663 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-1000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =   205627GF\n","  train_loss               =     3.7134\n","  train_runtime            = 0:05:01.36\n","  train_samples            =       1000\n","  train_samples_per_second =      9.955\n","  train_steps_per_second   =      1.244\n","03/10/2025 20:05:48 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-10 20:05:48,818 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-10 20:05:48,818 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-10 20:05:48,819 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [11:15<00:00,  5.40s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     2.5499\n","  eval_gen_len            =    53.7031\n","  eval_loss               =     3.9645\n","  eval_runtime            = 0:11:23.63\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.458\n","  eval_steps_per_second   =      0.183\n","03/10/2025 20:17:12 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-10 20:17:12,462 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-10 20:17:12,462 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-10 20:17:12,463 >>   Batch size = 8\n","100% 127/127 [12:15<00:00,  5.79s/it]\n","***** predict metrics *****\n","  predict_bleu               =     1.9234\n","  predict_gen_len            =    55.8024\n","  predict_loss               =      4.005\n","  predict_runtime            = 0:12:22.95\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.362\n","  predict_steps_per_second   =      0.171\n","[INFO|modelcard.py:449] 2025-03-10 20:29:51,653 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 2.5499}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_1000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-1000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":466,"status":"ok","timestamp":1741638594602,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"SKfZPpED_kMg","outputId":"c9f94013-b9fe-4117-c862-dc734231776f"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"comet-2000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48428,"status":"ok","timestamp":1741643610174,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"W_RO5tzt_ZvK","outputId":"f8b2d08e-44e9-4291-fa92-3f5c1909c8f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-10 20:29:57.977307: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-10 20:29:58.001934: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-10 20:29:58.008946: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-10 20:29:58.027664: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-10 20:29:59.075888: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/10/2025 20:30:01 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/10/2025 20:30:01 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-2000/runs/Mar10_20-30-01_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-2000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-2000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-63fa3ea70d279152\n","03/10/2025 20:30:01 - INFO - datasets.builder - Using custom data configuration default-63fa3ea70d279152\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/10/2025 20:30:01 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/10/2025 20:30:01 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 20:30:01 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/10/2025 20:30:01 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 20:30:01 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-10 20:30:01,948 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 20:30:01,950 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-10 20:30:02,055 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 20:30:02,056 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:30:02,056 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:30:02,056 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:30:02,056 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:30:02,057 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:30:02,057 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:30:02,057 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:30:02,057 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-10 20:30:02,057 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 20:30:02,058 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-10 20:30:02,968 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-10 20:30:03,035 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-10 20:30:03,142 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-10 20:30:03,142 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-10 20:30:03,238 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-10 20:30:03,238 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-10 20:30:03,954 >> Attempting to create safetensors variant\n","[INFO|safetensors_conversion.py:74] 2025-03-10 20:30:04,414 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-239a0c8168036379.arrow\n","03/10/2025 20:30:04 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-239a0c8168036379.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4e37c8bc3d2bbba6.arrow\n","03/10/2025 20:30:05 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4e37c8bc3d2bbba6.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-283cd450680fa67d.arrow\n","03/10/2025 20:30:07 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-283cd450680fa67d.arrow\n","[INFO|trainer.py:2407] 2025-03-10 20:30:09,596 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-10 20:30:09,596 >>   Num examples = 2,000\n","[INFO|trainer.py:2409] 2025-03-10 20:30:09,596 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-10 20:30:09,596 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-10 20:30:09,596 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-10 20:30:09,596 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-10 20:30:09,596 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-10 20:30:09,596 >>   Total optimization steps = 750\n","[INFO|trainer.py:2416] 2025-03-10 20:30:09,598 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-10 20:30:09,603 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_203009-l8ju2v1r\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-2000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/l8ju2v1r\u001b[0m\n","  0% 0/750 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.007, 'grad_norm': 9.431608200073242, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 500/750 [06:15<03:05,  1.34it/s][INFO|trainer.py:3944] 2025-03-10 20:36:26,493 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-2000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-10 20:36:26,496 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 20:36:26,497 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 20:36:31,570 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 20:36:31,575 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 20:36:31,576 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 20:36:31,576 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 750/750 [09:41<00:00,  1.35it/s][INFO|trainer.py:3944] 2025-03-10 20:39:52,523 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-2000/checkpoint-750\n","[INFO|configuration_utils.py:423] 2025-03-10 20:39:52,525 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-750/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 20:39:52,526 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-750/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 20:39:57,871 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-750/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 20:39:57,874 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-750/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 20:39:57,875 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-750/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 20:39:57,875 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-750/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-10 20:40:11,511 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 601.9137, 'train_samples_per_second': 9.968, 'train_steps_per_second': 1.246, 'train_loss': 3.600841959635417, 'epoch': 3.0}\n","100% 750/750 [10:00<00:00,  1.25it/s]\n","[INFO|trainer.py:3944] 2025-03-10 20:40:11,523 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-2000\n","[INFO|configuration_utils.py:423] 2025-03-10 20:40:11,524 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 20:40:11,525 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 20:40:20,440 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 20:40:20,444 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 20:40:20,444 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 20:40:20,445 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-2000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =   409031GF\n","  train_loss               =     3.6008\n","  train_runtime            = 0:10:01.91\n","  train_samples            =       2000\n","  train_samples_per_second =      9.968\n","  train_steps_per_second   =      1.246\n","03/10/2025 20:40:20 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-10 20:40:20,610 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-10 20:40:20,610 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-10 20:40:20,610 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [12:50<00:00,  6.16s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     2.3401\n","  eval_gen_len            =    56.6901\n","  eval_loss               =     3.7136\n","  eval_runtime            = 0:12:59.20\n","  eval_samples            =        997\n","  eval_samples_per_second =       1.28\n","  eval_steps_per_second   =       0.16\n","03/10/2025 20:53:19 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-10 20:53:19,820 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-10 20:53:19,820 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-10 20:53:19,821 >>   Batch size = 8\n","100% 127/127 [13:06<00:00,  6.19s/it]\n","***** predict metrics *****\n","  predict_bleu               =     2.2356\n","  predict_gen_len            =     57.754\n","  predict_loss               =      3.759\n","  predict_runtime            = 0:13:14.66\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.273\n","  predict_steps_per_second   =       0.16\n","[INFO|modelcard.py:449] 2025-03-10 21:06:51,407 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 2.3401}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_2000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-2000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"executionInfo":{"elapsed":9814,"status":"ok","timestamp":1741674498496,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"-sq2Lrua_oqR","outputId":"a9b06821-daf1-4f76-fa6d-d153c5f92a33"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.19.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20250311_062813-fax4wneh</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/paderborn-university/machine%20translation/runs/fax4wneh' target=\"_blank\">comet-4000</a></strong> to <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/paderborn-university/machine%20translation/runs/fax4wneh' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation/runs/fax4wneh</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/fax4wneh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x78058efec220>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"comet-4000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2620464,"status":"ok","timestamp":1741677124495,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"A9KnDDS6_ynN","outputId":"b555921b-b606-4094-ef28-6d41bbd4eb4d"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-11 06:28:31.262500: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-11 06:28:31.285008: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-11 06:28:31.291381: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-11 06:28:31.307241: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-11 06:28:32.284236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/11/2025 06:28:37 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/11/2025 06:28:37 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-4000/runs/Mar11_06-28-37_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-4000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-4000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-2c18f5caf6d15f80\n","03/11/2025 06:28:37 - INFO - datasets.builder - Using custom data configuration default-2c18f5caf6d15f80\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/11/2025 06:28:37 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/11/2025 06:28:37 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/11/2025 06:28:37 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/11/2025 06:28:37 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/11/2025 06:28:37 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-11 06:28:38,075 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-11 06:28:38,077 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-11 06:28:38,167 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-11 06:28:38,168 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 06:28:38,169 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 06:28:38,169 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 06:28:38,169 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 06:28:38,169 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 06:28:38,169 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 06:28:38,169 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 06:28:38,169 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-11 06:28:38,170 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-11 06:28:38,170 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-11 06:28:39,133 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|safetensors_conversion.py:61] 2025-03-11 06:28:39,255 >> Attempting to create safetensors variant\n","[INFO|safetensors_conversion.py:74] 2025-03-11 06:28:39,616 >> Safetensors PR exists\n","[INFO|configuration_utils.py:1140] 2025-03-11 06:28:39,638 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-11 06:28:39,817 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-11 06:28:39,817 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-11 06:28:40,156 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-11 06:28:40,157 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b3e5c8b83b5f5d81.arrow\n","03/11/2025 06:28:41 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b3e5c8b83b5f5d81.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d80ea2e702926f3c.arrow\n","03/11/2025 06:28:42 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d80ea2e702926f3c.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-94a06ef0cd1537df.arrow\n","03/11/2025 06:28:44 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-94a06ef0cd1537df.arrow\n","[INFO|trainer.py:2407] 2025-03-11 06:28:53,391 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-11 06:28:53,391 >>   Num examples = 4,000\n","[INFO|trainer.py:2409] 2025-03-11 06:28:53,391 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-11 06:28:53,391 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-11 06:28:53,391 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-11 06:28:53,391 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-11 06:28:53,392 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-11 06:28:53,392 >>   Total optimization steps = 1,500\n","[INFO|trainer.py:2416] 2025-03-11 06:28:53,393 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-11 06:28:53,400 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250311_062853-rva29lg3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-4000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/rva29lg3\u001b[0m\n","  0% 0/1500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.3891, 'grad_norm': 6.706293106079102, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"," 33% 500/1500 [06:06<12:25,  1.34it/s][INFO|trainer.py:3944] 2025-03-11 06:35:00,452 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-4000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-11 06:35:00,458 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 06:35:00,459 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 06:35:11,923 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 06:35:11,927 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 06:35:11,927 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 06:35:11,928 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.2881, 'grad_norm': 7.618316650390625, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 1000/1500 [12:41<06:35,  1.26it/s][INFO|trainer.py:3944] 2025-03-11 06:41:35,339 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-11 06:41:35,340 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 06:41:35,341 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 06:41:40,189 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 06:41:40,191 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 06:41:40,192 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 06:41:40,192 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.6393, 'grad_norm': 10.235444068908691, 'learning_rate': 0.0, 'epoch': 3.0}\n","100% 1500/1500 [19:04<00:00,  1.40it/s][INFO|trainer.py:3944] 2025-03-11 06:47:59,308 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-11 06:47:59,313 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 06:47:59,314 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 06:48:04,287 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 06:48:04,290 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 06:48:04,291 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 06:48:04,291 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1500/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-11 06:48:18,105 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 1164.713, 'train_samples_per_second': 10.303, 'train_steps_per_second': 1.288, 'train_loss': 3.438846923828125, 'epoch': 3.0}\n","100% 1500/1500 [19:23<00:00,  1.29it/s]\n","[INFO|trainer.py:3944] 2025-03-11 06:48:18,110 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-4000\n","[INFO|configuration_utils.py:423] 2025-03-11 06:48:18,199 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-4000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 06:48:18,200 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-4000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 06:48:32,969 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 06:48:32,973 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 06:48:32,973 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-4000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 06:48:32,974 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-4000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =   849267GF\n","  train_loss               =     3.4388\n","  train_runtime            = 0:19:24.71\n","  train_samples            =       4000\n","  train_samples_per_second =     10.303\n","  train_steps_per_second   =      1.288\n","03/11/2025 06:48:33 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-11 06:48:33,119 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-11 06:48:33,120 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-11 06:48:33,120 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [11:12<00:00,  5.38s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     3.4049\n","  eval_gen_len            =    50.9759\n","  eval_loss               =     3.3535\n","  eval_runtime            = 0:11:19.80\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.467\n","  eval_steps_per_second   =      0.184\n","03/11/2025 06:59:52 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-11 06:59:52,932 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-11 06:59:52,932 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-11 06:59:52,932 >>   Batch size = 8\n","100% 127/127 [11:45<00:00,  5.55s/it]\n","***** predict metrics *****\n","  predict_bleu               =     3.3486\n","  predict_gen_len            =    52.8893\n","  predict_loss               =     3.4086\n","  predict_runtime            = 0:11:52.53\n","  predict_samples            =       1012\n","  predict_samples_per_second =       1.42\n","  predict_steps_per_second   =      0.178\n","[INFO|modelcard.py:449] 2025-03-11 07:12:01,170 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 3.4049}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_4000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-4000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":335,"status":"ok","timestamp":1741677124497,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"zgBkTtEQ_qHT","outputId":"cefc32f0-a820-4c4b-b2f7-0483f139a3b9"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/fax4wneh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x78058efec220>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"comet-8000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2226696,"status":"ok","timestamp":1741680683851,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"XnLgBK_Q_0Nt","outputId":"743a55c6-529b-4968-b687-ec5c450364c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-11 07:12:15.544791: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-11 07:12:15.566555: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-11 07:12:15.573051: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-11 07:12:15.589312: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-11 07:12:16.661912: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/11/2025 07:12:21 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/11/2025 07:12:21 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-8000/runs/Mar11_07-12-21_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-8000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-8000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-5bc63a9eaf840ac0\n","03/11/2025 07:12:21 - INFO - datasets.builder - Using custom data configuration default-5bc63a9eaf840ac0\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/11/2025 07:12:21 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/11/2025 07:12:21 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/11/2025 07:12:21 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/11/2025 07:12:21 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/11/2025 07:12:21 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-11 07:12:22,204 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-11 07:12:22,206 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-11 07:12:22,312 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-11 07:12:22,313 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-11 07:12:22,315 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-11 07:12:22,315 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-11 07:12:23,259 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-11 07:12:23,339 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-11 07:12:23,447 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-11 07:12:23,447 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|safetensors_conversion.py:61] 2025-03-11 07:12:23,553 >> Attempting to create safetensors variant\n","[INFO|configuration_utils.py:1095] 2025-03-11 07:12:23,732 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-11 07:12:23,733 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-11 07:12:24,483 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-805d80dbaf682edc.arrow\n","03/11/2025 07:12:25 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-805d80dbaf682edc.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-8d35743ea3961495.arrow\n","03/11/2025 07:12:26 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-8d35743ea3961495.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a687ca1a7df8988f.arrow\n","03/11/2025 07:12:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a687ca1a7df8988f.arrow\n","[INFO|trainer.py:2407] 2025-03-11 07:12:37,293 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-11 07:12:37,293 >>   Num examples = 8,000\n","[INFO|trainer.py:2409] 2025-03-11 07:12:37,293 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-11 07:12:37,293 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-11 07:12:37,293 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-11 07:12:37,293 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-11 07:12:37,293 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-11 07:12:37,293 >>   Total optimization steps = 3,000\n","[INFO|trainer.py:2416] 2025-03-11 07:12:37,294 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-11 07:12:37,301 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250311_071237-d7dmi3j0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-8000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/d7dmi3j0\u001b[0m\n","  0% 0/3000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.3577, 'grad_norm': 8.726241111755371, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n"," 17% 500/3000 [05:59<29:48,  1.40it/s][INFO|trainer.py:3944] 2025-03-11 07:18:38,000 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-11 07:18:38,012 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 07:18:38,012 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 07:18:49,502 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:18:49,505 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:18:49,506 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:18:49,506 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.8395, 'grad_norm': 8.096263885498047, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"," 33% 1000/3000 [12:30<24:33,  1.36it/s][INFO|trainer.py:3944] 2025-03-11 07:25:08,462 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-11 07:25:08,464 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 07:25:08,465 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 07:25:13,388 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:25:13,391 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:25:13,392 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:25:13,392 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.045, 'grad_norm': 7.830597400665283, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n"," 50% 1500/3000 [18:47<18:04,  1.38it/s][INFO|trainer.py:3944] 2025-03-11 07:31:25,260 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-11 07:31:25,262 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 07:31:25,263 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 07:31:30,226 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:31:30,229 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:31:30,230 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:31:30,230 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.9503, 'grad_norm': 7.9375739097595215, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 2000/3000 [25:05<11:27,  1.45it/s][INFO|trainer.py:3944] 2025-03-11 07:37:44,118 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-11 07:37:44,120 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 07:37:44,121 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 07:37:49,103 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:37:49,107 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:37:49,107 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:37:49,108 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4077, 'grad_norm': 8.3837308883667, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"," 83% 2500/3000 [31:22<05:56,  1.40it/s][INFO|trainer.py:3944] 2025-03-11 07:44:00,670 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-11 07:44:00,673 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 07:44:00,674 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 07:44:05,567 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:44:05,570 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:44:05,570 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:44:05,571 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.359, 'grad_norm': 7.027705192565918, 'learning_rate': 0.0, 'epoch': 3.0}\n","100% 3000/3000 [37:42<00:00,  1.38it/s][INFO|trainer.py:3944] 2025-03-11 07:50:20,600 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-11 07:50:20,602 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 07:50:20,602 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 07:50:25,487 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:50:25,489 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:50:25,490 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:50:25,490 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-11 07:50:39,111 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2281.8171, 'train_samples_per_second': 10.518, 'train_steps_per_second': 1.315, 'train_loss': 3.159888224283854, 'epoch': 3.0}\n","100% 3000/3000 [38:00<00:00,  1.32it/s]\n","[INFO|trainer.py:3944] 2025-03-11 07:50:39,116 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000\n","[INFO|configuration_utils.py:423] 2025-03-11 07:50:39,120 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 07:50:39,169 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 07:50:54,302 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:50:54,306 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:50:54,307 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:50:54,309 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  1672691GF\n","  train_loss               =     3.1599\n","  train_runtime            = 0:38:01.81\n","  train_samples            =       8000\n","  train_samples_per_second =     10.518\n","  train_steps_per_second   =      1.315\n","03/11/2025 07:50:54 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-11 07:50:54,447 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-11 07:50:54,447 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-11 07:50:54,447 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:24<00:00,  4.52s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     5.5443\n","  eval_gen_len            =    47.0752\n","  eval_loss               =     2.9433\n","  eval_runtime            = 0:09:30.03\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.749\n","  eval_steps_per_second   =      0.219\n","03/11/2025 08:00:24 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-11 08:00:24,491 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-11 08:00:24,492 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-11 08:00:24,492 >>   Batch size = 8\n","100% 127/127 [10:32<00:00,  4.98s/it]\n","***** predict metrics *****\n","  predict_bleu               =     4.8857\n","  predict_gen_len            =    49.6897\n","  predict_loss               =     3.0054\n","  predict_runtime            = 0:10:40.04\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.581\n","  predict_steps_per_second   =      0.198\n","[INFO|modelcard.py:449] 2025-03-11 08:11:20,174 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 5.5443}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_8000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-8000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"executionInfo":{"elapsed":11410,"status":"ok","timestamp":1741682663701,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"38xgPP_G_rjq","outputId":"c92baf51-4e22-401b-b4ab-ef5f48f8bb19"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.19.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20250311_084418-vjxkfu79</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/paderborn-university/machine%20translation/runs/vjxkfu79' target=\"_blank\">comet-16000</a></strong> to <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/paderborn-university/machine%20translation/runs/vjxkfu79' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation/runs/vjxkfu79</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/vjxkfu79?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x79baa0d2c430>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"comet-16000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1983098,"status":"ok","timestamp":1741688608186,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"a8A_KBlC_2Q_","outputId":"16d9b74c-8102-4bf9-df55-eaf28a670771"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-11 08:45:17.490707: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-11 08:45:17.512410: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-11 08:45:17.519113: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-11 08:45:17.535362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-11 08:45:18.555378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/11/2025 08:45:23 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/11/2025 08:45:23 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-16000/runs/Mar11_08-45-23_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-16000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-16000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-8dd97eb64194ed63\n","03/11/2025 08:45:24 - INFO - datasets.builder - Using custom data configuration default-8dd97eb64194ed63\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/11/2025 08:45:24 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/11/2025 08:45:24 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/11/2025 08:45:24 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/11/2025 08:45:24 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/11/2025 08:45:24 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-11 08:45:24,413 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-11 08:45:24,416 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-11 08:45:24,503 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-11 08:45:24,504 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-11 08:45:24,506 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-11 08:45:24,507 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-11 08:45:25,474 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|safetensors_conversion.py:61] 2025-03-11 08:45:25,779 >> Attempting to create safetensors variant\n","[INFO|configuration_utils.py:1140] 2025-03-11 08:45:25,983 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-11 08:45:26,161 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-11 08:45:26,161 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-11 08:45:26,289 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-11 08:45:26,289 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-11 08:45:26,435 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4f73fd23cbbc2044.arrow\n","03/11/2025 08:45:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4f73fd23cbbc2044.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-61f573cc25eca874.arrow\n","03/11/2025 08:45:28 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-61f573cc25eca874.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-29c03a83c2242779.arrow\n","03/11/2025 08:45:30 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-29c03a83c2242779.arrow\n","[INFO|trainer.py:2407] 2025-03-11 08:45:39,481 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-11 08:45:39,481 >>   Num examples = 16,000\n","[INFO|trainer.py:2409] 2025-03-11 08:45:39,481 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-11 08:45:39,481 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-11 08:45:39,481 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-11 08:45:39,481 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-11 08:45:39,481 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-11 08:45:39,482 >>   Total optimization steps = 6,000\n","[INFO|trainer.py:2416] 2025-03-11 08:45:39,483 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-11 08:45:39,490 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250311_084539-hctutu3t\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-16000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/hctutu3t\u001b[0m\n","  0% 0/6000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.4124, 'grad_norm': 7.936241149902344, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}\n","  8% 500/6000 [06:05<1:05:10,  1.41it/s][INFO|trainer.py:3944] 2025-03-11 08:51:45,992 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-11 08:51:45,998 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 08:51:45,999 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 08:51:57,453 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 08:51:57,457 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 08:51:57,457 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 08:51:57,458 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.8947, 'grad_norm': 6.913146018981934, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n"," 17% 1000/6000 [12:42<1:03:27,  1.31it/s][INFO|trainer.py:3944] 2025-03-11 08:58:22,858 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-11 08:58:22,860 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 08:58:22,861 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 08:58:27,705 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 08:58:27,709 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 08:58:27,709 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 08:58:27,710 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.5953, 'grad_norm': 7.811131954193115, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}\n"," 25% 1500/6000 [19:05<55:15,  1.36it/s][INFO|trainer.py:3944] 2025-03-11 09:04:46,081 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-11 09:04:46,083 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:04:46,084 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:04:50,966 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:04:50,969 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:04:50,969 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:04:50,970 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.4118, 'grad_norm': 6.7470479011535645, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"," 33% 2000/6000 [25:30<48:39,  1.37it/s][INFO|trainer.py:3944] 2025-03-11 09:11:11,353 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-11 09:11:11,354 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:11:11,355 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:11:16,272 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:11:16,276 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:11:16,276 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:11:16,276 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.8207, 'grad_norm': 7.763012409210205, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}\n"," 42% 2500/6000 [31:54<42:08,  1.38it/s][INFO|trainer.py:3944] 2025-03-11 09:17:34,481 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-11 09:17:34,482 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:17:34,483 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:17:39,362 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:17:39,365 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:17:39,365 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:17:39,366 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.7635, 'grad_norm': 6.0602593421936035, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n"," 50% 3000/6000 [38:18<36:51,  1.36it/s][INFO|trainer.py:3944] 2025-03-11 09:23:59,149 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-11 09:23:59,151 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:23:59,151 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:24:04,056 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:24:04,059 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:24:04,059 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:24:04,060 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.7337, 'grad_norm': 7.55765962600708, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}\n"," 58% 3500/6000 [44:44<30:26,  1.37it/s][INFO|trainer.py:3944] 2025-03-11 09:30:24,508 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500\n","[INFO|configuration_utils.py:423] 2025-03-11 09:30:24,509 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:30:24,510 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:30:29,367 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:30:29,370 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:30:29,371 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:30:29,371 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.6668, 'grad_norm': 6.23962926864624, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 4000/6000 [51:07<25:24,  1.31it/s][INFO|trainer.py:3944] 2025-03-11 09:36:47,496 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000\n","[INFO|configuration_utils.py:423] 2025-03-11 09:36:47,498 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:36:47,498 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:36:52,406 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:36:52,409 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:36:52,409 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:36:52,410 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.2207, 'grad_norm': 6.355872631072998, 'learning_rate': 1.25e-05, 'epoch': 2.25}\n"," 75% 4500/6000 [57:31<18:13,  1.37it/s][INFO|trainer.py:3944] 2025-03-11 09:43:12,085 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500\n","[INFO|configuration_utils.py:423] 2025-03-11 09:43:12,087 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:43:12,088 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:43:17,005 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:43:17,008 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:43:17,009 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:43:17,009 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.2021, 'grad_norm': 7.565613746643066, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"," 83% 5000/6000 [1:03:55<12:06,  1.38it/s][INFO|trainer.py:3944] 2025-03-11 09:49:36,015 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000\n","[INFO|configuration_utils.py:423] 2025-03-11 09:49:36,017 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:49:36,017 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:49:40,929 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:49:40,932 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:49:40,933 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:49:40,933 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.1973, 'grad_norm': 14.712209701538086, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}\n"," 92% 5500/6000 [1:10:18<05:52,  1.42it/s][INFO|trainer.py:3944] 2025-03-11 09:55:59,282 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500\n","[INFO|configuration_utils.py:423] 2025-03-11 09:55:59,284 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 09:55:59,285 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 09:56:04,201 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:56:04,204 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:56:04,204 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:56:04,205 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.1321, 'grad_norm': 7.183525085449219, 'learning_rate': 0.0, 'epoch': 3.0}\n","100% 6000/6000 [1:16:43<00:00,  1.38it/s][INFO|trainer.py:3944] 2025-03-11 10:02:23,522 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000\n","[INFO|configuration_utils.py:423] 2025-03-11 10:02:23,525 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 10:02:23,526 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 10:02:28,451 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 10:02:28,454 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 10:02:28,454 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 10:02:28,455 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-11 10:02:42,100 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 4622.6168, 'train_samples_per_second': 10.384, 'train_steps_per_second': 1.298, 'train_loss': 2.920919514973958, 'epoch': 3.0}\n","100% 6000/6000 [1:17:01<00:00,  1.30it/s]\n","[INFO|trainer.py:3944] 2025-03-11 10:02:42,110 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000\n","[INFO|configuration_utils.py:423] 2025-03-11 10:02:42,137 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-11 10:02:42,138 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-11 10:02:57,056 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-11 10:02:57,060 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-11 10:02:57,061 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-11 10:02:57,061 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  3307792GF\n","  train_loss               =     2.9209\n","  train_runtime            = 1:17:02.61\n","  train_samples            =      16000\n","  train_samples_per_second =     10.384\n","  train_steps_per_second   =      1.298\n","03/11/2025 10:02:57 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-11 10:02:57,210 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-11 10:02:57,211 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-11 10:02:57,211 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:38<00:00,  4.63s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     6.7478\n","  eval_gen_len            =    46.4132\n","  eval_loss               =     2.5626\n","  eval_runtime            = 0:09:46.79\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.699\n","  eval_steps_per_second   =      0.213\n","03/11/2025 10:12:44 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-11 10:12:44,006 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-11 10:12:44,006 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-11 10:12:44,006 >>   Batch size = 8\n","100% 127/127 [10:18<00:00,  4.87s/it]\n","***** predict metrics *****\n","  predict_bleu               =     6.2998\n","  predict_gen_len            =    48.8399\n","  predict_loss               =     2.6004\n","  predict_runtime            = 0:10:25.58\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.618\n","  predict_steps_per_second   =      0.203\n","[INFO|modelcard.py:449] 2025-03-11 10:23:25,008 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 6.7478}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_16000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-16000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wslf2Suxpp-R"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}
=======
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-_Z-PYprlsd"
      },
      "source": [
        "# Leveraging Bitext mining and COMET-QE for improving parallel data selection of low-resource machine translation\n",
        "<a href=\"https://colab.research.google.com/github/emmanuelayanful/AIMS-NLP-Project/blob/main/M2M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqsa3QW1weGy"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ICCsdcJZpub"
      },
      "outputs": [],
      "source": [
        "# !rm -rf M2M-100/ wandb/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZQLQPZjy2QO"
      },
      "source": [
        "# Baseline Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "aggyfHIUR2zp",
        "outputId": "2d6920f7-eda9-447a-ef8f-12ed3c667da5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250310_141809-2aublbbj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj' target=\"_blank\">baseline</a></strong> to <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Log in with your API key\n",
        "wandb.login(key=\"7f13d9fe09d0856f7c12099a27ccda7aa15c8afd\")\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.init(project=\"machine translation\", name=\"baseline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvM-Zf4jUO81",
        "outputId": "f0790cb9-ef48-43a0-9f4a-89e380020650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-10 14:18:16.490271: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-03-10 14:18:16.513278: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-03-10 14:18:16.520425: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-10 14:18:16.537626: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-10 14:18:17.613945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/10/2025 14:18:20 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
            "03/10/2025 14:18:20 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=2,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=M2M-100/baseline/runs/Mar10_14-18-19_bcdd196dbaf2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=M2M-100/baseline,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=M2M-100/baseline,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tp_size=0,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-fcf11d994dda434e\n",
            "03/10/2025 14:18:20 - INFO - datasets.builder - Using custom data configuration default-fcf11d994dda434e\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "03/10/2025 14:18:20 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "03/10/2025 14:18:20 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/10/2025 14:18:20 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "03/10/2025 14:18:20 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/10/2025 14:18:20 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 14:18:20,481 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 14:18:20,483 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 14:18:20,581 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 14:18:20,582 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 14:18:20,583 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 14:18:20,583 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 14:18:20,583 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 14:18:20,583 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 14:18:20,583 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 14:18:20,583 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 14:18:20,583 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 14:18:20,584 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 14:18:20,584 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3984] 2025-03-10 14:18:21,486 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:1140] 2025-03-10 14:18:21,552 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:61] 2025-03-10 14:18:21,597 >> Attempting to create safetensors variant\n",
            "[INFO|modeling_utils.py:4972] 2025-03-10 14:18:21,661 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4980] 2025-03-10 14:18:21,661 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
            "[INFO|configuration_utils.py:1095] 2025-03-10 14:18:21,756 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-03-10 14:18:21,756 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:74] 2025-03-10 14:18:21,975 >> Safetensors PR exists\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-500813e1b41c1f29.arrow\n",
            "03/10/2025 14:18:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-500813e1b41c1f29.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-24d83da570939502.arrow\n",
            "03/10/2025 14:18:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-24d83da570939502.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c4510aa2c36da9e3.arrow\n",
            "03/10/2025 14:18:25 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c4510aa2c36da9e3.arrow\n",
            "[INFO|trainer.py:2407] 2025-03-10 14:18:28,106 >> ***** Running training *****\n",
            "[INFO|trainer.py:2408] 2025-03-10 14:18:28,106 >>   Num examples = 5,737\n",
            "[INFO|trainer.py:2409] 2025-03-10 14:18:28,106 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2410] 2025-03-10 14:18:28,106 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:2412] 2025-03-10 14:18:28,106 >>   Training with DataParallel so batch size has been adjusted to: 8\n",
            "[INFO|trainer.py:2413] 2025-03-10 14:18:28,106 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2414] 2025-03-10 14:18:28,106 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2415] 2025-03-10 14:18:28,106 >>   Total optimization steps = 2,154\n",
            "[INFO|trainer.py:2416] 2025-03-10 14:18:28,107 >>   Number of trainable parameters = 483,905,536\n",
            "[INFO|integration_utils.py:817] 2025-03-10 14:18:28,113 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_141828-ulsf7cg9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/baseline\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/ulsf7cg9\u001b[0m\n",
            "  0% 0/2154 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 3.642, 'grad_norm': 6.3416852951049805, 'learning_rate': 3.8393686165273915e-05, 'epoch': 0.7}\n",
            " 23% 500/2154 [07:30<23:59,  1.15it/s][INFO|trainer.py:3944] 2025-03-10 14:25:59,660 >> Saving model checkpoint to M2M-100/baseline/checkpoint-500\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 14:25:59,664 >> Configuration saved in M2M-100/baseline/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 14:25:59,664 >> Configuration saved in M2M-100/baseline/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 14:26:04,580 >> Model weights saved in M2M-100/baseline/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 14:26:04,583 >> tokenizer config file saved in M2M-100/baseline/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 14:26:04,584 >> Special tokens file saved in M2M-100/baseline/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 14:26:04,584 >> added tokens file saved in M2M-100/baseline/checkpoint-500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.781, 'grad_norm': 6.211764812469482, 'learning_rate': 2.678737233054782e-05, 'epoch': 1.39}\n",
            " 46% 1000/2154 [15:22<16:02,  1.20it/s][INFO|trainer.py:3944] 2025-03-10 14:33:51,805 >> Saving model checkpoint to M2M-100/baseline/checkpoint-1000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 14:33:51,807 >> Configuration saved in M2M-100/baseline/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 14:33:51,807 >> Configuration saved in M2M-100/baseline/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 14:33:56,812 >> Model weights saved in M2M-100/baseline/checkpoint-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 14:33:56,818 >> tokenizer config file saved in M2M-100/baseline/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 14:33:56,819 >> Special tokens file saved in M2M-100/baseline/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 14:33:56,819 >> added tokens file saved in M2M-100/baseline/checkpoint-1000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.3355, 'grad_norm': 5.986948490142822, 'learning_rate': 1.518105849582173e-05, 'epoch': 2.09}\n",
            " 70% 1500/2154 [23:16<09:25,  1.16it/s][INFO|trainer.py:3944] 2025-03-10 14:41:44,981 >> Saving model checkpoint to M2M-100/baseline/checkpoint-1500\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 14:41:44,982 >> Configuration saved in M2M-100/baseline/checkpoint-1500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 14:41:44,983 >> Configuration saved in M2M-100/baseline/checkpoint-1500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 14:41:49,973 >> Model weights saved in M2M-100/baseline/checkpoint-1500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 14:41:49,977 >> tokenizer config file saved in M2M-100/baseline/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 14:41:49,977 >> Special tokens file saved in M2M-100/baseline/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 14:41:49,978 >> added tokens file saved in M2M-100/baseline/checkpoint-1500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 1.9662, 'grad_norm': 6.2669148445129395, 'learning_rate': 3.574744661095636e-06, 'epoch': 2.79}\n",
            " 93% 2000/2154 [31:07<02:20,  1.10it/s][INFO|trainer.py:3944] 2025-03-10 14:49:36,623 >> Saving model checkpoint to M2M-100/baseline/checkpoint-2000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 14:49:36,624 >> Configuration saved in M2M-100/baseline/checkpoint-2000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 14:49:36,625 >> Configuration saved in M2M-100/baseline/checkpoint-2000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 14:49:41,557 >> Model weights saved in M2M-100/baseline/checkpoint-2000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 14:49:41,561 >> tokenizer config file saved in M2M-100/baseline/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 14:49:41,561 >> Special tokens file saved in M2M-100/baseline/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 14:49:41,562 >> added tokens file saved in M2M-100/baseline/checkpoint-2000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "100% 2154/2154 [33:44<00:00,  1.50it/s][INFO|trainer.py:3944] 2025-03-10 14:52:13,206 >> Saving model checkpoint to M2M-100/baseline/checkpoint-2154\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 14:52:13,208 >> Configuration saved in M2M-100/baseline/checkpoint-2154/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 14:52:13,208 >> Configuration saved in M2M-100/baseline/checkpoint-2154/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 14:52:18,268 >> Model weights saved in M2M-100/baseline/checkpoint-2154/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 14:52:18,270 >> tokenizer config file saved in M2M-100/baseline/checkpoint-2154/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 14:52:18,271 >> Special tokens file saved in M2M-100/baseline/checkpoint-2154/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 14:52:18,271 >> added tokens file saved in M2M-100/baseline/checkpoint-2154/added_tokens.json\n",
            "[INFO|trainer.py:2659] 2025-03-10 14:52:31,891 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2043.7839, 'train_samples_per_second': 8.421, 'train_steps_per_second': 1.054, 'train_loss': 2.626034424054811, 'epoch': 3.0}\n",
            "100% 2154/2154 [34:02<00:00,  1.05it/s]\n",
            "[INFO|trainer.py:3944] 2025-03-10 14:52:31,901 >> Saving model checkpoint to M2M-100/baseline\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 14:52:31,902 >> Configuration saved in M2M-100/baseline/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 14:52:31,903 >> Configuration saved in M2M-100/baseline/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 14:52:40,831 >> Model weights saved in M2M-100/baseline/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 14:52:40,833 >> tokenizer config file saved in M2M-100/baseline/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 14:52:40,834 >> Special tokens file saved in M2M-100/baseline/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 14:52:40,834 >> added tokens file saved in M2M-100/baseline/added_tokens.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =  2504202GF\n",
            "  train_loss               =      2.626\n",
            "  train_runtime            = 0:34:03.78\n",
            "  train_samples            =       5737\n",
            "  train_samples_per_second =      8.421\n",
            "  train_steps_per_second   =      1.054\n",
            "03/10/2025 14:52:40 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:4260] 2025-03-10 14:52:40,981 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4262] 2025-03-10 14:52:40,981 >>   Num examples = 997\n",
            "[INFO|trainer.py:4265] 2025-03-10 14:52:40,981 >>   Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "100% 125/125 [09:06<00:00,  4.37s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_bleu               =     7.2001\n",
            "  eval_gen_len            =    47.1946\n",
            "  eval_loss               =     2.5306\n",
            "  eval_runtime            = 0:09:14.70\n",
            "  eval_samples            =        997\n",
            "  eval_samples_per_second =      1.797\n",
            "  eval_steps_per_second   =      0.225\n",
            "03/10/2025 15:01:55 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:4260] 2025-03-10 15:01:55,694 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4262] 2025-03-10 15:01:55,695 >>   Num examples = 1012\n",
            "[INFO|trainer.py:4265] 2025-03-10 15:01:55,695 >>   Batch size = 8\n",
            "100% 127/127 [09:47<00:00,  4.63s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu               =     7.3419\n",
            "  predict_gen_len            =    49.1245\n",
            "  predict_loss               =     2.5737\n",
            "  predict_runtime            = 0:09:54.65\n",
            "  predict_samples            =       1012\n",
            "  predict_samples_per_second =      1.702\n",
            "  predict_steps_per_second   =      0.214\n",
            "[INFO|modelcard.py:449] 2025-03-10 15:12:05,599 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 7.2001}]}\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path facebook/m2m100_418M \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --source_lang en \\\n",
        "    --target_lang zu \\\n",
        "    --train_file /content/AIMS-NLP-Project/data/mafand/en-zu/merged.json \\\n",
        "    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n",
        "    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n",
        "    --num_beams 10 \\\n",
        "    --output_dir M2M-100/baseline \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OEaIczVafcf"
      },
      "source": [
        "# Africomet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "Mp6AahGfufdK",
        "outputId": "8fd256aa-0f29-4dcb-ef13-06a227a1f225"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize WandB\n",
        "wandb.init(project=\"machine translation\", name=\"africomet-1000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbNrB5boVIgU",
        "outputId": "940328a7-f8fb-4a5f-939c-7508d167fd19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-10 15:12:13.555077: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-03-10 15:12:13.576922: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-03-10 15:12:13.583428: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-10 15:12:13.599610: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-10 15:12:14.655466: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/10/2025 15:12:18 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
            "03/10/2025 15:12:18 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=2,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=M2M-100/africomet-qe-stl-1.1-1000/runs/Mar10_15-12-18_bcdd196dbaf2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=M2M-100/africomet-qe-stl-1.1-1000,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=M2M-100/africomet-qe-stl-1.1-1000,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tp_size=0,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-d22542853ecd2369\n",
            "03/10/2025 15:12:19 - INFO - datasets.builder - Using custom data configuration default-d22542853ecd2369\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "03/10/2025 15:12:19 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "03/10/2025 15:12:19 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/10/2025 15:12:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "03/10/2025 15:12:19 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/10/2025 15:12:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 15:12:19,381 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 15:12:19,383 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 15:12:19,615 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 15:12:19,615 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:12:19,616 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:12:19,616 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:12:19,616 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:12:19,616 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:12:19,616 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:12:19,616 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:12:19,616 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 15:12:19,617 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 15:12:19,617 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3984] 2025-03-10 15:12:20,849 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:1140] 2025-03-10 15:12:20,919 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:61] 2025-03-10 15:12:20,966 >> Attempting to create safetensors variant\n",
            "[INFO|modeling_utils.py:4972] 2025-03-10 15:12:21,024 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4980] 2025-03-10 15:12:21,024 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
            "[INFO|configuration_utils.py:1095] 2025-03-10 15:12:21,119 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-03-10 15:12:21,119 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:74] 2025-03-10 15:12:21,327 >> Safetensors PR exists\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e70b427f4acd1a90.arrow\n",
            "03/10/2025 15:12:22 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e70b427f4acd1a90.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1e1e9cb1ba5e0af7.arrow\n",
            "03/10/2025 15:12:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1e1e9cb1ba5e0af7.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b37e93e0dc9f95ce.arrow\n",
            "03/10/2025 15:12:25 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b37e93e0dc9f95ce.arrow\n",
            "[INFO|trainer.py:2407] 2025-03-10 15:12:27,279 >> ***** Running training *****\n",
            "[INFO|trainer.py:2408] 2025-03-10 15:12:27,279 >>   Num examples = 1,000\n",
            "[INFO|trainer.py:2409] 2025-03-10 15:12:27,279 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2410] 2025-03-10 15:12:27,280 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:2412] 2025-03-10 15:12:27,280 >>   Training with DataParallel so batch size has been adjusted to: 8\n",
            "[INFO|trainer.py:2413] 2025-03-10 15:12:27,280 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2414] 2025-03-10 15:12:27,280 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2415] 2025-03-10 15:12:27,280 >>   Total optimization steps = 375\n",
            "[INFO|trainer.py:2416] 2025-03-10 15:12:27,281 >>   Number of trainable parameters = 483,905,536\n",
            "[INFO|integration_utils.py:817] 2025-03-10 15:12:27,286 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_151227-bthnr4uy\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-1000\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/bthnr4uy\u001b[0m\n",
            "  0% 0/375 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "100% 375/375 [04:42<00:00,  1.35it/s][INFO|trainer.py:3944] 2025-03-10 15:17:10,484 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 15:17:10,487 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 15:17:10,488 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 15:17:15,538 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 15:17:15,541 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 15:17:15,542 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 15:17:15,543 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/added_tokens.json\n",
            "[INFO|trainer.py:2659] 2025-03-10 15:17:29,121 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 301.8402, 'train_samples_per_second': 9.939, 'train_steps_per_second': 1.242, 'train_loss': 3.7139088541666667, 'epoch': 3.0}\n",
            "100% 375/375 [05:00<00:00,  1.25it/s]\n",
            "[INFO|trainer.py:3944] 2025-03-10 15:17:29,132 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-1000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 15:17:29,134 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 15:17:29,134 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 15:17:38,072 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 15:17:38,075 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 15:17:38,075 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 15:17:38,076 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/added_tokens.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =   205627GF\n",
            "  train_loss               =     3.7139\n",
            "  train_runtime            = 0:05:01.84\n",
            "  train_samples            =       1000\n",
            "  train_samples_per_second =      9.939\n",
            "  train_steps_per_second   =      1.242\n",
            "03/10/2025 15:17:38 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:4260] 2025-03-10 15:17:38,225 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4262] 2025-03-10 15:17:38,226 >>   Num examples = 997\n",
            "[INFO|trainer.py:4265] 2025-03-10 15:17:38,226 >>   Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "100% 125/125 [11:36<00:00,  5.57s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_bleu               =     2.2044\n",
            "  eval_gen_len            =    54.6329\n",
            "  eval_loss               =     3.9725\n",
            "  eval_runtime            = 0:11:43.50\n",
            "  eval_samples            =        997\n",
            "  eval_samples_per_second =      1.417\n",
            "  eval_steps_per_second   =      0.178\n",
            "03/10/2025 15:29:21 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:4260] 2025-03-10 15:29:21,737 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4262] 2025-03-10 15:29:21,737 >>   Num examples = 1012\n",
            "[INFO|trainer.py:4265] 2025-03-10 15:29:21,737 >>   Batch size = 8\n",
            "100% 127/127 [13:07<00:00,  6.20s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu               =     1.9318\n",
            "  predict_gen_len            =    56.9022\n",
            "  predict_loss               =     4.0076\n",
            "  predict_runtime            = 0:13:16.19\n",
            "  predict_samples            =       1012\n",
            "  predict_samples_per_second =      1.271\n",
            "  predict_steps_per_second   =       0.16\n",
            "[INFO|modelcard.py:449] 2025-03-10 15:42:55,361 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 2.2044}]}\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path facebook/m2m100_418M \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --source_lang en \\\n",
        "    --target_lang zu \\\n",
        "    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_1000.json \\\n",
        "    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n",
        "    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n",
        "    --num_beams 10 \\\n",
        "    --output_dir M2M-100/africomet-qe-stl-1.1-1000 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "oAhoqvCbuk06",
        "outputId": "bf367f98-0f7e-4ae5-dd85-8a721d4a908a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize WandB\n",
        "wandb.init(project=\"machine translation\", name=\"africomet-2000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0Vip1YNtpQn",
        "outputId": "1479f6f2-e77a-4289-ea6a-281bf29bc7fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-10 15:43:04.181598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-03-10 15:43:04.206516: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-03-10 15:43:04.213821: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-10 15:43:04.232535: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-10 15:43:05.563373: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/10/2025 15:43:10 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
            "03/10/2025 15:43:10 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=2,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=M2M-100/africomet-qe-stl-1.1-2000/runs/Mar10_15-43-10_bcdd196dbaf2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=M2M-100/africomet-qe-stl-1.1-2000,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=M2M-100/africomet-qe-stl-1.1-2000,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tp_size=0,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-63fa3ea70d279152\n",
            "03/10/2025 15:43:10 - INFO - datasets.builder - Using custom data configuration default-63fa3ea70d279152\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "03/10/2025 15:43:10 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "03/10/2025 15:43:10 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/10/2025 15:43:10 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "03/10/2025 15:43:10 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/10/2025 15:43:10 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 15:43:10,630 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 15:43:10,632 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 15:43:10,719 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 15:43:10,720 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:43:10,721 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:43:10,721 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:43:10,721 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:43:10,721 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:43:10,721 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:43:10,721 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 15:43:10,721 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 15:43:10,722 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 15:43:10,723 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3984] 2025-03-10 15:43:11,819 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:1140] 2025-03-10 15:43:11,898 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:61] 2025-03-10 15:43:11,964 >> Attempting to create safetensors variant\n",
            "[INFO|modeling_utils.py:4972] 2025-03-10 15:43:12,040 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4980] 2025-03-10 15:43:12,040 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
            "[INFO|configuration_utils.py:1095] 2025-03-10 15:43:12,131 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-03-10 15:43:12,132 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:74] 2025-03-10 15:43:12,387 >> Safetensors PR exists\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-239a0c8168036379.arrow\n",
            "03/10/2025 15:43:13 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-239a0c8168036379.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4e37c8bc3d2bbba6.arrow\n",
            "03/10/2025 15:43:15 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4e37c8bc3d2bbba6.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-283cd450680fa67d.arrow\n",
            "03/10/2025 15:43:16 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-283cd450680fa67d.arrow\n",
            "[INFO|trainer.py:2407] 2025-03-10 15:43:19,332 >> ***** Running training *****\n",
            "[INFO|trainer.py:2408] 2025-03-10 15:43:19,332 >>   Num examples = 2,000\n",
            "[INFO|trainer.py:2409] 2025-03-10 15:43:19,332 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2410] 2025-03-10 15:43:19,332 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:2412] 2025-03-10 15:43:19,332 >>   Training with DataParallel so batch size has been adjusted to: 8\n",
            "[INFO|trainer.py:2413] 2025-03-10 15:43:19,332 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2414] 2025-03-10 15:43:19,332 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2415] 2025-03-10 15:43:19,332 >>   Total optimization steps = 750\n",
            "[INFO|trainer.py:2416] 2025-03-10 15:43:19,333 >>   Number of trainable parameters = 483,905,536\n",
            "[INFO|integration_utils.py:817] 2025-03-10 15:43:19,339 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_154319-6o0957if\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-2000\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/6o0957if\u001b[0m\n",
            "  0% 0/750 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 4.0076, 'grad_norm': 9.165338516235352, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
            " 67% 500/750 [06:18<03:06,  1.34it/s][INFO|trainer.py:3944] 2025-03-10 15:49:38,632 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 15:49:38,635 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 15:49:38,635 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 15:49:43,833 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 15:49:43,837 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 15:49:43,838 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 15:49:43,838 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "100% 750/750 [09:48<00:00,  1.32it/s][INFO|trainer.py:3944] 2025-03-10 15:53:08,314 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 15:53:08,316 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 15:53:08,317 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 15:53:14,152 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 15:53:14,157 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 15:53:14,157 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 15:53:14,158 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/added_tokens.json\n",
            "[INFO|trainer.py:2659] 2025-03-10 15:53:27,615 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 608.2826, 'train_samples_per_second': 9.864, 'train_steps_per_second': 1.233, 'train_loss': 3.6032588704427084, 'epoch': 3.0}\n",
            "100% 750/750 [10:07<00:00,  1.23it/s]\n",
            "[INFO|trainer.py:3944] 2025-03-10 15:53:27,626 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-2000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 15:53:27,629 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 15:53:27,630 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 15:53:36,540 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-2000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 15:53:36,544 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 15:53:36,545 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 15:53:36,545 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/added_tokens.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =   409031GF\n",
            "  train_loss               =     3.6033\n",
            "  train_runtime            = 0:10:08.28\n",
            "  train_samples            =       2000\n",
            "  train_samples_per_second =      9.864\n",
            "  train_steps_per_second   =      1.233\n",
            "03/10/2025 15:53:36 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:4260] 2025-03-10 15:53:36,707 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4262] 2025-03-10 15:53:36,707 >>   Num examples = 997\n",
            "[INFO|trainer.py:4265] 2025-03-10 15:53:36,708 >>   Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "100% 125/125 [12:21<00:00,  5.93s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_bleu               =     2.4532\n",
            "  eval_gen_len            =     54.345\n",
            "  eval_loss               =     3.7164\n",
            "  eval_runtime            = 0:12:27.98\n",
            "  eval_samples            =        997\n",
            "  eval_samples_per_second =      1.333\n",
            "  eval_steps_per_second   =      0.167\n",
            "03/10/2025 16:06:04 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:4260] 2025-03-10 16:06:04,703 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4262] 2025-03-10 16:06:04,704 >>   Num examples = 1012\n",
            "[INFO|trainer.py:4265] 2025-03-10 16:06:04,704 >>   Batch size = 8\n",
            "100% 127/127 [12:43<00:00,  6.01s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu               =     2.4691\n",
            "  predict_gen_len            =    55.2362\n",
            "  predict_loss               =     3.7659\n",
            "  predict_runtime            = 0:12:51.87\n",
            "  predict_samples            =       1012\n",
            "  predict_samples_per_second =      1.311\n",
            "  predict_steps_per_second   =      0.165\n",
            "[INFO|modelcard.py:449] 2025-03-10 16:19:15,368 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 2.4532}]}\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path facebook/m2m100_418M \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --source_lang en \\\n",
        "    --target_lang zu \\\n",
        "    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_2000.json \\\n",
        "    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n",
        "    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n",
        "    --num_beams 10 \\\n",
        "    --output_dir M2M-100/africomet-qe-stl-1.1-2000 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "0v4YeYRgumqu",
        "outputId": "a73abb8e-6f01-4eb2-a1f6-a68e504358a5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize WandB\n",
        "wandb.init(project=\"machine translation\", name=\"africomet-4000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGUYnc2wtrby",
        "outputId": "f6263987-ce01-4377-8bd5-c34a91612576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-10 16:19:23.692470: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-03-10 16:19:23.717422: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-03-10 16:19:23.724751: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-10 16:19:23.744686: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-10 16:19:25.130002: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/10/2025 16:19:28 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
            "03/10/2025 16:19:28 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=2,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=M2M-100/africomet-qe-stl-1.1-4000/runs/Mar10_16-19-28_bcdd196dbaf2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=M2M-100/africomet-qe-stl-1.1-4000,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=M2M-100/africomet-qe-stl-1.1-4000,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tp_size=0,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-2c18f5caf6d15f80\n",
            "03/10/2025 16:19:29 - INFO - datasets.builder - Using custom data configuration default-2c18f5caf6d15f80\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "03/10/2025 16:19:29 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "03/10/2025 16:19:29 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/10/2025 16:19:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "03/10/2025 16:19:29 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/10/2025 16:19:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 16:19:29,202 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 16:19:29,204 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 16:19:29,319 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 16:19:29,320 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 16:19:29,321 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 16:19:29,321 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 16:19:29,321 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 16:19:29,321 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 16:19:29,321 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 16:19:29,321 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 16:19:29,321 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 16:19:29,322 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 16:19:29,323 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3984] 2025-03-10 16:19:30,440 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:1140] 2025-03-10 16:19:30,519 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:61] 2025-03-10 16:19:30,556 >> Attempting to create safetensors variant\n",
            "[INFO|modeling_utils.py:4972] 2025-03-10 16:19:30,655 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4980] 2025-03-10 16:19:30,656 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
            "[INFO|configuration_utils.py:1095] 2025-03-10 16:19:30,752 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-03-10 16:19:30,752 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:74] 2025-03-10 16:19:31,115 >> Safetensors PR exists\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b3e5c8b83b5f5d81.arrow\n",
            "03/10/2025 16:19:32 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b3e5c8b83b5f5d81.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d80ea2e702926f3c.arrow\n",
            "03/10/2025 16:19:34 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d80ea2e702926f3c.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-94a06ef0cd1537df.arrow\n",
            "03/10/2025 16:19:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-94a06ef0cd1537df.arrow\n",
            "[INFO|trainer.py:2407] 2025-03-10 16:19:38,157 >> ***** Running training *****\n",
            "[INFO|trainer.py:2408] 2025-03-10 16:19:38,158 >>   Num examples = 4,000\n",
            "[INFO|trainer.py:2409] 2025-03-10 16:19:38,158 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2410] 2025-03-10 16:19:38,158 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:2412] 2025-03-10 16:19:38,158 >>   Training with DataParallel so batch size has been adjusted to: 8\n",
            "[INFO|trainer.py:2413] 2025-03-10 16:19:38,158 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2414] 2025-03-10 16:19:38,158 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2415] 2025-03-10 16:19:38,158 >>   Total optimization steps = 1,500\n",
            "[INFO|trainer.py:2416] 2025-03-10 16:19:38,159 >>   Number of trainable parameters = 483,905,536\n",
            "[INFO|integration_utils.py:817] 2025-03-10 16:19:38,165 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_161938-86eywks3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-4000\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/86eywks3\u001b[0m\n",
            "  0% 0/1500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 4.3848, 'grad_norm': 6.340878486633301, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
            " 33% 500/1500 [06:22<12:49,  1.30it/s][INFO|trainer.py:3944] 2025-03-10 16:26:01,177 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 16:26:01,181 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 16:26:01,182 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 16:26:06,218 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 16:26:06,221 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 16:26:06,222 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 16:26:06,222 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 3.2917, 'grad_norm': 8.45515251159668, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
            " 67% 1000/1500 [12:58<06:52,  1.21it/s][INFO|trainer.py:3944] 2025-03-10 16:32:37,705 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 16:32:37,707 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 16:32:37,708 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 16:32:43,422 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 16:32:43,425 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 16:32:43,426 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 16:32:43,426 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.64, 'grad_norm': 10.272100448608398, 'learning_rate': 0.0, 'epoch': 3.0}\n",
            "100% 1500/1500 [19:36<00:00,  1.35it/s][INFO|trainer.py:3944] 2025-03-10 16:39:15,183 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 16:39:15,184 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 16:39:15,185 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 16:39:20,610 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 16:39:20,614 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 16:39:20,615 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 16:39:20,615 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/added_tokens.json\n",
            "[INFO|trainer.py:2659] 2025-03-10 16:39:34,161 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1196.0024, 'train_samples_per_second': 10.033, 'train_steps_per_second': 1.254, 'train_loss': 3.438828369140625, 'epoch': 3.0}\n",
            "100% 1500/1500 [19:55<00:00,  1.26it/s]\n",
            "[INFO|trainer.py:3944] 2025-03-10 16:39:34,173 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-4000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 16:39:34,174 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 16:39:34,175 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 16:39:43,103 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-4000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 16:39:43,108 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 16:39:43,109 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 16:39:43,109 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/added_tokens.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =   849267GF\n",
            "  train_loss               =     3.4388\n",
            "  train_runtime            = 0:19:56.00\n",
            "  train_samples            =       4000\n",
            "  train_samples_per_second =     10.033\n",
            "  train_steps_per_second   =      1.254\n",
            "03/10/2025 16:39:43 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:4260] 2025-03-10 16:39:43,265 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4262] 2025-03-10 16:39:43,265 >>   Num examples = 997\n",
            "[INFO|trainer.py:4265] 2025-03-10 16:39:43,265 >>   Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "100% 125/125 [11:11<00:00,  5.37s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_bleu               =     3.6671\n",
            "  eval_gen_len            =    49.7061\n",
            "  eval_loss               =      3.353\n",
            "  eval_runtime            = 0:11:19.28\n",
            "  eval_samples            =        997\n",
            "  eval_samples_per_second =      1.468\n",
            "  eval_steps_per_second   =      0.184\n",
            "03/10/2025 16:51:02 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:4260] 2025-03-10 16:51:02,554 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4262] 2025-03-10 16:51:02,554 >>   Num examples = 1012\n",
            "[INFO|trainer.py:4265] 2025-03-10 16:51:02,554 >>   Batch size = 8\n",
            "100% 127/127 [12:03<00:00,  5.70s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu               =     3.0816\n",
            "  predict_gen_len            =    52.0968\n",
            "  predict_loss               =     3.4086\n",
            "  predict_runtime            = 0:12:09.82\n",
            "  predict_samples            =       1012\n",
            "  predict_samples_per_second =      1.387\n",
            "  predict_steps_per_second   =      0.174\n",
            "[INFO|modelcard.py:449] 2025-03-10 17:03:31,361 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 3.6671}]}\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path facebook/m2m100_418M \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --source_lang en \\\n",
        "    --target_lang zu \\\n",
        "    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_4000.json \\\n",
        "    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n",
        "    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n",
        "    --num_beams 10 \\\n",
        "    --output_dir M2M-100/africomet-qe-stl-1.1-4000 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "hKmXZ3xLuoYh",
        "outputId": "ce91efb5-bca3-48d9-bb5e-be0800661186"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize WandB\n",
        "wandb.init(project=\"machine translation\", name=\"africomet-8000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZSOQFxvttWv",
        "outputId": "747535e8-c367-45a9-d48d-fc14c8439854"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-10 17:03:38.550278: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-03-10 17:03:38.583109: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-03-10 17:03:38.590670: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-10 17:03:38.609732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-10 17:03:39.832340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/10/2025 17:03:42 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
            "03/10/2025 17:03:42 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=2,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=M2M-100/africomet-qe-stl-1.1-8000/runs/Mar10_17-03-42_bcdd196dbaf2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=M2M-100/africomet-qe-stl-1.1-8000,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=M2M-100/africomet-qe-stl-1.1-8000,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tp_size=0,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-5bc63a9eaf840ac0\n",
            "03/10/2025 17:03:43 - INFO - datasets.builder - Using custom data configuration default-5bc63a9eaf840ac0\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "03/10/2025 17:03:43 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "03/10/2025 17:03:43 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/10/2025 17:03:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "03/10/2025 17:03:43 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/10/2025 17:03:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 17:03:43,240 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 17:03:43,242 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 17:03:43,395 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 17:03:43,396 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,397 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,397 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,397 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,398 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,398 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,398 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 17:03:43,398 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 17:03:43,398 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 17:03:43,399 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3984] 2025-03-10 17:03:44,414 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:1140] 2025-03-10 17:03:44,491 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:61] 2025-03-10 17:03:44,531 >> Attempting to create safetensors variant\n",
            "[INFO|modeling_utils.py:4972] 2025-03-10 17:03:44,617 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4980] 2025-03-10 17:03:44,617 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
            "[INFO|configuration_utils.py:1095] 2025-03-10 17:03:44,715 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-03-10 17:03:44,715 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:74] 2025-03-10 17:03:44,904 >> Safetensors PR exists\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-805d80dbaf682edc.arrow\n",
            "03/10/2025 17:03:46 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-805d80dbaf682edc.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-8d35743ea3961495.arrow\n",
            "03/10/2025 17:03:47 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-8d35743ea3961495.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a687ca1a7df8988f.arrow\n",
            "03/10/2025 17:03:49 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a687ca1a7df8988f.arrow\n",
            "[INFO|trainer.py:2407] 2025-03-10 17:03:51,848 >> ***** Running training *****\n",
            "[INFO|trainer.py:2408] 2025-03-10 17:03:51,848 >>   Num examples = 8,000\n",
            "[INFO|trainer.py:2409] 2025-03-10 17:03:51,848 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2410] 2025-03-10 17:03:51,848 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:2412] 2025-03-10 17:03:51,848 >>   Training with DataParallel so batch size has been adjusted to: 8\n",
            "[INFO|trainer.py:2413] 2025-03-10 17:03:51,848 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2414] 2025-03-10 17:03:51,848 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2415] 2025-03-10 17:03:51,848 >>   Total optimization steps = 3,000\n",
            "[INFO|trainer.py:2416] 2025-03-10 17:03:51,849 >>   Number of trainable parameters = 483,905,536\n",
            "[INFO|integration_utils.py:817] 2025-03-10 17:03:51,855 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_170351-7zajcprb\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-8000\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/7zajcprb\u001b[0m\n",
            "  0% 0/3000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 4.3678, 'grad_norm': 8.133068084716797, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n",
            " 17% 500/3000 [06:18<31:10,  1.34it/s][INFO|trainer.py:3944] 2025-03-10 17:10:11,245 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 17:10:11,248 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 17:10:11,249 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 17:10:16,754 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:10:16,757 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:10:16,758 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:10:16,758 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 3.8433, 'grad_norm': 8.525639533996582, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
            " 33% 1000/3000 [12:57<25:38,  1.30it/s][INFO|trainer.py:3944] 2025-03-10 17:16:50,074 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 17:16:50,076 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 17:16:50,077 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 17:16:55,629 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:16:55,632 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:16:55,632 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:16:55,633 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 3.0452, 'grad_norm': 7.349609851837158, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n",
            " 50% 1500/3000 [19:33<18:57,  1.32it/s][INFO|trainer.py:3944] 2025-03-10 17:23:26,116 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 17:23:26,118 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 17:23:26,119 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 17:23:31,635 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:23:31,638 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:23:31,639 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:23:31,639 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.95, 'grad_norm': 8.351008415222168, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
            " 67% 2000/3000 [26:11<12:11,  1.37it/s][INFO|trainer.py:3944] 2025-03-10 17:30:03,819 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 17:30:03,821 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 17:30:03,822 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 17:30:09,382 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:30:09,386 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:30:09,387 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:30:09,388 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.4116, 'grad_norm': 9.160988807678223, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n",
            " 83% 2500/3000 [32:46<06:13,  1.34it/s][INFO|trainer.py:3944] 2025-03-10 17:36:39,364 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 17:36:39,366 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 17:36:39,367 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 17:36:44,901 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:36:44,908 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:36:44,909 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:36:44,909 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.3545, 'grad_norm': 6.916102886199951, 'learning_rate': 0.0, 'epoch': 3.0}\n",
            "100% 3000/3000 [39:25<00:00,  1.31it/s][INFO|trainer.py:3944] 2025-03-10 17:43:18,656 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 17:43:18,658 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 17:43:18,659 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 17:43:24,214 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:43:24,217 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:43:24,217 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:43:24,218 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/added_tokens.json\n",
            "[INFO|trainer.py:2659] 2025-03-10 17:43:37,703 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2385.8542, 'train_samples_per_second': 10.059, 'train_steps_per_second': 1.257, 'train_loss': 3.1620497639973957, 'epoch': 3.0}\n",
            "100% 3000/3000 [39:44<00:00,  1.26it/s]\n",
            "[INFO|trainer.py:3944] 2025-03-10 17:43:37,707 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 17:43:37,709 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 17:43:37,710 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 17:43:46,668 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 17:43:46,672 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 17:43:46,672 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 17:43:46,673 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/added_tokens.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =  1672691GF\n",
            "  train_loss               =      3.162\n",
            "  train_runtime            = 0:39:45.85\n",
            "  train_samples            =       8000\n",
            "  train_samples_per_second =     10.059\n",
            "  train_steps_per_second   =      1.257\n",
            "03/10/2025 17:43:46 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:4260] 2025-03-10 17:43:46,838 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4262] 2025-03-10 17:43:46,838 >>   Num examples = 997\n",
            "[INFO|trainer.py:4265] 2025-03-10 17:43:46,839 >>   Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "100% 125/125 [09:45<00:00,  4.68s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_bleu               =     5.3524\n",
            "  eval_gen_len            =    46.8586\n",
            "  eval_loss               =     2.9557\n",
            "  eval_runtime            = 0:09:51.81\n",
            "  eval_samples            =        997\n",
            "  eval_samples_per_second =      1.685\n",
            "  eval_steps_per_second   =      0.211\n",
            "03/10/2025 17:53:38 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:4260] 2025-03-10 17:53:38,657 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4262] 2025-03-10 17:53:38,657 >>   Num examples = 1012\n",
            "[INFO|trainer.py:4265] 2025-03-10 17:53:38,657 >>   Batch size = 8\n",
            "100% 127/127 [10:52<00:00,  5.14s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu               =     4.9433\n",
            "  predict_gen_len            =    49.2668\n",
            "  predict_loss               =     3.0128\n",
            "  predict_runtime            = 0:10:58.87\n",
            "  predict_samples            =       1012\n",
            "  predict_samples_per_second =      1.536\n",
            "  predict_steps_per_second   =      0.193\n",
            "[INFO|modelcard.py:449] 2025-03-10 18:04:55,614 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 5.3524}]}\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path facebook/m2m100_418M \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --source_lang en \\\n",
        "    --target_lang zu \\\n",
        "    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_8000.json \\\n",
        "    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n",
        "    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n",
        "    --num_beams 10 \\\n",
        "    --output_dir M2M-100/africomet-qe-stl-1.1-8000 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "50vt5hMgup68",
        "outputId": "63b5de95-854a-42da-ca9c-2117e0317b58"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize WandB\n",
        "wandb.init(project=\"machine translation\", name=\"africomet-16000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlfAMTxMtvOi",
        "outputId": "5aa226a1-9582-4fb7-80bd-d80b5cf5b37f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-10 18:05:02.627668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-03-10 18:05:02.653295: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-03-10 18:05:02.660907: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-10 18:05:02.679637: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-10 18:05:03.876059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/10/2025 18:05:06 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
            "03/10/2025 18:05:06 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=2,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=M2M-100/africomet-qe-stl-1.1-16000/runs/Mar10_18-05-06_bcdd196dbaf2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=M2M-100/africomet-qe-stl-1.1-16000,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=M2M-100/africomet-qe-stl-1.1-16000,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tp_size=0,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-8dd97eb64194ed63\n",
            "03/10/2025 18:05:06 - INFO - datasets.builder - Using custom data configuration default-8dd97eb64194ed63\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "03/10/2025 18:05:06 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "03/10/2025 18:05:06 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/10/2025 18:05:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "03/10/2025 18:05:06 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/10/2025 18:05:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 18:05:07,170 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 18:05:07,172 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 18:05:07,258 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 18:05:07,259 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,260 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,260 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,260 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,260 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,260 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,261 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 18:05:07,261 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 18:05:07,261 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 18:05:07,262 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3984] 2025-03-10 18:05:08,314 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:1140] 2025-03-10 18:05:08,391 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:61] 2025-03-10 18:05:08,426 >> Attempting to create safetensors variant\n",
            "[INFO|modeling_utils.py:4972] 2025-03-10 18:05:08,524 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4980] 2025-03-10 18:05:08,524 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
            "[INFO|configuration_utils.py:1095] 2025-03-10 18:05:08,671 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-03-10 18:05:08,671 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4f73fd23cbbc2044.arrow\n",
            "03/10/2025 18:05:10 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4f73fd23cbbc2044.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-61f573cc25eca874.arrow\n",
            "03/10/2025 18:05:11 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-61f573cc25eca874.arrow\n",
            "[INFO|safetensors_conversion.py:74] 2025-03-10 18:05:12,713 >> Safetensors PR exists\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-29c03a83c2242779.arrow\n",
            "03/10/2025 18:05:13 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-29c03a83c2242779.arrow\n",
            "[INFO|trainer.py:2407] 2025-03-10 18:05:15,747 >> ***** Running training *****\n",
            "[INFO|trainer.py:2408] 2025-03-10 18:05:15,747 >>   Num examples = 16,000\n",
            "[INFO|trainer.py:2409] 2025-03-10 18:05:15,747 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2410] 2025-03-10 18:05:15,747 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:2412] 2025-03-10 18:05:15,747 >>   Training with DataParallel so batch size has been adjusted to: 8\n",
            "[INFO|trainer.py:2413] 2025-03-10 18:05:15,747 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2414] 2025-03-10 18:05:15,747 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2415] 2025-03-10 18:05:15,747 >>   Total optimization steps = 6,000\n",
            "[INFO|trainer.py:2416] 2025-03-10 18:05:15,749 >>   Number of trainable parameters = 483,905,536\n",
            "[INFO|integration_utils.py:817] 2025-03-10 18:05:15,755 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_180515-53flw2ac\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-16000\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/53flw2ac\u001b[0m\n",
            "  0% 0/6000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 4.4093, 'grad_norm': 8.515572547912598, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}\n",
            "  8% 500/6000 [06:17<1:07:03,  1.37it/s][INFO|trainer.py:3944] 2025-03-10 18:11:34,382 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 18:11:34,386 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 18:11:34,387 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 18:11:39,789 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:11:39,792 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:11:39,792 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:11:39,793 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 3.8951, 'grad_norm': 7.606267929077148, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n",
            " 17% 1000/6000 [12:55<1:05:41,  1.27it/s][INFO|trainer.py:3944] 2025-03-10 18:18:12,301 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 18:18:12,303 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 18:18:12,304 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 18:18:17,852 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:18:17,856 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:18:17,857 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:18:17,857 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 3.5859, 'grad_norm': 7.814512252807617, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}\n",
            " 25% 1500/6000 [19:31<56:49,  1.32it/s][INFO|trainer.py:3944] 2025-03-10 18:24:47,824 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 18:24:47,826 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 18:24:47,827 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 18:24:53,373 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:24:53,377 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:24:53,377 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:24:53,378 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 3.4074, 'grad_norm': 7.271307468414307, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
            " 33% 2000/6000 [26:08<50:21,  1.32it/s][INFO|trainer.py:3944] 2025-03-10 18:31:25,350 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 18:31:25,352 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 18:31:25,353 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 18:31:30,850 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:31:30,853 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:31:30,853 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:31:30,854 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.8236, 'grad_norm': 7.814577102661133, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}\n",
            " 42% 2500/6000 [32:44<43:31,  1.34it/s][INFO|trainer.py:3944] 2025-03-10 18:38:00,848 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 18:38:00,850 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 18:38:00,851 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 18:38:06,401 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:38:06,404 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:38:06,405 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:38:06,406 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.758, 'grad_norm': 5.690384864807129, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n",
            " 50% 3000/6000 [39:21<38:08,  1.31it/s][INFO|trainer.py:3944] 2025-03-10 18:44:37,838 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 18:44:37,840 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 18:44:37,841 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 18:44:43,390 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:44:43,394 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:44:43,394 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:44:43,394 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.7309, 'grad_norm': 8.432010650634766, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}\n",
            " 58% 3500/6000 [45:58<31:25,  1.33it/s][INFO|trainer.py:3944] 2025-03-10 18:51:15,660 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 18:51:15,663 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 18:51:15,663 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 18:51:21,209 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:51:21,213 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:51:21,213 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:51:21,214 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.6624, 'grad_norm': 6.384344577789307, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
            " 67% 4000/6000 [52:34<26:04,  1.28it/s][INFO|trainer.py:3944] 2025-03-10 18:57:51,455 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 18:57:51,457 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 18:57:51,458 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 18:57:56,944 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 18:57:56,947 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 18:57:56,947 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 18:57:56,948 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.2164, 'grad_norm': 6.924351692199707, 'learning_rate': 1.25e-05, 'epoch': 2.25}\n",
            " 75% 4500/6000 [59:11<18:50,  1.33it/s][INFO|trainer.py:3944] 2025-03-10 19:04:28,469 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 19:04:28,471 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 19:04:28,472 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 19:04:34,039 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 19:04:34,042 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 19:04:34,043 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 19:04:34,043 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.2, 'grad_norm': 6.821500778198242, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n",
            " 83% 5000/6000 [1:05:46<12:19,  1.35it/s][INFO|trainer.py:3944] 2025-03-10 19:11:03,349 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 19:11:03,350 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 19:11:03,351 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 19:11:08,451 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 19:11:08,454 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 19:11:08,455 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 19:11:08,455 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.1951, 'grad_norm': 6.860134124755859, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}\n",
            " 92% 5500/6000 [1:12:19<06:01,  1.38it/s][INFO|trainer.py:3944] 2025-03-10 19:17:36,626 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 19:17:36,628 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 19:17:36,629 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 19:17:41,776 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 19:17:41,779 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 19:17:41,780 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 19:17:41,780 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.1311, 'grad_norm': 6.98347282409668, 'learning_rate': 0.0, 'epoch': 3.0}\n",
            "100% 6000/6000 [1:18:53<00:00,  1.36it/s][INFO|trainer.py:3944] 2025-03-10 19:24:10,314 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 19:24:10,316 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 19:24:10,317 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 19:24:15,588 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 19:24:15,592 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 19:24:15,592 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 19:24:15,593 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/added_tokens.json\n",
            "[INFO|trainer.py:2659] 2025-03-10 19:24:29,306 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 4753.5579, 'train_samples_per_second': 10.098, 'train_steps_per_second': 1.262, 'train_loss': 2.9179393717447915, 'epoch': 3.0}\n",
            "100% 6000/6000 [1:19:12<00:00,  1.26it/s]\n",
            "[INFO|trainer.py:3944] 2025-03-10 19:24:29,311 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 19:24:29,313 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 19:24:29,314 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 19:24:38,219 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 19:24:38,222 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 19:24:38,223 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 19:24:38,223 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/added_tokens.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =  3307792GF\n",
            "  train_loss               =     2.9179\n",
            "  train_runtime            = 1:19:13.55\n",
            "  train_samples            =      16000\n",
            "  train_samples_per_second =     10.098\n",
            "  train_steps_per_second   =      1.262\n",
            "03/10/2025 19:24:38 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:4260] 2025-03-10 19:24:38,375 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4262] 2025-03-10 19:24:38,375 >>   Num examples = 997\n",
            "[INFO|trainer.py:4265] 2025-03-10 19:24:38,375 >>   Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "100% 125/125 [09:35<00:00,  4.60s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_bleu               =     6.8131\n",
            "  eval_gen_len            =    46.3791\n",
            "  eval_loss               =      2.566\n",
            "  eval_runtime            = 0:09:44.12\n",
            "  eval_samples            =        997\n",
            "  eval_samples_per_second =      1.707\n",
            "  eval_steps_per_second   =      0.214\n",
            "03/10/2025 19:34:22 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:4260] 2025-03-10 19:34:22,502 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4262] 2025-03-10 19:34:22,503 >>   Num examples = 1012\n",
            "[INFO|trainer.py:4265] 2025-03-10 19:34:22,503 >>   Batch size = 8\n",
            "100% 127/127 [10:41<00:00,  5.05s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu               =     6.3958\n",
            "  predict_gen_len            =    49.4249\n",
            "  predict_loss               =     2.6116\n",
            "  predict_runtime            = 0:10:49.41\n",
            "  predict_samples            =       1012\n",
            "  predict_samples_per_second =      1.558\n",
            "  predict_steps_per_second   =      0.196\n",
            "[INFO|modelcard.py:449] 2025-03-10 19:45:27,886 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 6.8131}]}\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path facebook/m2m100_418M \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --source_lang en \\\n",
        "    --target_lang zu \\\n",
        "    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_16000.json \\\n",
        "    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n",
        "    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n",
        "    --num_beams 10 \\\n",
        "    --output_dir M2M-100/africomet-qe-stl-1.1-16000 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNdDNvlQ9cRn"
      },
      "source": [
        "# Comet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hlcDxJYg9Zzn",
        "outputId": "b5c36dd5-13ba-4680-fb50-b8687c9cee23"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize WandB\n",
        "wandb.init(project=\"machine translation\", name=\"comet-1000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIlSRCF9_CpV",
        "outputId": "f39857c3-3a83-405d-9ac4-ea1c5d1d7fc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-10 20:00:25.748656: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-03-10 20:00:25.771067: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-03-10 20:00:25.778534: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-10 20:00:25.798099: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-10 20:00:26.864180: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/10/2025 20:00:29 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
            "03/10/2025 20:00:29 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=2,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=M2M-100/wmt22-cometkiwi-da-1000/runs/Mar10_20-00-29_bcdd196dbaf2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=M2M-100/wmt22-cometkiwi-da-1000,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=M2M-100/wmt22-cometkiwi-da-1000,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tp_size=0,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-d22542853ecd2369\n",
            "03/10/2025 20:00:29 - INFO - datasets.builder - Using custom data configuration default-d22542853ecd2369\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "03/10/2025 20:00:29 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "03/10/2025 20:00:29 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/10/2025 20:00:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "03/10/2025 20:00:29 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/10/2025 20:00:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 20:00:29,901 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 20:00:29,902 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 20:00:30,221 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 20:00:30,221 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:00:30,222 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:00:30,222 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:00:30,222 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:00:30,222 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:00:30,222 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:00:30,222 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:00:30,222 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 20:00:30,223 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 20:00:30,224 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3984] 2025-03-10 20:00:31,399 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:1140] 2025-03-10 20:00:31,468 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4972] 2025-03-10 20:00:31,579 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4980] 2025-03-10 20:00:31,580 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
            "[INFO|safetensors_conversion.py:61] 2025-03-10 20:00:31,642 >> Attempting to create safetensors variant\n",
            "[INFO|configuration_utils.py:1095] 2025-03-10 20:00:31,706 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-03-10 20:00:31,707 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:74] 2025-03-10 20:00:32,193 >> Safetensors PR exists\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e70b427f4acd1a90.arrow\n",
            "03/10/2025 20:00:33 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e70b427f4acd1a90.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1e1e9cb1ba5e0af7.arrow\n",
            "03/10/2025 20:00:34 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1e1e9cb1ba5e0af7.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b37e93e0dc9f95ce.arrow\n",
            "03/10/2025 20:00:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b37e93e0dc9f95ce.arrow\n",
            "[INFO|trainer.py:2407] 2025-03-10 20:00:38,315 >> ***** Running training *****\n",
            "[INFO|trainer.py:2408] 2025-03-10 20:00:38,315 >>   Num examples = 1,000\n",
            "[INFO|trainer.py:2409] 2025-03-10 20:00:38,315 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2410] 2025-03-10 20:00:38,315 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:2412] 2025-03-10 20:00:38,315 >>   Training with DataParallel so batch size has been adjusted to: 8\n",
            "[INFO|trainer.py:2413] 2025-03-10 20:00:38,315 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2414] 2025-03-10 20:00:38,315 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2415] 2025-03-10 20:00:38,315 >>   Total optimization steps = 375\n",
            "[INFO|trainer.py:2416] 2025-03-10 20:00:38,317 >>   Number of trainable parameters = 483,905,536\n",
            "[INFO|integration_utils.py:817] 2025-03-10 20:00:38,322 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_200038-n45fc45w\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-1000\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/n45fc45w\u001b[0m\n",
            "  0% 0/375 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "100% 375/375 [04:41<00:00,  1.35it/s][INFO|trainer.py:3944] 2025-03-10 20:05:20,655 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-1000/checkpoint-375\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 20:05:20,658 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-1000/checkpoint-375/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 20:05:20,659 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-1000/checkpoint-375/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 20:05:25,922 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-1000/checkpoint-375/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 20:05:25,926 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-1000/checkpoint-375/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 20:05:25,927 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-1000/checkpoint-375/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 20:05:25,927 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-1000/checkpoint-375/added_tokens.json\n",
            "[INFO|trainer.py:2659] 2025-03-10 20:05:39,686 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 301.3697, 'train_samples_per_second': 9.955, 'train_steps_per_second': 1.244, 'train_loss': 3.713399088541667, 'epoch': 3.0}\n",
            "100% 375/375 [05:00<00:00,  1.25it/s]\n",
            "[INFO|trainer.py:3944] 2025-03-10 20:05:39,689 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-1000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 20:05:39,691 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-1000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 20:05:39,692 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 20:05:48,659 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 20:05:48,662 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 20:05:48,662 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-1000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 20:05:48,663 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-1000/added_tokens.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =   205627GF\n",
            "  train_loss               =     3.7134\n",
            "  train_runtime            = 0:05:01.36\n",
            "  train_samples            =       1000\n",
            "  train_samples_per_second =      9.955\n",
            "  train_steps_per_second   =      1.244\n",
            "03/10/2025 20:05:48 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:4260] 2025-03-10 20:05:48,818 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4262] 2025-03-10 20:05:48,818 >>   Num examples = 997\n",
            "[INFO|trainer.py:4265] 2025-03-10 20:05:48,819 >>   Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "100% 125/125 [11:15<00:00,  5.40s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_bleu               =     2.5499\n",
            "  eval_gen_len            =    53.7031\n",
            "  eval_loss               =     3.9645\n",
            "  eval_runtime            = 0:11:23.63\n",
            "  eval_samples            =        997\n",
            "  eval_samples_per_second =      1.458\n",
            "  eval_steps_per_second   =      0.183\n",
            "03/10/2025 20:17:12 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:4260] 2025-03-10 20:17:12,462 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4262] 2025-03-10 20:17:12,462 >>   Num examples = 1012\n",
            "[INFO|trainer.py:4265] 2025-03-10 20:17:12,463 >>   Batch size = 8\n",
            "100% 127/127 [12:15<00:00,  5.79s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu               =     1.9234\n",
            "  predict_gen_len            =    55.8024\n",
            "  predict_loss               =      4.005\n",
            "  predict_runtime            = 0:12:22.95\n",
            "  predict_samples            =       1012\n",
            "  predict_samples_per_second =      1.362\n",
            "  predict_steps_per_second   =      0.171\n",
            "[INFO|modelcard.py:449] 2025-03-10 20:29:51,653 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 2.5499}]}\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path facebook/m2m100_418M \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --source_lang en \\\n",
        "    --target_lang zu \\\n",
        "    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_1000.json \\\n",
        "    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n",
        "    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n",
        "    --num_beams 10 \\\n",
        "    --output_dir M2M-100/wmt22-cometkiwi-da-1000 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SKfZPpED_kMg",
        "outputId": "c9f94013-b9fe-4117-c862-dc734231776f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/2aublbbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c51e41e0430>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize WandB\n",
        "wandb.init(project=\"machine translation\", name=\"comet-2000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_RO5tzt_ZvK",
        "outputId": "f8b2d08e-44e9-4291-fa92-3f5c1909c8f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-10 20:29:57.977307: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-03-10 20:29:58.001934: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-03-10 20:29:58.008946: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-10 20:29:58.027664: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-10 20:29:59.075888: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/10/2025 20:30:01 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
            "03/10/2025 20:30:01 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=2,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=M2M-100/wmt22-cometkiwi-da-2000/runs/Mar10_20-30-01_bcdd196dbaf2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=M2M-100/wmt22-cometkiwi-da-2000,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=M2M-100/wmt22-cometkiwi-da-2000,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tp_size=0,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-63fa3ea70d279152\n",
            "03/10/2025 20:30:01 - INFO - datasets.builder - Using custom data configuration default-63fa3ea70d279152\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "03/10/2025 20:30:01 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "03/10/2025 20:30:01 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/10/2025 20:30:01 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "03/10/2025 20:30:01 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/10/2025 20:30:01 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 20:30:01,948 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 20:30:01,950 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 20:30:02,055 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 20:30:02,056 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:30:02,056 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:30:02,056 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:30:02,056 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:30:02,057 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:30:02,057 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:30:02,057 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-10 20:30:02,057 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:699] 2025-03-10 20:30:02,057 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-10 20:30:02,058 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3984] 2025-03-10 20:30:02,968 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:1140] 2025-03-10 20:30:03,035 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4972] 2025-03-10 20:30:03,142 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4980] 2025-03-10 20:30:03,142 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
            "[INFO|configuration_utils.py:1095] 2025-03-10 20:30:03,238 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-03-10 20:30:03,238 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:61] 2025-03-10 20:30:03,954 >> Attempting to create safetensors variant\n",
            "[INFO|safetensors_conversion.py:74] 2025-03-10 20:30:04,414 >> Safetensors PR exists\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-239a0c8168036379.arrow\n",
            "03/10/2025 20:30:04 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-239a0c8168036379.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4e37c8bc3d2bbba6.arrow\n",
            "03/10/2025 20:30:05 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4e37c8bc3d2bbba6.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-283cd450680fa67d.arrow\n",
            "03/10/2025 20:30:07 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-283cd450680fa67d.arrow\n",
            "[INFO|trainer.py:2407] 2025-03-10 20:30:09,596 >> ***** Running training *****\n",
            "[INFO|trainer.py:2408] 2025-03-10 20:30:09,596 >>   Num examples = 2,000\n",
            "[INFO|trainer.py:2409] 2025-03-10 20:30:09,596 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2410] 2025-03-10 20:30:09,596 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:2412] 2025-03-10 20:30:09,596 >>   Training with DataParallel so batch size has been adjusted to: 8\n",
            "[INFO|trainer.py:2413] 2025-03-10 20:30:09,596 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2414] 2025-03-10 20:30:09,596 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2415] 2025-03-10 20:30:09,596 >>   Total optimization steps = 750\n",
            "[INFO|trainer.py:2416] 2025-03-10 20:30:09,598 >>   Number of trainable parameters = 483,905,536\n",
            "[INFO|integration_utils.py:817] 2025-03-10 20:30:09,603 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_203009-l8ju2v1r\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-2000\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/l8ju2v1r\u001b[0m\n",
            "  0% 0/750 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 4.007, 'grad_norm': 9.431608200073242, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
            " 67% 500/750 [06:15<03:05,  1.34it/s][INFO|trainer.py:3944] 2025-03-10 20:36:26,493 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-2000/checkpoint-500\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 20:36:26,496 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 20:36:26,497 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 20:36:31,570 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 20:36:31,575 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 20:36:31,576 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 20:36:31,576 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "100% 750/750 [09:41<00:00,  1.35it/s][INFO|trainer.py:3944] 2025-03-10 20:39:52,523 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-2000/checkpoint-750\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 20:39:52,525 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-750/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 20:39:52,526 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-750/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 20:39:57,871 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-750/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 20:39:57,874 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 20:39:57,875 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-750/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 20:39:57,875 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-2000/checkpoint-750/added_tokens.json\n",
            "[INFO|trainer.py:2659] 2025-03-10 20:40:11,511 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 601.9137, 'train_samples_per_second': 9.968, 'train_steps_per_second': 1.246, 'train_loss': 3.600841959635417, 'epoch': 3.0}\n",
            "100% 750/750 [10:00<00:00,  1.25it/s]\n",
            "[INFO|trainer.py:3944] 2025-03-10 20:40:11,523 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-2000\n",
            "[INFO|configuration_utils.py:423] 2025-03-10 20:40:11,524 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-2000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-10 20:40:11,525 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-2000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-10 20:40:20,440 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-2000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-10 20:40:20,444 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-10 20:40:20,444 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-2000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-10 20:40:20,445 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-2000/added_tokens.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =   409031GF\n",
            "  train_loss               =     3.6008\n",
            "  train_runtime            = 0:10:01.91\n",
            "  train_samples            =       2000\n",
            "  train_samples_per_second =      9.968\n",
            "  train_steps_per_second   =      1.246\n",
            "03/10/2025 20:40:20 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:4260] 2025-03-10 20:40:20,610 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4262] 2025-03-10 20:40:20,610 >>   Num examples = 997\n",
            "[INFO|trainer.py:4265] 2025-03-10 20:40:20,610 >>   Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "100% 125/125 [12:50<00:00,  6.16s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_bleu               =     2.3401\n",
            "  eval_gen_len            =    56.6901\n",
            "  eval_loss               =     3.7136\n",
            "  eval_runtime            = 0:12:59.20\n",
            "  eval_samples            =        997\n",
            "  eval_samples_per_second =       1.28\n",
            "  eval_steps_per_second   =       0.16\n",
            "03/10/2025 20:53:19 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:4260] 2025-03-10 20:53:19,820 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4262] 2025-03-10 20:53:19,820 >>   Num examples = 1012\n",
            "[INFO|trainer.py:4265] 2025-03-10 20:53:19,821 >>   Batch size = 8\n",
            "100% 127/127 [13:06<00:00,  6.19s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu               =     2.2356\n",
            "  predict_gen_len            =     57.754\n",
            "  predict_loss               =      3.759\n",
            "  predict_runtime            = 0:13:14.66\n",
            "  predict_samples            =       1012\n",
            "  predict_samples_per_second =      1.273\n",
            "  predict_steps_per_second   =       0.16\n",
            "[INFO|modelcard.py:449] 2025-03-10 21:06:51,407 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 2.3401}]}\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path facebook/m2m100_418M \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --source_lang en \\\n",
        "    --target_lang zu \\\n",
        "    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_2000.json \\\n",
        "    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n",
        "    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n",
        "    --num_beams 10 \\\n",
        "    --output_dir M2M-100/wmt22-cometkiwi-da-2000 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "-sq2Lrua_oqR",
        "outputId": "a9b06821-daf1-4f76-fa6d-d153c5f92a33"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250311_062813-fax4wneh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/paderborn-university/machine%20translation/runs/fax4wneh' target=\"_blank\">comet-4000</a></strong> to <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/paderborn-university/machine%20translation/runs/fax4wneh' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation/runs/fax4wneh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/fax4wneh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x78058efec220>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize WandB\n",
        "wandb.init(project=\"machine translation\", name=\"comet-4000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9KnDDS6_ynN",
        "outputId": "b555921b-b606-4094-ef28-6d41bbd4eb4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-11 06:28:31.262500: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-03-11 06:28:31.285008: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-03-11 06:28:31.291381: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-11 06:28:31.307241: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-11 06:28:32.284236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/11/2025 06:28:37 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
            "03/11/2025 06:28:37 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=2,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=M2M-100/wmt22-cometkiwi-da-4000/runs/Mar11_06-28-37_bcdd196dbaf2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=M2M-100/wmt22-cometkiwi-da-4000,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=M2M-100/wmt22-cometkiwi-da-4000,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tp_size=0,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-2c18f5caf6d15f80\n",
            "03/11/2025 06:28:37 - INFO - datasets.builder - Using custom data configuration default-2c18f5caf6d15f80\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "03/11/2025 06:28:37 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "03/11/2025 06:28:37 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/11/2025 06:28:37 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "03/11/2025 06:28:37 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/11/2025 06:28:37 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "[INFO|configuration_utils.py:699] 2025-03-11 06:28:38,075 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-11 06:28:38,077 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-03-11 06:28:38,167 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-11 06:28:38,168 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 06:28:38,169 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 06:28:38,169 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 06:28:38,169 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 06:28:38,169 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 06:28:38,169 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 06:28:38,169 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 06:28:38,169 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:699] 2025-03-11 06:28:38,170 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-11 06:28:38,170 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3984] 2025-03-11 06:28:39,133 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
            "[INFO|safetensors_conversion.py:61] 2025-03-11 06:28:39,255 >> Attempting to create safetensors variant\n",
            "[INFO|safetensors_conversion.py:74] 2025-03-11 06:28:39,616 >> Safetensors PR exists\n",
            "[INFO|configuration_utils.py:1140] 2025-03-11 06:28:39,638 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4972] 2025-03-11 06:28:39,817 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4980] 2025-03-11 06:28:39,817 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
            "[INFO|configuration_utils.py:1095] 2025-03-11 06:28:40,156 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-03-11 06:28:40,157 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b3e5c8b83b5f5d81.arrow\n",
            "03/11/2025 06:28:41 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b3e5c8b83b5f5d81.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d80ea2e702926f3c.arrow\n",
            "03/11/2025 06:28:42 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d80ea2e702926f3c.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-94a06ef0cd1537df.arrow\n",
            "03/11/2025 06:28:44 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-94a06ef0cd1537df.arrow\n",
            "[INFO|trainer.py:2407] 2025-03-11 06:28:53,391 >> ***** Running training *****\n",
            "[INFO|trainer.py:2408] 2025-03-11 06:28:53,391 >>   Num examples = 4,000\n",
            "[INFO|trainer.py:2409] 2025-03-11 06:28:53,391 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2410] 2025-03-11 06:28:53,391 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:2412] 2025-03-11 06:28:53,391 >>   Training with DataParallel so batch size has been adjusted to: 8\n",
            "[INFO|trainer.py:2413] 2025-03-11 06:28:53,391 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2414] 2025-03-11 06:28:53,392 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2415] 2025-03-11 06:28:53,392 >>   Total optimization steps = 1,500\n",
            "[INFO|trainer.py:2416] 2025-03-11 06:28:53,393 >>   Number of trainable parameters = 483,905,536\n",
            "[INFO|integration_utils.py:817] 2025-03-11 06:28:53,400 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250311_062853-rva29lg3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-4000\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/rva29lg3\u001b[0m\n",
            "  0% 0/1500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 4.3891, 'grad_norm': 6.706293106079102, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
            " 33% 500/1500 [06:06<12:25,  1.34it/s][INFO|trainer.py:3944] 2025-03-11 06:35:00,452 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-4000/checkpoint-500\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 06:35:00,458 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 06:35:00,459 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 06:35:11,923 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 06:35:11,927 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 06:35:11,927 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 06:35:11,928 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 3.2881, 'grad_norm': 7.618316650390625, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
            " 67% 1000/1500 [12:41<06:35,  1.26it/s][INFO|trainer.py:3944] 2025-03-11 06:41:35,339 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1000\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 06:41:35,340 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 06:41:35,341 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 06:41:40,189 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 06:41:40,191 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 06:41:40,192 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 06:41:40,192 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.6393, 'grad_norm': 10.235444068908691, 'learning_rate': 0.0, 'epoch': 3.0}\n",
            "100% 1500/1500 [19:04<00:00,  1.40it/s][INFO|trainer.py:3944] 2025-03-11 06:47:59,308 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1500\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 06:47:59,313 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 06:47:59,314 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 06:48:04,287 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 06:48:04,290 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 06:48:04,291 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 06:48:04,291 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-4000/checkpoint-1500/added_tokens.json\n",
            "[INFO|trainer.py:2659] 2025-03-11 06:48:18,105 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1164.713, 'train_samples_per_second': 10.303, 'train_steps_per_second': 1.288, 'train_loss': 3.438846923828125, 'epoch': 3.0}\n",
            "100% 1500/1500 [19:23<00:00,  1.29it/s]\n",
            "[INFO|trainer.py:3944] 2025-03-11 06:48:18,110 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-4000\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 06:48:18,199 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-4000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 06:48:18,200 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-4000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 06:48:32,969 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-4000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 06:48:32,973 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 06:48:32,973 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-4000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 06:48:32,974 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-4000/added_tokens.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =   849267GF\n",
            "  train_loss               =     3.4388\n",
            "  train_runtime            = 0:19:24.71\n",
            "  train_samples            =       4000\n",
            "  train_samples_per_second =     10.303\n",
            "  train_steps_per_second   =      1.288\n",
            "03/11/2025 06:48:33 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:4260] 2025-03-11 06:48:33,119 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4262] 2025-03-11 06:48:33,120 >>   Num examples = 997\n",
            "[INFO|trainer.py:4265] 2025-03-11 06:48:33,120 >>   Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "100% 125/125 [11:12<00:00,  5.38s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_bleu               =     3.4049\n",
            "  eval_gen_len            =    50.9759\n",
            "  eval_loss               =     3.3535\n",
            "  eval_runtime            = 0:11:19.80\n",
            "  eval_samples            =        997\n",
            "  eval_samples_per_second =      1.467\n",
            "  eval_steps_per_second   =      0.184\n",
            "03/11/2025 06:59:52 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:4260] 2025-03-11 06:59:52,932 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4262] 2025-03-11 06:59:52,932 >>   Num examples = 1012\n",
            "[INFO|trainer.py:4265] 2025-03-11 06:59:52,932 >>   Batch size = 8\n",
            "100% 127/127 [11:45<00:00,  5.55s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu               =     3.3486\n",
            "  predict_gen_len            =    52.8893\n",
            "  predict_loss               =     3.4086\n",
            "  predict_runtime            = 0:11:52.53\n",
            "  predict_samples            =       1012\n",
            "  predict_samples_per_second =       1.42\n",
            "  predict_steps_per_second   =      0.178\n",
            "[INFO|modelcard.py:449] 2025-03-11 07:12:01,170 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 3.4049}]}\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path facebook/m2m100_418M \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --source_lang en \\\n",
        "    --target_lang zu \\\n",
        "    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_4000.json \\\n",
        "    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n",
        "    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n",
        "    --num_beams 10 \\\n",
        "    --output_dir M2M-100/wmt22-cometkiwi-da-4000 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zgBkTtEQ_qHT",
        "outputId": "cefc32f0-a820-4c4b-b2f7-0483f139a3b9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/fax4wneh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x78058efec220>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize WandB\n",
        "wandb.init(project=\"machine translation\", name=\"comet-8000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnLgBK_Q_0Nt",
        "outputId": "743a55c6-529b-4968-b687-ec5c450364c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-11 07:12:15.544791: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-03-11 07:12:15.566555: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-03-11 07:12:15.573051: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-11 07:12:15.589312: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-11 07:12:16.661912: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/11/2025 07:12:21 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
            "03/11/2025 07:12:21 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=2,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=M2M-100/wmt22-cometkiwi-da-8000/runs/Mar11_07-12-21_bcdd196dbaf2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=M2M-100/wmt22-cometkiwi-da-8000,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=M2M-100/wmt22-cometkiwi-da-8000,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tp_size=0,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-5bc63a9eaf840ac0\n",
            "03/11/2025 07:12:21 - INFO - datasets.builder - Using custom data configuration default-5bc63a9eaf840ac0\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "03/11/2025 07:12:21 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "03/11/2025 07:12:21 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/11/2025 07:12:21 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "03/11/2025 07:12:21 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/11/2025 07:12:21 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "[INFO|configuration_utils.py:699] 2025-03-11 07:12:22,204 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-11 07:12:22,206 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-03-11 07:12:22,312 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-11 07:12:22,313 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 07:12:22,314 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:699] 2025-03-11 07:12:22,315 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-11 07:12:22,315 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3984] 2025-03-11 07:12:23,259 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:1140] 2025-03-11 07:12:23,339 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4972] 2025-03-11 07:12:23,447 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4980] 2025-03-11 07:12:23,447 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
            "[INFO|safetensors_conversion.py:61] 2025-03-11 07:12:23,553 >> Attempting to create safetensors variant\n",
            "[INFO|configuration_utils.py:1095] 2025-03-11 07:12:23,732 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-03-11 07:12:23,733 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:74] 2025-03-11 07:12:24,483 >> Safetensors PR exists\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-805d80dbaf682edc.arrow\n",
            "03/11/2025 07:12:25 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-805d80dbaf682edc.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-8d35743ea3961495.arrow\n",
            "03/11/2025 07:12:26 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-8d35743ea3961495.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a687ca1a7df8988f.arrow\n",
            "03/11/2025 07:12:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a687ca1a7df8988f.arrow\n",
            "[INFO|trainer.py:2407] 2025-03-11 07:12:37,293 >> ***** Running training *****\n",
            "[INFO|trainer.py:2408] 2025-03-11 07:12:37,293 >>   Num examples = 8,000\n",
            "[INFO|trainer.py:2409] 2025-03-11 07:12:37,293 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2410] 2025-03-11 07:12:37,293 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:2412] 2025-03-11 07:12:37,293 >>   Training with DataParallel so batch size has been adjusted to: 8\n",
            "[INFO|trainer.py:2413] 2025-03-11 07:12:37,293 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2414] 2025-03-11 07:12:37,293 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2415] 2025-03-11 07:12:37,293 >>   Total optimization steps = 3,000\n",
            "[INFO|trainer.py:2416] 2025-03-11 07:12:37,294 >>   Number of trainable parameters = 483,905,536\n",
            "[INFO|integration_utils.py:817] 2025-03-11 07:12:37,301 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250311_071237-d7dmi3j0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-8000\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/d7dmi3j0\u001b[0m\n",
            "  0% 0/3000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 4.3577, 'grad_norm': 8.726241111755371, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n",
            " 17% 500/3000 [05:59<29:48,  1.40it/s][INFO|trainer.py:3944] 2025-03-11 07:18:38,000 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 07:18:38,012 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 07:18:38,012 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 07:18:49,502 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:18:49,505 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:18:49,506 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:18:49,506 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 3.8395, 'grad_norm': 8.096263885498047, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
            " 33% 1000/3000 [12:30<24:33,  1.36it/s][INFO|trainer.py:3944] 2025-03-11 07:25:08,462 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 07:25:08,464 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 07:25:08,465 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 07:25:13,388 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:25:13,391 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:25:13,392 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:25:13,392 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 3.045, 'grad_norm': 7.830597400665283, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n",
            " 50% 1500/3000 [18:47<18:04,  1.38it/s][INFO|trainer.py:3944] 2025-03-11 07:31:25,260 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 07:31:25,262 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 07:31:25,263 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 07:31:30,226 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:31:30,229 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:31:30,230 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:31:30,230 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-1500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.9503, 'grad_norm': 7.9375739097595215, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
            " 67% 2000/3000 [25:05<11:27,  1.45it/s][INFO|trainer.py:3944] 2025-03-11 07:37:44,118 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 07:37:44,120 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 07:37:44,121 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 07:37:49,103 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:37:49,107 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:37:49,107 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:37:49,108 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.4077, 'grad_norm': 8.3837308883667, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n",
            " 83% 2500/3000 [31:22<05:56,  1.40it/s][INFO|trainer.py:3944] 2025-03-11 07:44:00,670 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 07:44:00,673 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 07:44:00,674 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 07:44:05,567 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:44:05,570 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:44:05,570 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:44:05,571 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-2500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.359, 'grad_norm': 7.027705192565918, 'learning_rate': 0.0, 'epoch': 3.0}\n",
            "100% 3000/3000 [37:42<00:00,  1.38it/s][INFO|trainer.py:3944] 2025-03-11 07:50:20,600 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 07:50:20,602 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 07:50:20,602 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 07:50:25,487 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:50:25,489 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:50:25,490 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:50:25,490 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/checkpoint-3000/added_tokens.json\n",
            "[INFO|trainer.py:2659] 2025-03-11 07:50:39,111 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2281.8171, 'train_samples_per_second': 10.518, 'train_steps_per_second': 1.315, 'train_loss': 3.159888224283854, 'epoch': 3.0}\n",
            "100% 3000/3000 [38:00<00:00,  1.32it/s]\n",
            "[INFO|trainer.py:3944] 2025-03-11 07:50:39,116 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-8000\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 07:50:39,120 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 07:50:39,169 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-8000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 07:50:54,302 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-8000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 07:50:54,306 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-8000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 07:50:54,307 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 07:50:54,309 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-8000/added_tokens.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =  1672691GF\n",
            "  train_loss               =     3.1599\n",
            "  train_runtime            = 0:38:01.81\n",
            "  train_samples            =       8000\n",
            "  train_samples_per_second =     10.518\n",
            "  train_steps_per_second   =      1.315\n",
            "03/11/2025 07:50:54 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:4260] 2025-03-11 07:50:54,447 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4262] 2025-03-11 07:50:54,447 >>   Num examples = 997\n",
            "[INFO|trainer.py:4265] 2025-03-11 07:50:54,447 >>   Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "100% 125/125 [09:24<00:00,  4.52s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_bleu               =     5.5443\n",
            "  eval_gen_len            =    47.0752\n",
            "  eval_loss               =     2.9433\n",
            "  eval_runtime            = 0:09:30.03\n",
            "  eval_samples            =        997\n",
            "  eval_samples_per_second =      1.749\n",
            "  eval_steps_per_second   =      0.219\n",
            "03/11/2025 08:00:24 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:4260] 2025-03-11 08:00:24,491 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4262] 2025-03-11 08:00:24,492 >>   Num examples = 1012\n",
            "[INFO|trainer.py:4265] 2025-03-11 08:00:24,492 >>   Batch size = 8\n",
            "100% 127/127 [10:32<00:00,  4.98s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu               =     4.8857\n",
            "  predict_gen_len            =    49.6897\n",
            "  predict_loss               =     3.0054\n",
            "  predict_runtime            = 0:10:40.04\n",
            "  predict_samples            =       1012\n",
            "  predict_samples_per_second =      1.581\n",
            "  predict_steps_per_second   =      0.198\n",
            "[INFO|modelcard.py:449] 2025-03-11 08:11:20,174 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 5.5443}]}\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path facebook/m2m100_418M \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --source_lang en \\\n",
        "    --target_lang zu \\\n",
        "    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_8000.json \\\n",
        "    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n",
        "    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n",
        "    --num_beams 10 \\\n",
        "    --output_dir M2M-100/wmt22-cometkiwi-da-8000 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "38xgPP_G_rjq",
        "outputId": "c92baf51-4e22-401b-b4ab-ef5f48f8bb19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250311_084418-vjxkfu79</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/paderborn-university/machine%20translation/runs/vjxkfu79' target=\"_blank\">comet-16000</a></strong> to <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/paderborn-university/machine%20translation/runs/vjxkfu79' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation/runs/vjxkfu79</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/vjxkfu79?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x79baa0d2c430>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize WandB\n",
        "wandb.init(project=\"machine translation\", name=\"comet-16000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8A_KBlC_2Q_",
        "outputId": "16d9b74c-8102-4bf9-df55-eaf28a670771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-11 08:45:17.490707: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-03-11 08:45:17.512410: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-03-11 08:45:17.519113: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-11 08:45:17.535362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-11 08:45:18.555378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/11/2025 08:45:23 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
            "03/11/2025 08:45:23 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=2,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=M2M-100/wmt22-cometkiwi-da-16000/runs/Mar11_08-45-23_bcdd196dbaf2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=M2M-100/wmt22-cometkiwi-da-16000,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=M2M-100/wmt22-cometkiwi-da-16000,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tp_size=0,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-8dd97eb64194ed63\n",
            "03/11/2025 08:45:24 - INFO - datasets.builder - Using custom data configuration default-8dd97eb64194ed63\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "03/11/2025 08:45:24 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "03/11/2025 08:45:24 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/11/2025 08:45:24 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "03/11/2025 08:45:24 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "03/11/2025 08:45:24 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "[INFO|configuration_utils.py:699] 2025-03-11 08:45:24,413 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-11 08:45:24,416 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-03-11 08:45:24,503 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-11 08:45:24,504 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-03-11 08:45:24,505 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:699] 2025-03-11 08:45:24,506 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-11 08:45:24,507 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"facebook/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.50.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3984] 2025-03-11 08:45:25,474 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
            "[INFO|safetensors_conversion.py:61] 2025-03-11 08:45:25,779 >> Attempting to create safetensors variant\n",
            "[INFO|configuration_utils.py:1140] 2025-03-11 08:45:25,983 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4972] 2025-03-11 08:45:26,161 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4980] 2025-03-11 08:45:26,161 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
            "[INFO|configuration_utils.py:1095] 2025-03-11 08:45:26,289 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-03-11 08:45:26,289 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 200,\n",
            "  \"num_beams\": 5,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:74] 2025-03-11 08:45:26,435 >> Safetensors PR exists\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4f73fd23cbbc2044.arrow\n",
            "03/11/2025 08:45:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4f73fd23cbbc2044.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-61f573cc25eca874.arrow\n",
            "03/11/2025 08:45:28 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-61f573cc25eca874.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-29c03a83c2242779.arrow\n",
            "03/11/2025 08:45:30 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-29c03a83c2242779.arrow\n",
            "[INFO|trainer.py:2407] 2025-03-11 08:45:39,481 >> ***** Running training *****\n",
            "[INFO|trainer.py:2408] 2025-03-11 08:45:39,481 >>   Num examples = 16,000\n",
            "[INFO|trainer.py:2409] 2025-03-11 08:45:39,481 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2410] 2025-03-11 08:45:39,481 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:2412] 2025-03-11 08:45:39,481 >>   Training with DataParallel so batch size has been adjusted to: 8\n",
            "[INFO|trainer.py:2413] 2025-03-11 08:45:39,481 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2414] 2025-03-11 08:45:39,481 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2415] 2025-03-11 08:45:39,482 >>   Total optimization steps = 6,000\n",
            "[INFO|trainer.py:2416] 2025-03-11 08:45:39,483 >>   Number of trainable parameters = 483,905,536\n",
            "[INFO|integration_utils.py:817] 2025-03-11 08:45:39,490 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250311_084539-hctutu3t\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-16000\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/hctutu3t\u001b[0m\n",
            "  0% 0/6000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 4.4124, 'grad_norm': 7.936241149902344, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}\n",
            "  8% 500/6000 [06:05<1:05:10,  1.41it/s][INFO|trainer.py:3944] 2025-03-11 08:51:45,992 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 08:51:45,998 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 08:51:45,999 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 08:51:57,453 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 08:51:57,457 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 08:51:57,457 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 08:51:57,458 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 3.8947, 'grad_norm': 6.913146018981934, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n",
            " 17% 1000/6000 [12:42<1:03:27,  1.31it/s][INFO|trainer.py:3944] 2025-03-11 08:58:22,858 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 08:58:22,860 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 08:58:22,861 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 08:58:27,705 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 08:58:27,709 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 08:58:27,709 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 08:58:27,710 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 3.5953, 'grad_norm': 7.811131954193115, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}\n",
            " 25% 1500/6000 [19:05<55:15,  1.36it/s][INFO|trainer.py:3944] 2025-03-11 09:04:46,081 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 09:04:46,083 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 09:04:46,084 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 09:04:50,966 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:04:50,969 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:04:50,969 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:04:50,970 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-1500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 3.4118, 'grad_norm': 6.7470479011535645, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
            " 33% 2000/6000 [25:30<48:39,  1.37it/s][INFO|trainer.py:3944] 2025-03-11 09:11:11,353 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 09:11:11,354 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 09:11:11,355 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 09:11:16,272 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:11:16,276 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:11:16,276 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:11:16,276 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.8207, 'grad_norm': 7.763012409210205, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}\n",
            " 42% 2500/6000 [31:54<42:08,  1.38it/s][INFO|trainer.py:3944] 2025-03-11 09:17:34,481 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 09:17:34,482 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 09:17:34,483 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 09:17:39,362 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:17:39,365 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:17:39,365 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:17:39,366 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-2500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.7635, 'grad_norm': 6.0602593421936035, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n",
            " 50% 3000/6000 [38:18<36:51,  1.36it/s][INFO|trainer.py:3944] 2025-03-11 09:23:59,149 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 09:23:59,151 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 09:23:59,151 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 09:24:04,056 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:24:04,059 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:24:04,059 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:24:04,060 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.7337, 'grad_norm': 7.55765962600708, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}\n",
            " 58% 3500/6000 [44:44<30:26,  1.37it/s][INFO|trainer.py:3944] 2025-03-11 09:30:24,508 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 09:30:24,509 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 09:30:24,510 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 09:30:29,367 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:30:29,370 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:30:29,371 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:30:29,371 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-3500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.6668, 'grad_norm': 6.23962926864624, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
            " 67% 4000/6000 [51:07<25:24,  1.31it/s][INFO|trainer.py:3944] 2025-03-11 09:36:47,496 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 09:36:47,498 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 09:36:47,498 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 09:36:52,406 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:36:52,409 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:36:52,409 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:36:52,410 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.2207, 'grad_norm': 6.355872631072998, 'learning_rate': 1.25e-05, 'epoch': 2.25}\n",
            " 75% 4500/6000 [57:31<18:13,  1.37it/s][INFO|trainer.py:3944] 2025-03-11 09:43:12,085 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 09:43:12,087 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 09:43:12,088 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 09:43:17,005 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:43:17,008 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:43:17,009 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:43:17,009 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-4500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.2021, 'grad_norm': 7.565613746643066, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n",
            " 83% 5000/6000 [1:03:55<12:06,  1.38it/s][INFO|trainer.py:3944] 2025-03-11 09:49:36,015 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 09:49:36,017 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 09:49:36,017 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 09:49:40,929 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:49:40,932 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:49:40,933 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:49:40,933 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5000/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.1973, 'grad_norm': 14.712209701538086, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}\n",
            " 92% 5500/6000 [1:10:18<05:52,  1.42it/s][INFO|trainer.py:3944] 2025-03-11 09:55:59,282 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 09:55:59,284 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 09:55:59,285 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 09:56:04,201 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 09:56:04,204 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 09:56:04,204 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 09:56:04,205 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-5500/added_tokens.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "{'loss': 2.1321, 'grad_norm': 7.183525085449219, 'learning_rate': 0.0, 'epoch': 3.0}\n",
            "100% 6000/6000 [1:16:43<00:00,  1.38it/s][INFO|trainer.py:3944] 2025-03-11 10:02:23,522 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 10:02:23,525 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 10:02:23,526 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 10:02:28,451 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 10:02:28,454 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 10:02:28,454 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 10:02:28,455 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/checkpoint-6000/added_tokens.json\n",
            "[INFO|trainer.py:2659] 2025-03-11 10:02:42,100 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 4622.6168, 'train_samples_per_second': 10.384, 'train_steps_per_second': 1.298, 'train_loss': 2.920919514973958, 'epoch': 3.0}\n",
            "100% 6000/6000 [1:17:01<00:00,  1.30it/s]\n",
            "[INFO|trainer.py:3944] 2025-03-11 10:02:42,110 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-16000\n",
            "[INFO|configuration_utils.py:423] 2025-03-11 10:02:42,137 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/config.json\n",
            "[INFO|configuration_utils.py:909] 2025-03-11 10:02:42,138 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-16000/generation_config.json\n",
            "[INFO|modeling_utils.py:3040] 2025-03-11 10:02:57,056 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-16000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-11 10:02:57,060 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-16000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-03-11 10:02:57,061 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-03-11 10:02:57,061 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-16000/added_tokens.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =  3307792GF\n",
            "  train_loss               =     2.9209\n",
            "  train_runtime            = 1:17:02.61\n",
            "  train_samples            =      16000\n",
            "  train_samples_per_second =     10.384\n",
            "  train_steps_per_second   =      1.298\n",
            "03/11/2025 10:02:57 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:4260] 2025-03-11 10:02:57,210 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4262] 2025-03-11 10:02:57,211 >>   Num examples = 997\n",
            "[INFO|trainer.py:4265] 2025-03-11 10:02:57,211 >>   Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n",
            "100% 125/125 [09:38<00:00,  4.63s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_bleu               =     6.7478\n",
            "  eval_gen_len            =    46.4132\n",
            "  eval_loss               =     2.5626\n",
            "  eval_runtime            = 0:09:46.79\n",
            "  eval_samples            =        997\n",
            "  eval_samples_per_second =      1.699\n",
            "  eval_steps_per_second   =      0.213\n",
            "03/11/2025 10:12:44 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:4260] 2025-03-11 10:12:44,006 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4262] 2025-03-11 10:12:44,006 >>   Num examples = 1012\n",
            "[INFO|trainer.py:4265] 2025-03-11 10:12:44,006 >>   Batch size = 8\n",
            "100% 127/127 [10:18<00:00,  4.87s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu               =     6.2998\n",
            "  predict_gen_len            =    48.8399\n",
            "  predict_loss               =     2.6004\n",
            "  predict_runtime            = 0:10:25.58\n",
            "  predict_samples            =       1012\n",
            "  predict_samples_per_second =      1.618\n",
            "  predict_steps_per_second   =      0.203\n",
            "[INFO|modelcard.py:449] 2025-03-11 10:23:25,008 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 6.7478}]}\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path facebook/m2m100_418M \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --source_lang en \\\n",
        "    --target_lang zu \\\n",
        "    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_16000.json \\\n",
        "    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n",
        "    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n",
        "    --num_beams 10 \\\n",
        "    --output_dir M2M-100/wmt22-cometkiwi-da-16000 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wslf2Suxpp-R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
>>>>>>> 0b9f19581db47efa598d1e35e34d5dd482341b74
