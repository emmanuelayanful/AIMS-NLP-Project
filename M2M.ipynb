{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1840,"status":"ok","timestamp":1743353790310,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"vqsa3QW1weGy"},"outputs":[],"source":["import wandb\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ICCsdcJZpub"},"outputs":[],"source":["# !rm -rf Google_T5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEU-1o2gFQkC"},"outputs":[],"source":["!chmod u+x /content/AIMS-NLP-Project/run_translation.py"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"GPgjaPdU-cgP","executionInfo":{"status":"ok","timestamp":1743353790311,"user_tz":-120,"elapsed":2,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"outputs":[],"source":["language = \"Zulu\""]},{"cell_type":"markdown","metadata":{"id":"qZQLQPZjy2QO"},"source":["# Baseline Run"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"executionInfo":{"elapsed":12867,"status":"ok","timestamp":1742633941883,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"aggyfHIUR2zp","outputId":"bf1462d2-74be-4094-d305-ebc5aa0d39a2"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.19.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20250322_085854-0njzblna</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna' target=\"_blank\">baseline</a></strong> to <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a7e2df34250>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Log in with your API key\n","wandb.login(key=\"7f13d9fe09d0856f7c12099a27ccda7aa15c8afd\")\n","\n","# Initialize WandB\n","wandb.init(project=\"machine translation\", name=f\"baseline_{language}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1874933,"status":"ok","timestamp":1742637345127,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"VvM-Zf4jUO81","outputId":"342ba6d4-0c7f-4ffa-f7a4-5ac29cda097e"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-22 09:00:54.631515: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 09:00:54.653331: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 09:00:54.660040: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-22 09:00:54.676194: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-22 09:00:55.669976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/22/2025 09:00:58 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/22/2025 09:00:58 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/baseline/runs/Mar22_09-00-57_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/baseline,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/baseline,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-fcf11d994dda434e\n","03/22/2025 09:00:58 - INFO - datasets.builder - Using custom data configuration default-fcf11d994dda434e\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/22/2025 09:00:58 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/22/2025 09:00:58 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 09:00:58 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/22/2025 09:00:58 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 09:00:58 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-22 09:00:58,508 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 09:00:58,510 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-22 09:00:58,595 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 09:00:58,596 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:00:58,596 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:00:58,596 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:00:58,597 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:00:58,597 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:00:58,597 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:00:58,597 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:00:58,597 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-22 09:00:58,597 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 09:00:58,598 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-22 09:00:59,550 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|safetensors_conversion.py:61] 2025-03-22 09:00:59,735 >> Attempting to create safetensors variant\n","[INFO|safetensors_conversion.py:74] 2025-03-22 09:01:00,105 >> Safetensors PR exists\n","[INFO|configuration_utils.py:1140] 2025-03-22 09:01:00,192 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-22 09:01:00,363 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-22 09:01:00,363 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-22 09:01:00,453 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-22 09:01:00,454 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-500813e1b41c1f29.arrow\n","03/22/2025 09:01:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-500813e1b41c1f29.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-24d83da570939502.arrow\n","03/22/2025 09:01:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-24d83da570939502.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c4510aa2c36da9e3.arrow\n","03/22/2025 09:01:04 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c4510aa2c36da9e3.arrow\n","Downloading builder script: 100% 9.01k/9.01k [00:00<00:00, 15.4MB/s]\n","[INFO|trainer.py:2407] 2025-03-22 09:01:14,521 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-22 09:01:14,521 >>   Num examples = 5,737\n","[INFO|trainer.py:2409] 2025-03-22 09:01:14,521 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-22 09:01:14,521 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-22 09:01:14,521 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-22 09:01:14,521 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-22 09:01:14,521 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-22 09:01:14,521 >>   Total optimization steps = 2,154\n","[INFO|trainer.py:2416] 2025-03-22 09:01:14,522 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-22 09:01:14,529 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250322_090114-yu3f7j7b\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/baseline\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/yu3f7j7b\u001b[0m\n","  0% 0/2154 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.6395, 'grad_norm': 6.515448093414307, 'learning_rate': 3.8393686165273915e-05, 'epoch': 0.7}\n"," 23% 500/2154 [07:26<23:44,  1.16it/s][INFO|trainer.py:3944] 2025-03-22 09:08:41,635 >> Saving model checkpoint to M2M-100/baseline/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-22 09:08:41,644 >> Configuration saved in M2M-100/baseline/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 09:08:41,645 >> Configuration saved in M2M-100/baseline/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 09:08:53,122 >> Model weights saved in M2M-100/baseline/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 09:08:53,126 >> tokenizer config file saved in M2M-100/baseline/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 09:08:53,127 >> Special tokens file saved in M2M-100/baseline/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 09:08:53,127 >> added tokens file saved in M2M-100/baseline/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.7846, 'grad_norm': 6.764095783233643, 'learning_rate': 2.678737233054782e-05, 'epoch': 1.39}\n"," 46% 1000/2154 [15:24<15:48,  1.22it/s][INFO|trainer.py:3944] 2025-03-22 09:16:39,965 >> Saving model checkpoint to M2M-100/baseline/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-22 09:16:39,970 >> Configuration saved in M2M-100/baseline/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 09:16:39,971 >> Configuration saved in M2M-100/baseline/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 09:16:51,462 >> Model weights saved in M2M-100/baseline/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 09:16:51,465 >> tokenizer config file saved in M2M-100/baseline/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 09:16:51,466 >> Special tokens file saved in M2M-100/baseline/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 09:16:51,466 >> added tokens file saved in M2M-100/baseline/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3359, 'grad_norm': 6.238084316253662, 'learning_rate': 1.518105849582173e-05, 'epoch': 2.09}\n"," 70% 1500/2154 [23:24<09:20,  1.17it/s][INFO|trainer.py:3944] 2025-03-22 09:24:39,800 >> Saving model checkpoint to M2M-100/baseline/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-22 09:24:39,809 >> Configuration saved in M2M-100/baseline/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 09:24:39,810 >> Configuration saved in M2M-100/baseline/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 09:24:51,286 >> Model weights saved in M2M-100/baseline/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 09:24:51,288 >> tokenizer config file saved in M2M-100/baseline/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 09:24:51,289 >> Special tokens file saved in M2M-100/baseline/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 09:24:51,289 >> added tokens file saved in M2M-100/baseline/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9657, 'grad_norm': 6.731072425842285, 'learning_rate': 3.574744661095636e-06, 'epoch': 2.79}\n"," 93% 2000/2154 [31:22<02:20,  1.10it/s][INFO|trainer.py:3944] 2025-03-22 09:32:38,220 >> Saving model checkpoint to M2M-100/baseline/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-22 09:32:38,225 >> Configuration saved in M2M-100/baseline/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 09:32:38,226 >> Configuration saved in M2M-100/baseline/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 09:32:49,793 >> Model weights saved in M2M-100/baseline/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 09:32:49,797 >> tokenizer config file saved in M2M-100/baseline/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 09:32:49,798 >> Special tokens file saved in M2M-100/baseline/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 09:32:49,798 >> added tokens file saved in M2M-100/baseline/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 2154/2154 [34:09<00:00,  1.51it/s][INFO|trainer.py:3944] 2025-03-22 09:35:24,559 >> Saving model checkpoint to M2M-100/baseline/checkpoint-2154\n","[INFO|configuration_utils.py:423] 2025-03-22 09:35:24,562 >> Configuration saved in M2M-100/baseline/checkpoint-2154/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 09:35:24,563 >> Configuration saved in M2M-100/baseline/checkpoint-2154/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 09:35:36,155 >> Model weights saved in M2M-100/baseline/checkpoint-2154/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 09:35:36,158 >> tokenizer config file saved in M2M-100/baseline/checkpoint-2154/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 09:35:36,159 >> Special tokens file saved in M2M-100/baseline/checkpoint-2154/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 09:35:36,159 >> added tokens file saved in M2M-100/baseline/checkpoint-2154/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-22 09:35:54,195 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2079.6733, 'train_samples_per_second': 8.276, 'train_steps_per_second': 1.036, 'train_loss': 2.626155800141996, 'epoch': 3.0}\n","100% 2154/2154 [34:38<00:00,  1.04it/s]\n","[INFO|trainer.py:3944] 2025-03-22 09:35:54,204 >> Saving model checkpoint to M2M-100/baseline\n","[INFO|configuration_utils.py:423] 2025-03-22 09:35:54,205 >> Configuration saved in M2M-100/baseline/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 09:35:54,206 >> Configuration saved in M2M-100/baseline/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 09:36:05,773 >> Model weights saved in M2M-100/baseline/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 09:36:05,776 >> tokenizer config file saved in M2M-100/baseline/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 09:36:05,776 >> Special tokens file saved in M2M-100/baseline/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 09:36:05,777 >> added tokens file saved in M2M-100/baseline/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  2504202GF\n","  train_loss               =     2.6262\n","  train_runtime            = 0:34:39.67\n","  train_samples            =       5737\n","  train_samples_per_second =      8.276\n","  train_steps_per_second   =      1.036\n","03/22/2025 09:36:05 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-22 09:36:05,916 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-22 09:36:05,916 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-22 09:36:05,916 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:10<00:00,  4.40s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =      7.053\n","  eval_chrf               =    41.9664\n","  eval_gen_len            =    46.8676\n","  eval_loss               =     2.5353\n","  eval_runtime            = 0:09:16.35\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.792\n","  eval_steps_per_second   =      0.225\n","03/22/2025 09:45:22 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-22 09:45:22,277 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-22 09:45:22,277 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-22 09:45:22,278 >>   Batch size = 8\n","100% 127/127 [09:48<00:00,  4.63s/it]\n","***** predict metrics *****\n","  predict_bleu               =     7.2825\n","  predict_chrf               =    41.7196\n","  predict_gen_len            =    48.5593\n","  predict_loss               =     2.5738\n","  predict_runtime            = 0:09:55.51\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.699\n","  predict_steps_per_second   =      0.213\n","[INFO|modelcard.py:449] 2025-03-22 09:55:33,296 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 7.053}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/mafand/en-zu/merged.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/baseline \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"markdown","metadata":{"id":"1OEaIczVafcf"},"source":["# Africomet with top 1000"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"elapsed":648,"status":"ok","timestamp":1742637406571,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"Mp6AahGfufdK","outputId":"c1457e4e-1886-4351-af51-e7abef95c143"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a7e2df34250>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=f\"africomet_{language}-1000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1353332,"status":"ok","timestamp":1742639311479,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"hbNrB5boVIgU","outputId":"1fe98c73-6580-46b2-ee67-fef61d3771e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-22 09:56:56.806833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 09:56:56.828987: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 09:56:56.836168: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-22 09:56:56.852748: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-22 09:56:57.914509: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/22/2025 09:57:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/22/2025 09:57:00 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-1000/runs/Mar22_09-57-00_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-1000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-1000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-d22542853ecd2369\n","03/22/2025 09:57:00 - INFO - datasets.builder - Using custom data configuration default-d22542853ecd2369\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/22/2025 09:57:00 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/22/2025 09:57:00 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 09:57:00 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/22/2025 09:57:00 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 09:57:00 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-22 09:57:01,010 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 09:57:01,011 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-22 09:57:01,095 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 09:57:01,096 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:57:01,097 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:57:01,097 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:57:01,097 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:57:01,097 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:57:01,097 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:57:01,097 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 09:57:01,097 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-22 09:57:01,097 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 09:57:01,098 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-22 09:57:02,147 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-22 09:57:02,217 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-22 09:57:02,254 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-22 09:57:02,327 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-22 09:57:02,327 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-22 09:57:02,411 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-22 09:57:02,412 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-22 09:57:02,688 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e70b427f4acd1a90.arrow\n","03/22/2025 09:57:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e70b427f4acd1a90.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1e1e9cb1ba5e0af7.arrow\n","03/22/2025 09:57:05 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1e1e9cb1ba5e0af7.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b37e93e0dc9f95ce.arrow\n","03/22/2025 09:57:06 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b37e93e0dc9f95ce.arrow\n","[INFO|trainer.py:2407] 2025-03-22 09:57:16,033 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-22 09:57:16,033 >>   Num examples = 1,000\n","[INFO|trainer.py:2409] 2025-03-22 09:57:16,033 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-22 09:57:16,033 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-22 09:57:16,033 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-22 09:57:16,033 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-22 09:57:16,033 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-22 09:57:16,033 >>   Total optimization steps = 375\n","[INFO|trainer.py:2416] 2025-03-22 09:57:16,034 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-22 09:57:16,047 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250322_095716-zqoky06s\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-1000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/zqoky06s\u001b[0m\n","  0% 0/375 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 375/375 [04:32<00:00,  1.41it/s][INFO|trainer.py:3944] 2025-03-22 10:01:49,415 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-22 10:01:49,424 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 10:01:49,425 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 10:02:01,112 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 10:02:01,116 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 10:02:01,116 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 10:02:01,117 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-375/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-22 10:02:18,962 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 302.9291, 'train_samples_per_second': 9.903, 'train_steps_per_second': 1.238, 'train_loss': 3.718626627604167, 'epoch': 3.0}\n","100% 375/375 [05:02<00:00,  1.24it/s]\n","[INFO|trainer.py:3944] 2025-03-22 10:02:18,966 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-1000\n","[INFO|configuration_utils.py:423] 2025-03-22 10:02:18,968 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 10:02:18,968 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 10:02:30,512 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 10:02:30,516 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 10:02:30,516 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 10:02:30,517 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =   205627GF\n","  train_loss               =     3.7186\n","  train_runtime            = 0:05:02.92\n","  train_samples            =       1000\n","  train_samples_per_second =      9.903\n","  train_steps_per_second   =      1.238\n","03/22/2025 10:02:30 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-22 10:02:30,660 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-22 10:02:30,660 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-22 10:02:30,661 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [12:24<00:00,  5.96s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     2.1764\n","  eval_chrf               =     20.964\n","  eval_gen_len            =    55.7432\n","  eval_loss               =     3.9585\n","  eval_runtime            = 0:12:31.81\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.326\n","  eval_steps_per_second   =      0.166\n","03/22/2025 10:15:02 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-22 10:15:02,488 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-22 10:15:02,488 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-22 10:15:02,488 >>   Batch size = 8\n","100% 127/127 [13:01<00:00,  6.15s/it]\n","***** predict metrics *****\n","  predict_bleu               =     1.7597\n","  predict_chrf               =    20.5157\n","  predict_gen_len            =       57.0\n","  predict_loss               =     3.9932\n","  predict_runtime            = 0:13:10.13\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.281\n","  predict_steps_per_second   =      0.161\n","[INFO|modelcard.py:449] 2025-03-22 10:28:28,622 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 2.1764}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_1000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-1000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"elapsed":1552,"status":"ok","timestamp":1742639474702,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"hNlChp_8JQJr","outputId":"8f6ab5e6-85a9-43a7-9853-48d28d0f6ff3"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a7e2df34250>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init(project=\"machine translation\", name=f\"africomet_{language}-plus-1000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":869476,"status":"ok","timestamp":1742643222135,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"XCaK_LmtJbGp","outputId":"58942fe0-0f64-4d52-a41f-fbb018ef354e"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-22 10:31:54.836387: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 10:31:54.858128: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 10:31:54.864716: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-22 10:31:54.883951: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-22 10:31:55.950268: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/22/2025 10:31:58 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/22/2025 10:31:58 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-plus-1000/runs/Mar22_10-31-58_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-plus-1000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-plus-1000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-d09fb30739d7f4ef\n","03/22/2025 10:31:58 - INFO - datasets.builder - Using custom data configuration default-d09fb30739d7f4ef\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/22/2025 10:31:58 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/22/2025 10:31:58 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 10:31:58 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/22/2025 10:31:58 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 10:31:58 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-22 10:31:58,960 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 10:31:58,961 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-22 10:31:59,046 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 10:31:59,046 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 10:31:59,047 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 10:31:59,047 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 10:31:59,047 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 10:31:59,047 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 10:31:59,047 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 10:31:59,047 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 10:31:59,047 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-22 10:31:59,048 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 10:31:59,048 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-22 10:31:59,966 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-22 10:32:00,035 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-22 10:32:00,077 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-22 10:32:00,139 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-22 10:32:00,139 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-22 10:32:00,230 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-22 10:32:00,231 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-22 10:32:00,459 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-23e62197603ff7b5.arrow\n","03/22/2025 10:32:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-23e62197603ff7b5.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0d3debe34b3fe7c6.arrow\n","03/22/2025 10:32:02 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0d3debe34b3fe7c6.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7608692f880482db.arrow\n","03/22/2025 10:32:04 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-d09fb30739d7f4ef/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7608692f880482db.arrow\n","[INFO|trainer.py:2407] 2025-03-22 10:32:09,639 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-22 10:32:09,639 >>   Num examples = 6,737\n","[INFO|trainer.py:2409] 2025-03-22 10:32:09,639 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-22 10:32:09,639 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-22 10:32:09,639 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-22 10:32:09,639 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-22 10:32:09,639 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-22 10:32:09,639 >>   Total optimization steps = 2,529\n","[INFO|trainer.py:2416] 2025-03-22 10:32:09,641 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-22 10:32:09,647 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250322_103209-0toqe8lw\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-plus-1000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/0toqe8lw\u001b[0m\n","  0% 0/2529 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.7582, 'grad_norm': 5.092475414276123, 'learning_rate': 4.011466982997232e-05, 'epoch': 0.59}\n"," 20% 500/2529 [07:27<30:42,  1.10it/s][INFO|trainer.py:3944] 2025-03-22 10:39:38,039 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-22 10:39:38,044 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 10:39:38,045 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 10:39:49,478 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 10:39:49,481 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 10:39:49,482 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 10:39:49,482 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.9769, 'grad_norm': 5.819267272949219, 'learning_rate': 3.0229339659944645e-05, 'epoch': 1.19}\n"," 40% 1000/2529 [15:23<22:59,  1.11it/s][INFO|trainer.py:3944] 2025-03-22 10:47:33,653 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-22 10:47:33,656 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 10:47:33,657 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 10:47:45,177 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 10:47:45,182 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 10:47:45,182 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 10:47:45,183 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4669, 'grad_norm': 5.872149467468262, 'learning_rate': 2.0344009489916967e-05, 'epoch': 1.78}\n"," 59% 1500/2529 [23:19<15:47,  1.09it/s][INFO|trainer.py:3944] 2025-03-22 10:55:30,146 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-22 10:55:30,151 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 10:55:30,151 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 10:55:41,735 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 10:55:41,739 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 10:55:41,740 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 10:55:41,740 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.13, 'grad_norm': 6.943439960479736, 'learning_rate': 1.0458679319889285e-05, 'epoch': 2.37}\n"," 79% 2000/2529 [31:16<07:47,  1.13it/s][INFO|trainer.py:3944] 2025-03-22 11:03:26,797 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-22 11:03:26,802 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 11:03:26,803 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 11:03:38,366 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 11:03:38,369 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 11:03:38,370 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 11:03:38,370 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9484, 'grad_norm': 5.667131423950195, 'learning_rate': 5.733491498616055e-07, 'epoch': 2.97}\n"," 99% 2500/2529 [39:12<00:25,  1.14it/s][INFO|trainer.py:3944] 2025-03-22 11:11:22,975 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-22 11:11:22,980 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 11:11:22,980 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 11:11:34,485 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 11:11:34,489 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 11:11:34,490 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 11:11:34,491 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 2529/2529 [40:07<00:00,  1.36it/s][INFO|trainer.py:3944] 2025-03-22 11:12:17,646 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529\n","[INFO|configuration_utils.py:423] 2025-03-22 11:12:17,649 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 11:12:17,650 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 11:12:29,343 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 11:12:29,347 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 11:12:29,348 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 11:12:29,348 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/checkpoint-2529/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-22 11:12:47,583 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2437.9438, 'train_samples_per_second': 8.29, 'train_steps_per_second': 1.037, 'train_loss': 2.647326598010925, 'epoch': 3.0}\n","100% 2529/2529 [40:36<00:00,  1.04it/s]\n","[INFO|trainer.py:3944] 2025-03-22 11:12:47,591 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-1000\n","[INFO|configuration_utils.py:423] 2025-03-22 11:12:47,593 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 11:12:47,593 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 11:12:59,040 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 11:12:59,043 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 11:12:59,043 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 11:12:59,044 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-1000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  2830543GF\n","  train_loss               =     2.6473\n","  train_runtime            = 0:40:37.94\n","  train_samples            =       6737\n","  train_samples_per_second =       8.29\n","  train_steps_per_second   =      1.037\n","03/22/2025 11:12:59 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-22 11:12:59,190 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-22 11:12:59,190 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-22 11:12:59,190 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:47<00:00,  4.70s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     7.5784\n","  eval_chrf               =    42.7323\n","  eval_gen_len            =    47.5797\n","  eval_loss               =      2.476\n","  eval_runtime            = 0:09:53.81\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.679\n","  eval_steps_per_second   =      0.211\n","03/22/2025 11:22:53 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-22 11:22:53,016 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-22 11:22:53,016 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-22 11:22:53,017 >>   Batch size = 8\n","100% 127/127 [10:22<00:00,  4.90s/it]\n","***** predict metrics *****\n","  predict_bleu               =     6.9985\n","  predict_chrf               =    41.5549\n","  predict_gen_len            =    49.2016\n","  predict_loss               =     2.5135\n","  predict_runtime            = 0:10:30.58\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.605\n","  predict_steps_per_second   =      0.201\n","[INFO|modelcard.py:449] 2025-03-22 11:33:39,797 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 7.5784}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/trainer_1000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-plus-1000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"id":"X0K_kEpR-cgY","executionInfo":{"status":"ok","timestamp":1743353823805,"user_tz":-120,"elapsed":11519,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"1c0f9b6a-5111-49c5-f1ac-cb04c59926da"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250330_165658-3jgivrj7</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/paderborn-university/machine%20translation/runs/3jgivrj7' target=\"_blank\">random_Zulu-1000</a></strong> to <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/paderborn-university/machine%20translation/runs/3jgivrj7' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation/runs/3jgivrj7</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/3jgivrj7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7d1bd3cf4580>"]},"metadata":{},"execution_count":3}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=f\"random_{language}-1000\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LGRpKh00-cgY","executionInfo":{"status":"ok","timestamp":1743354822945,"user_tz":-120,"elapsed":701531,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"52cbbb8f-9bf8-4e33-c66a-bfc744539221"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-30 17:02:05.531544: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-30 17:02:05.552633: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-30 17:02:05.559220: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-30 17:02:05.576011: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-30 17:02:06.606859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/30/2025 17:02:09 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/30/2025 17:02:09 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/random-1000/runs/Mar30_17-02-08_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/random-1000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/random-1000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-d1348ad11d437974\n","03/30/2025 17:02:09 - INFO - datasets.builder - Using custom data configuration default-d1348ad11d437974\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/30/2025 17:02:09 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Generating dataset json (/root/.cache/huggingface/datasets/json/default-d1348ad11d437974/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/30/2025 17:02:09 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-d1348ad11d437974/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-d1348ad11d437974/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","03/30/2025 17:02:09 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-d1348ad11d437974/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","Downloading took 0.0 min\n","03/30/2025 17:02:09 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","03/30/2025 17:02:09 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Generating train split\n","03/30/2025 17:02:09 - INFO - datasets.builder - Generating train split\n","Generating train split: 2000 examples [00:00, 93552.90 examples/s]\n","Generating validation split\n","03/30/2025 17:02:09 - INFO - datasets.builder - Generating validation split\n","Generating validation split: 997 examples [00:00, 193231.42 examples/s]\n","Generating test split\n","03/30/2025 17:02:09 - INFO - datasets.builder - Generating test split\n","Generating test split: 1012 examples [00:00, 214929.14 examples/s]\n","Unable to verify splits sizes.\n","03/30/2025 17:02:09 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-d1348ad11d437974/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","03/30/2025 17:02:09 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-d1348ad11d437974/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","[INFO|configuration_utils.py:699] 2025-03-30 17:02:09,493 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-30 17:02:09,495 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-30 17:02:09,583 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-30 17:02:09,584 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-30 17:02:09,585 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-30 17:02:09,585 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-30 17:02:09,586 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-30 17:02:09,586 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-30 17:02:09,586 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-30 17:02:09,586 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-30 17:02:09,586 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-30 17:02:09,586 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-30 17:02:09,587 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-30 17:02:11,452 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|safetensors_conversion.py:61] 2025-03-30 17:02:11,561 >> Attempting to create safetensors variant\n","[INFO|safetensors_conversion.py:74] 2025-03-30 17:02:11,871 >> Safetensors PR exists\n","[INFO|configuration_utils.py:1140] 2025-03-30 17:02:12,291 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-30 17:02:12,468 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-30 17:02:12,468 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-30 17:02:12,559 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-30 17:02:12,560 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","Running tokenizer on train dataset:   0% 0/2000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d1348ad11d437974/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e6385d8b8afabfdc.arrow\n","03/30/2025 17:02:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d1348ad11d437974/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e6385d8b8afabfdc.arrow\n","Running tokenizer on train dataset: 100% 2000/2000 [00:00<00:00, 3577.11 examples/s]\n","Running tokenizer on validation dataset:   0% 0/997 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d1348ad11d437974/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-afc1fe3ca37944b1.arrow\n","03/30/2025 17:02:16 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d1348ad11d437974/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-afc1fe3ca37944b1.arrow\n","Running tokenizer on validation dataset: 100% 997/997 [00:00<00:00, 1878.82 examples/s]\n","Running tokenizer on prediction dataset:   0% 0/1012 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d1348ad11d437974/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2da2b2aa282a240f.arrow\n","03/30/2025 17:02:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d1348ad11d437974/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2da2b2aa282a240f.arrow\n","Running tokenizer on prediction dataset: 100% 1012/1012 [00:00<00:00, 1802.28 examples/s]\n","[INFO|trainer.py:2407] 2025-03-30 17:02:28,759 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-30 17:02:28,759 >>   Num examples = 2,000\n","[INFO|trainer.py:2409] 2025-03-30 17:02:28,759 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-30 17:02:28,760 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-30 17:02:28,760 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-30 17:02:28,760 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-30 17:02:28,760 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-30 17:02:28,760 >>   Total optimization steps = 750\n","[INFO|trainer.py:2416] 2025-03-30 17:02:28,762 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-30 17:02:28,771 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250330_170228-ezw64cl4\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/random-1000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/ezw64cl4\u001b[0m\n","  0% 0/750 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.2394, 'grad_norm': 8.503841400146484, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 500/750 [06:02<02:59,  1.40it/s][INFO|trainer.py:3944] 2025-03-30 17:08:32,056 >> Saving model checkpoint to M2M-100/random-1000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-30 17:08:32,066 >> Configuration saved in M2M-100/random-1000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-30 17:08:32,066 >> Configuration saved in M2M-100/random-1000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-30 17:08:37,095 >> Model weights saved in M2M-100/random-1000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-30 17:08:37,098 >> tokenizer config file saved in M2M-100/random-1000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-30 17:08:37,099 >> Special tokens file saved in M2M-100/random-1000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-30 17:08:37,099 >> added tokens file saved in M2M-100/random-1000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 750/750 [09:21<00:00,  1.42it/s][INFO|trainer.py:3944] 2025-03-30 17:11:51,498 >> Saving model checkpoint to M2M-100/random-1000/checkpoint-750\n","[INFO|configuration_utils.py:423] 2025-03-30 17:11:51,500 >> Configuration saved in M2M-100/random-1000/checkpoint-750/config.json\n","[INFO|configuration_utils.py:909] 2025-03-30 17:11:51,500 >> Configuration saved in M2M-100/random-1000/checkpoint-750/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-30 17:11:56,642 >> Model weights saved in M2M-100/random-1000/checkpoint-750/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-30 17:11:56,645 >> tokenizer config file saved in M2M-100/random-1000/checkpoint-750/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-30 17:11:56,645 >> Special tokens file saved in M2M-100/random-1000/checkpoint-750/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-30 17:11:56,646 >> added tokens file saved in M2M-100/random-1000/checkpoint-750/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-30 17:12:10,577 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 581.8158, 'train_samples_per_second': 10.313, 'train_steps_per_second': 1.289, 'train_loss': 3.823823567708333, 'epoch': 3.0}\n","100% 750/750 [09:40<00:00,  1.29it/s]\n","[INFO|trainer.py:3944] 2025-03-30 17:12:10,588 >> Saving model checkpoint to M2M-100/random-1000\n","[INFO|configuration_utils.py:423] 2025-03-30 17:12:10,590 >> Configuration saved in M2M-100/random-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-30 17:12:10,591 >> Configuration saved in M2M-100/random-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-30 17:12:19,390 >> Model weights saved in M2M-100/random-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-30 17:12:19,394 >> tokenizer config file saved in M2M-100/random-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-30 17:12:19,394 >> Special tokens file saved in M2M-100/random-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-30 17:12:19,395 >> added tokens file saved in M2M-100/random-1000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =   377779GF\n","  train_loss               =     3.8238\n","  train_runtime            = 0:09:41.81\n","  train_samples            =       2000\n","  train_samples_per_second =     10.313\n","  train_steps_per_second   =      1.289\n","03/30/2025 17:12:19 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-30 17:12:19,547 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-30 17:12:19,548 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-30 17:12:19,548 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n"," 10% 13/125 [01:08<10:12,  5.47s/it]Traceback (most recent call last):\n","  File \"/content/AIMS-NLP-Project/run_translation.py\", line 706, in <module>\n","    main()\n","  File \"/content/AIMS-NLP-Project/run_translation.py\", line 645, in main\n","    metrics = trainer.evaluate(max_length=max_length, num_beams=num_beams, metric_key_prefix=\"eval\")\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_seq2seq.py\", line 197, in evaluate\n","    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 4107, in evaluate\n","    output = eval_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 4301, in evaluation_loop\n","    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_seq2seq.py\", line 333, in prediction_step\n","    generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2258, in generate\n","    result = self._beam_search(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 3535, in _beam_search\n","    beam_outputs = beam_scorer.process(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/beam_search.py\", line 255, in process\n","    if self._done[batch_group_idx]:\n","KeyboardInterrupt\n"," 10% 13/125 [01:13<10:30,  5.63s/it]\n","^C\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/random/en-zu/train_1000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/random-1000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"elapsed":592,"status":"ok","timestamp":1742643238911,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"oAhoqvCbuk06","outputId":"7ca2308e-1f3c-46dd-f52f-dad773deea17"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a7e2df34250>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=f\"africomet_{language}-2000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1854023,"status":"ok","timestamp":1742645413262,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"S0Vip1YNtpQn","outputId":"8bef0194-7dec-47d9-d889-9d1e5bff57d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-22 11:34:07.492651: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 11:34:07.514254: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 11:34:07.520847: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-22 11:34:07.536833: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-22 11:34:08.551261: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/22/2025 11:34:10 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/22/2025 11:34:10 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-2000/runs/Mar22_11-34-10_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-2000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-2000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-63fa3ea70d279152\n","03/22/2025 11:34:11 - INFO - datasets.builder - Using custom data configuration default-63fa3ea70d279152\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/22/2025 11:34:11 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/22/2025 11:34:11 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 11:34:11 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/22/2025 11:34:11 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 11:34:11 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-22 11:34:11,355 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 11:34:11,357 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-22 11:34:11,440 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 11:34:11,441 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 11:34:11,442 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 11:34:11,442 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 11:34:11,442 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 11:34:11,442 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 11:34:11,442 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 11:34:11,442 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 11:34:11,442 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-22 11:34:11,442 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 11:34:11,443 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-22 11:34:12,314 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-22 11:34:12,378 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-22 11:34:12,427 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-22 11:34:12,483 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-22 11:34:12,483 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-22 11:34:12,569 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-22 11:34:12,569 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-22 11:34:12,891 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-239a0c8168036379.arrow\n","03/22/2025 11:34:13 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-239a0c8168036379.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4e37c8bc3d2bbba6.arrow\n","03/22/2025 11:34:15 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4e37c8bc3d2bbba6.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-283cd450680fa67d.arrow\n","03/22/2025 11:34:16 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-283cd450680fa67d.arrow\n","[INFO|trainer.py:2407] 2025-03-22 11:34:19,466 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-22 11:34:19,466 >>   Num examples = 2,000\n","[INFO|trainer.py:2409] 2025-03-22 11:34:19,466 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-22 11:34:19,466 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-22 11:34:19,466 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-22 11:34:19,466 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-22 11:34:19,466 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-22 11:34:19,466 >>   Total optimization steps = 750\n","[INFO|trainer.py:2416] 2025-03-22 11:34:19,467 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-22 11:34:19,478 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250322_113419-oj48qc4m\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-2000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/oj48qc4m\u001b[0m\n","  0% 0/750 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.0068, 'grad_norm': 9.566374778747559, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 500/750 [06:08<03:02,  1.37it/s][INFO|trainer.py:3944] 2025-03-22 11:40:28,779 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-22 11:40:28,784 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 11:40:28,785 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 11:40:40,273 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 11:40:40,276 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 11:40:40,276 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 11:40:40,277 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 750/750 [09:41<00:00,  1.37it/s][INFO|trainer.py:3944] 2025-03-22 11:44:02,043 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750\n","[INFO|configuration_utils.py:423] 2025-03-22 11:44:02,046 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 11:44:02,046 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 11:44:13,808 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 11:44:13,813 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 11:44:13,813 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 11:44:13,814 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-750/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-22 11:44:32,070 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 612.603, 'train_samples_per_second': 9.794, 'train_steps_per_second': 1.224, 'train_loss': 3.6032134602864585, 'epoch': 3.0}\n","100% 750/750 [10:11<00:00,  1.23it/s]\n","[INFO|trainer.py:3944] 2025-03-22 11:44:32,078 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-2000\n","[INFO|configuration_utils.py:423] 2025-03-22 11:44:32,080 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 11:44:32,080 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 11:44:43,632 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 11:44:43,635 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 11:44:43,635 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 11:44:43,636 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =   409031GF\n","  train_loss               =     3.6032\n","  train_runtime            = 0:10:12.60\n","  train_samples            =       2000\n","  train_samples_per_second =      9.794\n","  train_steps_per_second   =      1.224\n","03/22/2025 11:44:43 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-22 11:44:43,776 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-22 11:44:43,776 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-22 11:44:43,776 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [11:53<00:00,  5.71s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     2.8885\n","  eval_chrf               =    24.2956\n","  eval_gen_len            =    54.0822\n","  eval_loss               =     3.7205\n","  eval_runtime            = 0:12:02.26\n","  eval_samples            =        997\n","  eval_samples_per_second =       1.38\n","  eval_steps_per_second   =      0.173\n","03/22/2025 11:56:46 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-22 11:56:46,052 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-22 11:56:46,052 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-22 11:56:46,052 >>   Batch size = 8\n","100% 127/127 [13:00<00:00,  6.14s/it]\n","***** predict metrics *****\n","  predict_bleu               =     2.5867\n","  predict_chrf               =    23.9403\n","  predict_gen_len            =    55.6423\n","  predict_loss               =     3.7725\n","  predict_runtime            = 0:13:08.91\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.283\n","  predict_steps_per_second   =      0.161\n","[INFO|modelcard.py:449] 2025-03-22 12:10:10,690 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 2.8885}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_2000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-2000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"elapsed":215,"status":"ok","timestamp":1742645413263,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"NRaFN03_pcfD","outputId":"7a20afbb-9ed7-4e98-dfd9-e6525e71490d"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a7e2df34250>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init(project=\"machine translation\", name=f\"africomet_{language}-plus-2000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2465846,"status":"ok","timestamp":1742649304176,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"sEJT63ZLpVl-","outputId":"5b9e554f-64e5-4bd2-e072-0a25a4676489"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-22 12:10:16.914157: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 12:10:16.936412: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 12:10:16.943127: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-22 12:10:16.959268: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-22 12:10:17.957737: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/22/2025 12:10:20 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/22/2025 12:10:20 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-plus-2000/runs/Mar22_12-10-20_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-plus-2000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-plus-2000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-00c1f33ddcf7c443\n","03/22/2025 12:10:20 - INFO - datasets.builder - Using custom data configuration default-00c1f33ddcf7c443\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/22/2025 12:10:20 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/22/2025 12:10:20 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 12:10:20 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/22/2025 12:10:20 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 12:10:20 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-22 12:10:20,738 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 12:10:20,739 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-22 12:10:20,828 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 12:10:20,829 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 12:10:20,829 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 12:10:20,829 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 12:10:20,829 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 12:10:20,829 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 12:10:20,830 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 12:10:20,830 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 12:10:20,830 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-22 12:10:20,830 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 12:10:20,831 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-22 12:10:21,993 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-22 12:10:22,058 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-22 12:10:22,159 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-22 12:10:22,159 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|safetensors_conversion.py:61] 2025-03-22 12:10:22,233 >> Attempting to create safetensors variant\n","[INFO|configuration_utils.py:1095] 2025-03-22 12:10:22,252 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-22 12:10:22,252 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-22 12:10:22,771 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d9db24e4ea92c1e2.arrow\n","03/22/2025 12:10:23 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d9db24e4ea92c1e2.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9e62119e384f3dba.arrow\n","03/22/2025 12:10:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9e62119e384f3dba.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2488e0046852a6d9.arrow\n","03/22/2025 12:10:26 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-00c1f33ddcf7c443/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2488e0046852a6d9.arrow\n","[INFO|trainer.py:2407] 2025-03-22 12:10:28,846 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-22 12:10:28,846 >>   Num examples = 7,737\n","[INFO|trainer.py:2409] 2025-03-22 12:10:28,846 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-22 12:10:28,846 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-22 12:10:28,846 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-22 12:10:28,846 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-22 12:10:28,846 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-22 12:10:28,846 >>   Total optimization steps = 2,904\n","[INFO|trainer.py:2416] 2025-03-22 12:10:28,847 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-22 12:10:28,854 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250322_121028-xdtr0l5a\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-plus-2000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/xdtr0l5a\u001b[0m\n","  0% 0/2904 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.8378, 'grad_norm': 5.276322841644287, 'learning_rate': 4.139118457300275e-05, 'epoch': 0.52}\n"," 17% 500/2904 [07:21<36:08,  1.11it/s][INFO|trainer.py:3944] 2025-03-22 12:17:50,743 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-22 12:17:50,746 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 12:17:50,747 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 12:17:55,568 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 12:17:55,571 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 12:17:55,572 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 12:17:55,572 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.1627, 'grad_norm': 5.931947231292725, 'learning_rate': 3.278236914600551e-05, 'epoch': 1.03}\n"," 34% 1000/2904 [14:58<28:36,  1.11it/s][INFO|trainer.py:3944] 2025-03-22 12:25:27,745 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-22 12:25:27,747 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 12:25:27,747 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 12:25:32,664 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 12:25:32,667 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 12:25:32,668 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 12:25:32,668 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.5523, 'grad_norm': 5.885161876678467, 'learning_rate': 2.4173553719008264e-05, 'epoch': 1.55}\n"," 52% 1500/2904 [22:33<20:29,  1.14it/s][INFO|trainer.py:3944] 2025-03-22 12:33:03,687 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-22 12:33:03,689 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 12:33:03,690 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 12:33:08,539 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 12:33:08,542 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 12:33:08,543 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 12:33:08,543 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3528, 'grad_norm': 6.115939617156982, 'learning_rate': 1.5564738292011018e-05, 'epoch': 2.07}\n"," 69% 2000/2904 [30:10<13:14,  1.14it/s][INFO|trainer.py:3944] 2025-03-22 12:40:39,945 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-22 12:40:39,947 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 12:40:39,947 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 12:40:44,829 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 12:40:44,832 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 12:40:44,832 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 12:40:44,833 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9936, 'grad_norm': 6.911382675170898, 'learning_rate': 6.955922865013774e-06, 'epoch': 2.58}\n"," 86% 2500/2904 [37:50<05:38,  1.19it/s][INFO|trainer.py:3944] 2025-03-22 12:48:20,187 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-22 12:48:20,188 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 12:48:20,189 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 12:48:25,111 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 12:48:25,114 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 12:48:25,115 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 12:48:25,115 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 2904/2904 [43:58<00:00,  1.40it/s][INFO|trainer.py:3944] 2025-03-22 12:54:27,744 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2904\n","[INFO|configuration_utils.py:423] 2025-03-22 12:54:27,746 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2904/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 12:54:27,747 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2904/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 12:54:32,793 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2904/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 12:54:32,795 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2904/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 12:54:32,796 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2904/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 12:54:32,796 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/checkpoint-2904/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-22 12:54:46,617 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2657.7704, 'train_samples_per_second': 8.733, 'train_steps_per_second': 1.093, 'train_loss': 2.66652855991332, 'epoch': 3.0}\n","100% 2904/2904 [44:16<00:00,  1.09it/s]\n","[INFO|trainer.py:3944] 2025-03-22 12:54:46,622 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-2000\n","[INFO|configuration_utils.py:423] 2025-03-22 12:54:46,624 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 12:54:46,625 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 12:54:55,583 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 12:54:55,587 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 12:54:55,588 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 12:54:55,588 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-2000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  3124969GF\n","  train_loss               =     2.6665\n","  train_runtime            = 0:44:17.77\n","  train_samples            =       7737\n","  train_samples_per_second =      8.733\n","  train_steps_per_second   =      1.093\n","03/22/2025 12:54:55 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-22 12:54:55,729 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-22 12:54:55,729 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-22 12:54:55,729 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:20<00:00,  4.48s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =      7.606\n","  eval_chrf               =    42.7291\n","  eval_gen_len            =    46.9649\n","  eval_loss               =     2.4326\n","  eval_runtime            = 0:09:26.32\n","  eval_samples            =        997\n","  eval_samples_per_second =       1.76\n","  eval_steps_per_second   =      0.221\n","03/22/2025 13:04:22 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-22 13:04:22,064 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-22 13:04:22,064 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-22 13:04:22,065 >>   Batch size = 8\n","100% 127/127 [10:13<00:00,  4.83s/it]\n","***** predict metrics *****\n","  predict_bleu               =     7.5372\n","  predict_chrf               =    42.2996\n","  predict_gen_len            =     48.833\n","  predict_loss               =     2.4674\n","  predict_runtime            = 0:10:20.33\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.631\n","  predict_steps_per_second   =      0.205\n","[INFO|modelcard.py:449] 2025-03-22 13:14:58,109 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 7.606}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/trainer_2000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-plus-2000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"5SFZ2Tgf-cgZ","executionInfo":{"status":"ok","timestamp":1743354822945,"user_tz":-120,"elapsed":7,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"6deacf5a-b0bb-445c-82ac-18faabf45302"},"outputs":[{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/3jgivrj7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7d1bd3cf4580>"]},"metadata":{},"execution_count":6}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=f\"random_{language}-2000\")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SyAWS4Th-cgZ","executionInfo":{"status":"ok","timestamp":1743354831295,"user_tz":-120,"elapsed":8354,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"831feb6e-a951-4302-817c-d673d7a225d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-30 17:13:47.143600: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-30 17:13:47.165221: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-30 17:13:47.172518: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-30 17:13:47.189612: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-30 17:13:48.304409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Traceback (most recent call last):\n","  File \"/content/AIMS-NLP-Project/run_translation.py\", line 28, in <module>\n","    import evaluate\n","  File \"/usr/local/lib/python3.10/dist-packages/evaluate/__init__.py\", line 29, in <module>\n","    from .evaluation_suite import EvaluationSuite\n","  File \"/usr/local/lib/python3.10/dist-packages/evaluate/evaluation_suite/__init__.py\", line 10, in <module>\n","    from ..evaluator import evaluator\n","  File \"/usr/local/lib/python3.10/dist-packages/evaluate/evaluator/__init__.py\", line 17, in <module>\n","    from transformers.pipelines import SUPPORTED_TASKS as SUPPORTED_PIPELINE_TASKS\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\", line 26, in <module>\n","    from ..image_processing_utils import BaseImageProcessor\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py\", line 22, in <module>\n","    from .image_transforms import center_crop, normalize, rescale\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\", line 48, in <module>\n","    import tensorflow as tf\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\", line 53, in <module>\n","    from tensorflow._api.v2 import compat\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/compat/__init__.py\", line 8, in <module>\n","    from tensorflow._api.v2.compat import v1\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py\", line 30, in <module>\n","    from tensorflow._api.v2.compat.v1 import compat\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\", line 8, in <module>\n","    from tensorflow._api.v2.compat.v1.compat import v1\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\", line 47, in <module>\n","    from tensorflow._api.v2.compat.v1 import lite\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py\", line 9, in <module>\n","    from tensorflow._api.v2.compat.v1.lite import experimental\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py\", line 8, in <module>\n","    from tensorflow._api.v2.compat.v1.lite.experimental import authoring\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py\", line 8, in <module>\n","    from tensorflow.lite.python.authoring.authoring import compatible # line: 265\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/authoring/authoring.py\", line 44, in <module>\n","    from tensorflow.lite.python import lite\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\", line 33, in <module>\n","    from tensorflow.lite.experimental.microfrontend.python.ops import audio_microfrontend_op  # pylint: disable=unused-import\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/lite/experimental/microfrontend/python/ops/audio_microfrontend_op.py\", line 24, in <module>\n","    _audio_microfrontend_op = load_library.load_op_library(\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/load_library.py\", line 54, in load_op_library\n","    lib_handle = py_tf.TF_LoadLibrary(library_filename)\n","KeyboardInterrupt\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/random/en-zu/train_2000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/random-2000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1435,"status":"ok","timestamp":1742649366291,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"0v4YeYRgumqu","outputId":"aef268cc-af4b-4a8d-c6a0-87c8aa83c6d0"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a7e2df34250>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=f\"africomet_{language}-4000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":315259,"status":"ok","timestamp":1742651990496,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"yGUYnc2wtrby","outputId":"97b59ec9-0741-4582-8899-f451dd2e5fa0"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-22 13:16:13.983970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 13:16:14.005927: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 13:16:14.013098: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-22 13:16:14.030972: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-22 13:16:15.362446: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/22/2025 13:16:18 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/22/2025 13:16:18 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-4000/runs/Mar22_13-16-18_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-4000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-4000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-2c18f5caf6d15f80\n","03/22/2025 13:16:18 - INFO - datasets.builder - Using custom data configuration default-2c18f5caf6d15f80\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/22/2025 13:16:18 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/22/2025 13:16:18 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 13:16:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/22/2025 13:16:18 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/22/2025 13:16:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-22 13:16:18,687 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 13:16:18,688 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-22 13:16:18,894 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 13:16:18,895 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 13:16:18,896 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 13:16:18,896 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 13:16:18,896 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 13:16:18,896 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 13:16:18,896 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 13:16:18,896 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 13:16:18,896 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-22 13:16:18,896 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 13:16:18,897 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-22 13:16:19,894 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-22 13:16:19,970 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-22 13:16:20,006 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-22 13:16:20,075 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-22 13:16:20,075 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-22 13:16:20,163 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-22 13:16:20,164 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-22 13:16:20,400 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b3e5c8b83b5f5d81.arrow\n","03/22/2025 13:16:21 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b3e5c8b83b5f5d81.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d80ea2e702926f3c.arrow\n","03/22/2025 13:16:22 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d80ea2e702926f3c.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-94a06ef0cd1537df.arrow\n","03/22/2025 13:16:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2c18f5caf6d15f80/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-94a06ef0cd1537df.arrow\n","[INFO|trainer.py:2407] 2025-03-22 13:16:26,887 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-22 13:16:26,887 >>   Num examples = 4,000\n","[INFO|trainer.py:2409] 2025-03-22 13:16:26,887 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-22 13:16:26,887 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-22 13:16:26,887 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-22 13:16:26,887 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-22 13:16:26,887 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-22 13:16:26,887 >>   Total optimization steps = 1,500\n","[INFO|trainer.py:2416] 2025-03-22 13:16:26,888 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-22 13:16:26,899 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250322_131627-yxsftebf\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-4000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/yxsftebf\u001b[0m\n","  0% 0/1500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.3832, 'grad_norm': 6.679620265960693, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"," 33% 500/1500 [06:12<12:41,  1.31it/s][INFO|trainer.py:3944] 2025-03-22 13:22:40,359 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-22 13:22:40,365 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 13:22:40,366 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 13:22:51,898 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 13:22:51,904 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 13:22:51,904 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 13:22:51,905 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.2835, 'grad_norm': 7.958261489868164, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 1000/1500 [12:53<06:44,  1.24it/s][INFO|trainer.py:3944] 2025-03-22 13:29:21,123 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-22 13:29:21,126 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 13:29:21,127 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 13:29:32,646 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 13:29:32,650 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 13:29:32,650 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 13:29:32,650 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.6419, 'grad_norm': 9.801591873168945, 'learning_rate': 0.0, 'epoch': 3.0}\n","100% 1500/1500 [19:34<00:00,  1.37it/s][INFO|trainer.py:3944] 2025-03-22 13:36:02,341 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-22 13:36:02,345 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 13:36:02,346 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 13:36:13,868 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 13:36:13,872 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 13:36:13,873 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 13:36:13,873 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/checkpoint-1500/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-22 13:36:31,709 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 1204.8211, 'train_samples_per_second': 9.96, 'train_steps_per_second': 1.245, 'train_loss': 3.436231201171875, 'epoch': 3.0}\n","100% 1500/1500 [20:03<00:00,  1.25it/s]\n","[INFO|trainer.py:3944] 2025-03-22 13:36:31,712 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-4000\n","[INFO|configuration_utils.py:423] 2025-03-22 13:36:31,714 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 13:36:31,715 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-4000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 13:36:43,184 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 13:36:43,188 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 13:36:43,188 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 13:36:43,189 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-4000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =   849267GF\n","  train_loss               =     3.4362\n","  train_runtime            = 0:20:04.82\n","  train_samples            =       4000\n","  train_samples_per_second =       9.96\n","  train_steps_per_second   =      1.245\n","03/22/2025 13:36:43 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-22 13:36:43,327 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-22 13:36:43,327 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-22 13:36:43,328 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [10:49<00:00,  5.20s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     3.7529\n","  eval_chrf               =    28.1439\n","  eval_gen_len            =    49.7332\n","  eval_loss               =     3.3484\n","  eval_runtime            = 0:10:58.37\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.514\n","  eval_steps_per_second   =       0.19\n","03/22/2025 13:47:41 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-22 13:47:41,708 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-22 13:47:41,708 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-22 13:47:41,708 >>   Batch size = 8\n","100% 127/127 [11:42<00:00,  5.53s/it]\n","***** predict metrics *****\n","  predict_bleu               =      3.384\n","  predict_chrf               =    27.3592\n","  predict_gen_len            =    51.6087\n","  predict_loss               =     3.4078\n","  predict_runtime            = 0:11:51.00\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.423\n","  predict_steps_per_second   =      0.179\n","[INFO|modelcard.py:449] 2025-03-22 13:59:48,360 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 3.7529}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_4000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-4000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"elapsed":534,"status":"ok","timestamp":1742656189031,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"NbZGn4hdAM3V","outputId":"2f954fa1-4b78-4494-c05a-796be1d8f31c"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/0njzblna?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7a7e2df34250>"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init(project=\"machine translation\", name=f\"africomet_{language}-plus-4000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2013339,"status":"ok","timestamp":1742660701508,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"pC_AF5g-AVy3","outputId":"336fc9ce-e522-4e59-8bd1-33efb7d085e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-22 15:09:56.838909: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-22 15:09:56.860287: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-22 15:09:56.866879: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-22 15:09:56.882884: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-22 15:09:57.914709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/22/2025 15:10:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/22/2025 15:10:00 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-plus-4000/runs/Mar22_15-10-00_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-plus-4000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-plus-4000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-f3c362cd2250520f\n","03/22/2025 15:10:00 - INFO - datasets.builder - Using custom data configuration default-f3c362cd2250520f\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/22/2025 15:10:00 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Generating dataset json (/root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/22/2025 15:10:00 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","03/22/2025 15:10:00 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","Downloading took 0.0 min\n","03/22/2025 15:10:00 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","03/22/2025 15:10:00 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Generating train split\n","03/22/2025 15:10:00 - INFO - datasets.builder - Generating train split\n","Generating train split: 9737 examples [00:00, 207328.27 examples/s]\n","Generating validation split\n","03/22/2025 15:10:00 - INFO - datasets.builder - Generating validation split\n","Generating validation split: 997 examples [00:00, 218047.82 examples/s]\n","Generating test split\n","03/22/2025 15:10:00 - INFO - datasets.builder - Generating test split\n","Generating test split: 1012 examples [00:00, 234756.69 examples/s]\n","Unable to verify splits sizes.\n","03/22/2025 15:10:00 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","03/22/2025 15:10:00 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","[INFO|configuration_utils.py:699] 2025-03-22 15:10:00,804 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 15:10:00,806 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-22 15:10:00,905 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 15:10:00,905 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 15:10:00,906 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 15:10:00,906 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 15:10:00,906 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 15:10:00,906 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 15:10:00,906 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 15:10:00,906 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-22 15:10:00,906 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-22 15:10:00,907 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-22 15:10:00,907 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-22 15:10:01,817 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-22 15:10:01,881 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-22 15:10:01,977 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-22 15:10:01,977 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-22 15:10:02,067 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-22 15:10:02,067 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-22 15:10:02,748 >> Attempting to create safetensors variant\n","[INFO|safetensors_conversion.py:74] 2025-03-22 15:10:03,168 >> Safetensors PR exists\n","Running tokenizer on train dataset:   0% 0/9737 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a84d01f0e3c506e2.arrow\n","03/22/2025 15:10:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a84d01f0e3c506e2.arrow\n","Running tokenizer on train dataset: 100% 9737/9737 [00:04<00:00, 2147.16 examples/s]\n","Running tokenizer on validation dataset:   0% 0/997 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-3ba84a83df7f0235.arrow\n","03/22/2025 15:10:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-3ba84a83df7f0235.arrow\n","Running tokenizer on validation dataset: 100% 997/997 [00:00<00:00, 1854.50 examples/s]\n","Running tokenizer on prediction dataset:   0% 0/1012 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-6ca21061951ac471.arrow\n","03/22/2025 15:10:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-f3c362cd2250520f/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-6ca21061951ac471.arrow\n","Running tokenizer on prediction dataset: 100% 1012/1012 [00:00<00:00, 1856.60 examples/s]\n","[INFO|trainer.py:2407] 2025-03-22 15:10:15,163 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-22 15:10:15,163 >>   Num examples = 9,737\n","[INFO|trainer.py:2409] 2025-03-22 15:10:15,163 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-22 15:10:15,163 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-22 15:10:15,163 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-22 15:10:15,164 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-22 15:10:15,164 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-22 15:10:15,164 >>   Total optimization steps = 3,654\n","[INFO|trainer.py:2416] 2025-03-22 15:10:15,165 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-22 15:10:15,170 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250322_151015-tpjlmnb0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-plus-4000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/tpjlmnb0\u001b[0m\n","  0% 0/3654 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.949, 'grad_norm': 8.702216148376465, 'learning_rate': 4.315818281335523e-05, 'epoch': 0.41}\n"," 14% 500/3654 [07:09<42:47,  1.23it/s][INFO|trainer.py:3944] 2025-03-22 15:17:26,301 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-22 15:17:26,304 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 15:17:26,305 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 15:17:31,194 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 15:17:31,197 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 15:17:31,197 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 15:17:31,198 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.3469, 'grad_norm': 7.719375133514404, 'learning_rate': 3.6316365626710456e-05, 'epoch': 0.82}\n"," 27% 1000/3654 [14:39<39:48,  1.11it/s][INFO|trainer.py:3944] 2025-03-22 15:24:55,714 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-22 15:24:55,716 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 15:24:55,717 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 15:25:00,711 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 15:25:00,714 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 15:25:00,714 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 15:25:00,715 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.8412, 'grad_norm': 6.0307440757751465, 'learning_rate': 2.947454844006568e-05, 'epoch': 1.23}\n"," 41% 1500/3654 [22:06<28:17,  1.27it/s][INFO|trainer.py:3944] 2025-03-22 15:32:22,783 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-22 15:32:22,784 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 15:32:22,785 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 15:32:27,716 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 15:32:27,720 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 15:32:27,720 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 15:32:27,721 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.5354, 'grad_norm': 5.58447790145874, 'learning_rate': 2.263273125342091e-05, 'epoch': 1.64}\n"," 55% 2000/3654 [29:35<23:13,  1.19it/s][INFO|trainer.py:3944] 2025-03-22 15:39:51,869 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-22 15:39:51,871 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 15:39:51,872 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 15:39:56,733 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 15:39:56,736 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 15:39:56,736 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 15:39:56,737 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3739, 'grad_norm': 5.18902587890625, 'learning_rate': 1.5790914066776137e-05, 'epoch': 2.05}\n"," 68% 2500/3654 [37:04<16:35,  1.16it/s][INFO|trainer.py:3944] 2025-03-22 15:47:21,098 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-22 15:47:21,100 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 15:47:21,101 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 15:47:26,014 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 15:47:26,017 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 15:47:26,018 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 15:47:26,018 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.025, 'grad_norm': 6.271111488342285, 'learning_rate': 8.949096880131364e-06, 'epoch': 2.46}\n"," 82% 3000/3654 [44:34<09:10,  1.19it/s][INFO|trainer.py:3944] 2025-03-22 15:54:51,017 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-22 15:54:51,019 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 15:54:51,020 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 15:54:55,934 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 15:54:55,937 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 15:54:55,938 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 15:54:55,938 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9933, 'grad_norm': 6.618305683135986, 'learning_rate': 2.1072796934865904e-06, 'epoch': 2.87}\n"," 96% 3500/3654 [52:05<02:09,  1.19it/s][INFO|trainer.py:3944] 2025-03-22 16:02:22,253 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3500\n","[INFO|configuration_utils.py:423] 2025-03-22 16:02:22,255 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 16:02:22,256 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 16:02:27,145 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 16:02:27,149 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 16:02:27,150 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 16:02:27,150 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 3654/3654 [54:35<00:00,  1.40it/s][INFO|trainer.py:3944] 2025-03-22 16:04:51,818 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3654\n","[INFO|configuration_utils.py:423] 2025-03-22 16:04:51,819 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3654/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 16:04:51,820 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3654/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 16:04:56,840 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3654/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 16:04:56,843 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3654/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 16:04:56,843 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3654/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 16:04:56,844 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/checkpoint-3654/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-22 16:05:10,707 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 3295.5424, 'train_samples_per_second': 8.864, 'train_steps_per_second': 1.109, 'train_loss': 2.6922839435646293, 'epoch': 3.0}\n","100% 3654/3654 [54:54<00:00,  1.11it/s]\n","[INFO|trainer.py:3944] 2025-03-22 16:05:10,716 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-4000\n","[INFO|configuration_utils.py:423] 2025-03-22 16:05:10,718 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-22 16:05:10,719 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-4000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-22 16:05:19,653 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-22 16:05:19,657 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-22 16:05:19,658 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-22 16:05:19,658 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-4000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  3718749GF\n","  train_loss               =     2.6923\n","  train_runtime            = 0:54:55.54\n","  train_samples            =       9737\n","  train_samples_per_second =      8.864\n","  train_steps_per_second   =      1.109\n","03/22/2025 16:05:19 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-22 16:05:19,820 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-22 16:05:19,821 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-22 16:05:19,821 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:19<00:00,  4.47s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     8.3068\n","  eval_chrf               =    43.7736\n","  eval_gen_len            =    46.2227\n","  eval_loss               =      2.353\n","  eval_runtime            = 0:09:25.60\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.763\n","  eval_steps_per_second   =      0.221\n","03/22/2025 16:14:45 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-22 16:14:45,430 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-22 16:14:45,430 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-22 16:14:45,430 >>   Batch size = 8\n","100% 127/127 [09:50<00:00,  4.65s/it]\n","***** predict metrics *****\n","  predict_bleu               =     7.9085\n","  predict_chrf               =    43.2905\n","  predict_gen_len            =     48.083\n","  predict_loss               =     2.3917\n","  predict_runtime            = 0:09:58.16\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.692\n","  predict_steps_per_second   =      0.212\n","[INFO|modelcard.py:449] 2025-03-22 16:24:58,942 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 8.3068}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/trainer_4000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-plus-4000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1743354831295,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"vccJW8nKsBGP","outputId":"ecc78951-abff-4d18-b2b1-841f1714933b"},"outputs":[{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/3jgivrj7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7d1bd3cf4580>"]},"metadata":{},"execution_count":8}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=f\"random_{language}-4000\")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXp3zJrE-cga","executionInfo":{"status":"ok","timestamp":1743354833239,"user_tz":-120,"elapsed":1947,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"ab6f2942-6a37-496e-f4f0-45e969b25107"},"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/content/AIMS-NLP-Project/run_translation.py\", line 28, in <module>\n","    import evaluate\n","  File \"/usr/local/lib/python3.10/dist-packages/evaluate/__init__.py\", line 29, in <module>\n","    from .evaluation_suite import EvaluationSuite\n","  File \"/usr/local/lib/python3.10/dist-packages/evaluate/evaluation_suite/__init__.py\", line 10, in <module>\n","    from ..evaluator import evaluator\n","  File \"/usr/local/lib/python3.10/dist-packages/evaluate/evaluator/__init__.py\", line 17, in <module>\n","    from transformers.pipelines import SUPPORTED_TASKS as SUPPORTED_PIPELINE_TASKS\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/__init__.py\", line 26, in <module>\n","    from . import dependency_versions_check\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n","    from .utils.versions import require_version, require_version_core\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/__init__.py\", line 27, in <module>\n","    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n","    from torch import Tensor\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2016, in <module>\n","    from torch import _VF as _VF, functional as functional  # usort: skip\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/functional.py\", line 7, in <module>\n","    import torch.nn.functional as F\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/__init__.py\", line 8, in <module>\n","    from torch.nn.modules import *  # usort: skip # noqa: F403\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/__init__.py\", line 1, in <module>\n","    from .module import Module  # usort: skip\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 29, in <module>\n","    from torch.utils._python_dispatch import is_traceable_wrapper_subclass\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_python_dispatch.py\", line 12, in <module>\n","    import torchgen.model\n","  File \"/usr/local/lib/python3.10/dist-packages/torchgen/model.py\", line 1998, in <module>\n","    class Argument:\n","  File \"/usr/lib/python3.10/dataclasses.py\", line 1175, in wrap\n","    return _process_class(cls, init, repr, eq, order, unsafe_hash,\n","  File \"/usr/lib/python3.10/dataclasses.py\", line 1088, in _process_class\n","    cls.__hash__ = hash_action(cls, field_list, globals)\n","  File \"/usr/lib/python3.10/dataclasses.py\", line 844, in _hash_add\n","    return _set_qualname(cls, _hash_fn(flds, globals))\n","  File \"/usr/lib/python3.10/dataclasses.py\", line 639, in _hash_fn\n","    return _create_fn('__hash__',\n","  File \"/usr/lib/python3.10/dataclasses.py\", line 424, in _create_fn\n","    body = '\\n'.join(f'  {b}' for b in body)\n","KeyboardInterrupt\n","^C\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/random/en-zu/train_4000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/random-4000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"executionInfo":{"elapsed":11272,"status":"ok","timestamp":1742831225311,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"hKmXZ3xLuoYh","outputId":"db4f2486-1a48-4aad-cd30-deeadb09cbf4"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.19.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20250324_154659-mgk6bytk</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/paderborn-university/machine%20translation/runs/mgk6bytk' target=\"_blank\">africomet-8000</a></strong> to <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/paderborn-university/machine%20translation/runs/mgk6bytk' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation/runs/mgk6bytk</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/mgk6bytk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7b0e028ec280>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=f\"africomet_{language}-8000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3599131,"status":"ok","timestamp":1742834854630,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"TZSOQFxvttWv","outputId":"b8fd0cd6-07a1-461d-f6cb-7997b3d1f226"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-24 15:47:42.556798: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-24 15:47:42.579116: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-24 15:47:42.586255: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-24 15:47:42.602197: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-24 15:47:43.613935: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/24/2025 15:47:48 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/24/2025 15:47:48 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-8000/runs/Mar24_15-47-48_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-8000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-8000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-5bc63a9eaf840ac0\n","03/24/2025 15:47:49 - INFO - datasets.builder - Using custom data configuration default-5bc63a9eaf840ac0\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/24/2025 15:47:49 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/24/2025 15:47:49 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/24/2025 15:47:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/24/2025 15:47:49 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/24/2025 15:47:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-24 15:47:49,388 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-24 15:47:49,390 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-24 15:47:49,529 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-24 15:47:49,530 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 15:47:49,532 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 15:47:49,532 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 15:47:49,532 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 15:47:49,532 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 15:47:49,532 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 15:47:49,532 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 15:47:49,532 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-24 15:47:49,532 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-24 15:47:49,533 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-24 15:47:50,525 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|safetensors_conversion.py:61] 2025-03-24 15:47:50,650 >> Attempting to create safetensors variant\n","[INFO|safetensors_conversion.py:74] 2025-03-24 15:47:50,962 >> Safetensors PR exists\n","[INFO|configuration_utils.py:1140] 2025-03-24 15:47:51,270 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-24 15:47:51,469 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-24 15:47:51,470 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-24 15:47:51,560 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-24 15:47:51,560 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-805d80dbaf682edc.arrow\n","03/24/2025 15:47:52 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-805d80dbaf682edc.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-8d35743ea3961495.arrow\n","03/24/2025 15:47:54 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-8d35743ea3961495.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a687ca1a7df8988f.arrow\n","03/24/2025 15:47:55 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5bc63a9eaf840ac0/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a687ca1a7df8988f.arrow\n","[INFO|trainer.py:2407] 2025-03-24 15:48:05,776 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-24 15:48:05,776 >>   Num examples = 8,000\n","[INFO|trainer.py:2409] 2025-03-24 15:48:05,776 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-24 15:48:05,776 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-24 15:48:05,776 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-24 15:48:05,776 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-24 15:48:05,776 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-24 15:48:05,776 >>   Total optimization steps = 3,000\n","[INFO|trainer.py:2416] 2025-03-24 15:48:05,778 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-24 15:48:05,793 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250324_154805-zjr5oxoa\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-8000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/zjr5oxoa\u001b[0m\n","  0% 0/3000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.3621, 'grad_norm': 8.457220077514648, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n"," 17% 500/3000 [06:06<30:17,  1.38it/s][INFO|trainer.py:3944] 2025-03-24 15:54:13,498 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-24 15:54:13,505 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 15:54:13,506 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 15:54:25,004 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 15:54:25,008 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 15:54:25,009 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 15:54:25,009 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.8407, 'grad_norm': 9.071094512939453, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"," 33% 1000/3000 [12:43<24:41,  1.35it/s][INFO|trainer.py:3944] 2025-03-24 16:00:50,004 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-24 16:00:50,009 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 16:00:50,010 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 16:01:01,536 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 16:01:01,541 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 16:01:01,541 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 16:01:01,542 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.0429, 'grad_norm': 8.176770210266113, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n"," 50% 1500/3000 [19:17<18:18,  1.37it/s][INFO|trainer.py:3944] 2025-03-24 16:07:24,307 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-24 16:07:24,310 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 16:07:24,311 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 16:07:35,807 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 16:07:35,812 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 16:07:35,812 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 16:07:35,813 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.9449, 'grad_norm': 9.169672012329102, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 2000/3000 [25:53<11:47,  1.41it/s][INFO|trainer.py:3944] 2025-03-24 16:13:59,914 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-24 16:13:59,923 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 16:13:59,924 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 16:14:11,461 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 16:14:11,465 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 16:14:11,466 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 16:14:11,466 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4138, 'grad_norm': 8.797719955444336, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"," 83% 2500/3000 [32:26<06:00,  1.39it/s][INFO|trainer.py:3944] 2025-03-24 16:20:33,110 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-24 16:20:33,115 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 16:20:33,116 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 16:20:44,611 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 16:20:44,614 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 16:20:44,615 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 16:20:44,615 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3627, 'grad_norm': 6.994338512420654, 'learning_rate': 0.0, 'epoch': 3.0}\n","100% 3000/3000 [39:03<00:00,  1.35it/s][INFO|trainer.py:3944] 2025-03-24 16:27:10,038 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-24 16:27:10,041 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 16:27:10,042 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 16:27:21,574 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 16:27:21,579 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 16:27:21,580 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 16:27:21,580 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/checkpoint-3000/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-24 16:27:39,754 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2373.9765, 'train_samples_per_second': 10.11, 'train_steps_per_second': 1.264, 'train_loss': 3.1611649983723957, 'epoch': 3.0}\n","100% 3000/3000 [39:33<00:00,  1.26it/s]\n","[INFO|trainer.py:3944] 2025-03-24 16:27:39,762 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-8000\n","[INFO|configuration_utils.py:423] 2025-03-24 16:27:39,849 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 16:27:39,850 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-8000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 16:27:51,303 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-8000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 16:27:51,306 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 16:27:51,306 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 16:27:51,307 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-8000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  1672691GF\n","  train_loss               =     3.1612\n","  train_runtime            = 0:39:33.97\n","  train_samples            =       8000\n","  train_samples_per_second =      10.11\n","  train_steps_per_second   =      1.264\n","03/24/2025 16:27:51 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-24 16:27:51,450 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-24 16:27:51,450 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-24 16:27:51,450 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:13<00:00,  4.43s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     5.3249\n","  eval_chrf               =     34.754\n","  eval_gen_len            =    46.2387\n","  eval_loss               =     2.9513\n","  eval_runtime            = 0:09:19.31\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.783\n","  eval_steps_per_second   =      0.223\n","03/24/2025 16:37:10 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-24 16:37:10,774 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-24 16:37:10,774 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-24 16:37:10,775 >>   Batch size = 8\n","100% 127/127 [09:58<00:00,  4.71s/it]\n","***** predict metrics *****\n","  predict_bleu               =     4.8876\n","  predict_chrf               =    34.3485\n","  predict_gen_len            =    48.1611\n","  predict_loss               =     3.0137\n","  predict_runtime            = 0:10:04.74\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.673\n","  predict_steps_per_second   =       0.21\n","[INFO|modelcard.py:449] 2025-03-24 16:47:31,327 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 5.3249}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_8000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-8000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":40},"executionInfo":{"elapsed":361,"status":"ok","timestamp":1742834858561,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"sCFvQq0b1sWB","outputId":"dd633172-28b0-4e5c-f06c-1a4d77405744"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/mgk6bytk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7b0e028ec280>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init(project=\"machine translation\", name=f\"africomet_{language}-plus-8000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5607920,"status":"ok","timestamp":1742840474639,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"Anml0GjD1sU5","outputId":"49d4f8d0-c902-46dc-d9eb-ead4f8c7847d"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-24 16:47:52.748131: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-24 16:47:52.769848: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-24 16:47:52.776215: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-24 16:47:52.792682: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-24 16:47:53.846867: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/24/2025 16:47:58 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/24/2025 16:47:58 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-plus-8000/runs/Mar24_16-47-58_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-plus-8000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-plus-8000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-3a356ec34b2797b9\n","03/24/2025 16:47:58 - INFO - datasets.builder - Using custom data configuration default-3a356ec34b2797b9\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/24/2025 16:47:58 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Generating dataset json (/root/.cache/huggingface/datasets/json/default-3a356ec34b2797b9/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/24/2025 16:47:58 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-3a356ec34b2797b9/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-3a356ec34b2797b9/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","03/24/2025 16:47:58 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-3a356ec34b2797b9/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","Downloading took 0.0 min\n","03/24/2025 16:47:58 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","03/24/2025 16:47:58 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Generating train split\n","03/24/2025 16:47:58 - INFO - datasets.builder - Generating train split\n","Generating train split: 13737 examples [00:00, 358315.63 examples/s]\n","Generating validation split\n","03/24/2025 16:47:58 - INFO - datasets.builder - Generating validation split\n","Generating validation split: 997 examples [00:00, 219121.83 examples/s]\n","Generating test split\n","03/24/2025 16:47:58 - INFO - datasets.builder - Generating test split\n","Generating test split: 1012 examples [00:00, 215059.82 examples/s]\n","Unable to verify splits sizes.\n","03/24/2025 16:47:58 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-3a356ec34b2797b9/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","03/24/2025 16:47:58 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-3a356ec34b2797b9/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","[INFO|configuration_utils.py:699] 2025-03-24 16:47:58,931 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-24 16:47:58,934 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-24 16:47:59,020 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-24 16:47:59,021 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 16:47:59,021 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 16:47:59,021 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 16:47:59,021 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 16:47:59,021 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 16:47:59,021 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 16:47:59,021 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 16:47:59,021 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-24 16:47:59,022 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-24 16:47:59,022 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-24 16:47:59,950 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-24 16:48:00,046 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-24 16:48:00,069 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-24 16:48:00,151 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-24 16:48:00,152 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-24 16:48:00,246 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-24 16:48:00,246 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-24 16:48:00,443 >> Safetensors PR exists\n","Running tokenizer on train dataset:   0% 0/13737 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3a356ec34b2797b9/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-53bb8f4edfeb1a46.arrow\n","03/24/2025 16:48:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3a356ec34b2797b9/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-53bb8f4edfeb1a46.arrow\n","Running tokenizer on train dataset: 100% 13737/13737 [00:05<00:00, 2386.03 examples/s]\n","Running tokenizer on validation dataset:   0% 0/997 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3a356ec34b2797b9/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9af310ba714072a1.arrow\n","03/24/2025 16:48:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3a356ec34b2797b9/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-9af310ba714072a1.arrow\n","Running tokenizer on validation dataset: 100% 997/997 [00:00<00:00, 1914.40 examples/s]\n","Running tokenizer on prediction dataset:   0% 0/1012 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3a356ec34b2797b9/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-710fb297dc06ba2d.arrow\n","03/24/2025 16:48:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-3a356ec34b2797b9/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-710fb297dc06ba2d.arrow\n","Running tokenizer on prediction dataset: 100% 1012/1012 [00:00<00:00, 1853.13 examples/s]\n","[INFO|trainer.py:2407] 2025-03-24 16:48:21,207 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-24 16:48:21,208 >>   Num examples = 13,737\n","[INFO|trainer.py:2409] 2025-03-24 16:48:21,208 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-24 16:48:21,208 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-24 16:48:21,208 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-24 16:48:21,208 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-24 16:48:21,208 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-24 16:48:21,208 >>   Total optimization steps = 5,154\n","[INFO|trainer.py:2416] 2025-03-24 16:48:21,209 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-24 16:48:21,214 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250324_164821-lisvz4vg\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-plus-8000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/lisvz4vg\u001b[0m\n","  0% 0/5154 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.0866, 'grad_norm': 7.590505599975586, 'learning_rate': 4.514939852541715e-05, 'epoch': 0.29}\n"," 10% 500/5154 [06:53<1:00:37,  1.28it/s][INFO|trainer.py:3944] 2025-03-24 16:55:15,685 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-24 16:55:15,695 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 16:55:15,695 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 16:55:20,562 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 16:55:20,565 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 16:55:20,566 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 16:55:20,566 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.4869, 'grad_norm': 6.757532596588135, 'learning_rate': 4.02987970508343e-05, 'epoch': 0.58}\n"," 19% 1000/5154 [13:58<53:53,  1.28it/s][INFO|trainer.py:3944] 2025-03-24 17:02:20,913 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-24 17:02:20,915 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 17:02:20,916 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 17:02:25,789 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 17:02:25,791 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 17:02:25,792 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 17:02:25,792 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.2102, 'grad_norm': 5.204808235168457, 'learning_rate': 3.544819557625146e-05, 'epoch': 0.87}\n"," 29% 1500/5154 [21:08<49:33,  1.23it/s][INFO|trainer.py:3944] 2025-03-24 17:09:30,460 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-24 17:09:30,463 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 17:09:30,463 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 17:09:35,336 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 17:09:35,338 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 17:09:35,339 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 17:09:35,339 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.7638, 'grad_norm': 6.48525333404541, 'learning_rate': 3.059759410166861e-05, 'epoch': 1.16}\n"," 39% 2000/5154 [28:18<43:22,  1.21it/s][INFO|trainer.py:3944] 2025-03-24 17:16:40,788 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-24 17:16:40,790 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 17:16:40,791 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 17:16:45,659 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 17:16:45,661 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 17:16:45,662 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 17:16:45,662 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.5135, 'grad_norm': 5.433412075042725, 'learning_rate': 2.574699262708576e-05, 'epoch': 1.46}\n"," 49% 2500/5154 [35:27<36:50,  1.20it/s][INFO|trainer.py:3944] 2025-03-24 17:23:49,727 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-24 17:23:49,729 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 17:23:49,730 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 17:23:54,615 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 17:23:54,620 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 17:23:54,620 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 17:23:54,621 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4595, 'grad_norm': 6.525064945220947, 'learning_rate': 2.0896391152502913e-05, 'epoch': 1.75}\n"," 58% 3000/5154 [42:37<29:41,  1.21it/s][INFO|trainer.py:3944] 2025-03-24 17:30:59,238 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-24 17:30:59,240 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 17:30:59,241 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 17:31:04,157 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 17:31:04,160 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 17:31:04,161 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 17:31:04,161 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-3000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3655, 'grad_norm': 5.001039505004883, 'learning_rate': 1.6045789677920063e-05, 'epoch': 2.04}\n"," 68% 3500/5154 [49:46<23:17,  1.18it/s][INFO|trainer.py:3944] 2025-03-24 17:38:08,262 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-3500\n","[INFO|configuration_utils.py:423] 2025-03-24 17:38:08,264 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-3500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 17:38:08,265 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-3500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 17:38:13,157 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-3500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 17:38:13,160 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 17:38:13,161 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-3500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 17:38:13,161 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-3500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.0179, 'grad_norm': 6.524746894836426, 'learning_rate': 1.1195188203337214e-05, 'epoch': 2.33}\n"," 78% 4000/5154 [56:55<15:40,  1.23it/s][INFO|trainer.py:3944] 2025-03-24 17:45:17,679 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-4000\n","[INFO|configuration_utils.py:423] 2025-03-24 17:45:17,681 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-4000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 17:45:17,681 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-4000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 17:45:22,611 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 17:45:22,614 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 17:45:22,615 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-4000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 17:45:22,615 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-4000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9669, 'grad_norm': 5.724503993988037, 'learning_rate': 6.344586728754366e-06, 'epoch': 2.62}\n"," 87% 4500/5154 [1:04:03<08:31,  1.28it/s][INFO|trainer.py:3944] 2025-03-24 17:52:25,973 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-4500\n","[INFO|configuration_utils.py:423] 2025-03-24 17:52:25,975 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-4500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 17:52:25,976 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-4500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 17:52:30,886 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-4500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 17:52:30,889 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 17:52:30,890 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-4500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 17:52:30,890 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-4500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9482, 'grad_norm': 7.143683433532715, 'learning_rate': 1.4939852541715173e-06, 'epoch': 2.91}\n"," 97% 5000/5154 [1:11:13<02:09,  1.19it/s][INFO|trainer.py:3944] 2025-03-24 17:59:35,869 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-5000\n","[INFO|configuration_utils.py:423] 2025-03-24 17:59:35,871 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-5000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 17:59:35,872 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-5000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 17:59:40,710 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-5000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 17:59:40,713 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 17:59:40,714 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-5000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 17:59:40,714 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-5000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 5154/5154 [1:13:37<00:00,  1.54it/s][INFO|trainer.py:3944] 2025-03-24 18:01:59,708 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-5154\n","[INFO|configuration_utils.py:423] 2025-03-24 18:01:59,710 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-5154/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 18:01:59,711 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-5154/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 18:02:04,739 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-5154/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 18:02:04,742 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-5154/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 18:02:04,742 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-5154/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 18:02:04,743 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/checkpoint-5154/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-24 18:02:18,642 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 4437.4334, 'train_samples_per_second': 9.287, 'train_steps_per_second': 1.161, 'train_loss': 2.66027265377733, 'epoch': 3.0}\n","100% 5154/5154 [1:13:56<00:00,  1.16it/s]\n","[INFO|trainer.py:3944] 2025-03-24 18:02:18,653 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-8000\n","[INFO|configuration_utils.py:423] 2025-03-24 18:02:18,655 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 18:02:18,655 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-8000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 18:02:27,607 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-8000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 18:02:27,612 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 18:02:27,613 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 18:02:27,613 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-8000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  4774172GF\n","  train_loss               =     2.6603\n","  train_runtime            = 1:13:57.43\n","  train_samples            =      13737\n","  train_samples_per_second =      9.287\n","  train_steps_per_second   =      1.161\n","03/24/2025 18:02:27 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-24 18:02:27,758 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-24 18:02:27,758 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-24 18:02:27,758 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [08:39<00:00,  4.15s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =      8.703\n","  eval_chrf               =    44.9152\n","  eval_gen_len            =    46.2808\n","  eval_loss               =     2.2373\n","  eval_runtime            = 0:08:45.56\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.897\n","  eval_steps_per_second   =      0.238\n","03/24/2025 18:11:13 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-24 18:11:13,334 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-24 18:11:13,334 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-24 18:11:13,334 >>   Batch size = 8\n","100% 127/127 [09:34<00:00,  4.53s/it]\n","***** predict metrics *****\n","  predict_bleu               =      8.262\n","  predict_chrf               =    44.2729\n","  predict_gen_len            =     48.163\n","  predict_loss               =     2.2844\n","  predict_runtime            = 0:09:42.34\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.738\n","  predict_steps_per_second   =      0.218\n","[INFO|modelcard.py:449] 2025-03-24 18:21:11,197 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 8.703}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/trainer_8000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-plus-8000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"E1xlYKIs-cgb","executionInfo":{"status":"ok","timestamp":1743354833239,"user_tz":-120,"elapsed":4,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"a367450e-076a-4d6f-8786-e8d5b3287947"},"outputs":[{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/3jgivrj7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7d1bd3cf4580>"]},"metadata":{},"execution_count":10}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=f\"random_{language}-8000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7uSLk6YU-cgc","outputId":"c96bceb3-b9b2-49c7-aad1-3d9d5d0c15d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-30 17:13:56.823163: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-30 17:13:56.846203: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-30 17:13:56.853308: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-30 17:13:56.869987: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-30 17:13:57.921411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/30/2025 17:14:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/30/2025 17:14:00 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/random-8000/runs/Mar30_17-14-00_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/random-8000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/random-8000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-b95efb0a199eb91d\n","03/30/2025 17:14:00 - INFO - datasets.builder - Using custom data configuration default-b95efb0a199eb91d\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/30/2025 17:14:00 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Generating dataset json (/root/.cache/huggingface/datasets/json/default-b95efb0a199eb91d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/30/2025 17:14:00 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-b95efb0a199eb91d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-b95efb0a199eb91d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","03/30/2025 17:14:00 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-b95efb0a199eb91d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","Downloading took 0.0 min\n","03/30/2025 17:14:00 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","03/30/2025 17:14:00 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Generating train split\n","03/30/2025 17:14:00 - INFO - datasets.builder - Generating train split\n","Generating train split: 16000 examples [00:00, 496602.42 examples/s]\n","Generating validation split\n","03/30/2025 17:14:00 - INFO - datasets.builder - Generating validation split\n","Generating validation split: 997 examples [00:00, 93302.42 examples/s]\n","Generating test split\n","03/30/2025 17:14:00 - INFO - datasets.builder - Generating test split\n","Generating test split: 1012 examples [00:00, 224145.09 examples/s]\n","Unable to verify splits sizes.\n","03/30/2025 17:14:00 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b95efb0a199eb91d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","03/30/2025 17:14:00 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b95efb0a199eb91d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","[INFO|configuration_utils.py:699] 2025-03-30 17:14:00,986 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-30 17:14:00,989 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-30 17:14:01,070 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-30 17:14:01,071 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-30 17:14:01,072 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-30 17:14:01,072 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-30 17:14:01,072 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-30 17:14:01,072 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-30 17:14:01,072 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-30 17:14:01,072 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-30 17:14:01,072 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-30 17:14:01,072 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-30 17:14:01,073 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-30 17:14:02,006 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-30 17:14:02,078 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-30 17:14:02,120 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-30 17:14:02,185 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-30 17:14:02,185 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-30 17:14:02,280 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-30 17:14:02,280 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-30 17:14:02,471 >> Safetensors PR exists\n","Running tokenizer on train dataset:   0% 0/16000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b95efb0a199eb91d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c12b229da5b2f1dc.arrow\n","03/30/2025 17:14:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b95efb0a199eb91d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c12b229da5b2f1dc.arrow\n","Running tokenizer on train dataset: 100% 16000/16000 [00:04<00:00, 3463.32 examples/s]\n","Running tokenizer on validation dataset:   0% 0/997 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b95efb0a199eb91d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0fe6e05c084178de.arrow\n","03/30/2025 17:14:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b95efb0a199eb91d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0fe6e05c084178de.arrow\n","Running tokenizer on validation dataset: 100% 997/997 [00:00<00:00, 1834.51 examples/s]\n","Running tokenizer on prediction dataset:   0% 0/1012 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b95efb0a199eb91d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0168855141fdbc90.arrow\n","03/30/2025 17:14:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b95efb0a199eb91d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0168855141fdbc90.arrow\n","Running tokenizer on prediction dataset: 100% 1012/1012 [00:00<00:00, 1712.42 examples/s]\n","[INFO|trainer.py:2407] 2025-03-30 17:14:22,473 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-30 17:14:22,473 >>   Num examples = 16,000\n","[INFO|trainer.py:2409] 2025-03-30 17:14:22,473 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-30 17:14:22,473 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-30 17:14:22,473 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-30 17:14:22,473 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-30 17:14:22,474 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-30 17:14:22,474 >>   Total optimization steps = 6,000\n","[INFO|trainer.py:2416] 2025-03-30 17:14:22,475 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-30 17:14:22,480 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250330_171422-1hjb7e1d\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/random-8000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/1hjb7e1d\u001b[0m\n","  0% 0/6000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.6638, 'grad_norm': 6.638711452484131, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}\n","  8% 500/6000 [06:03<1:06:52,  1.37it/s][INFO|trainer.py:3944] 2025-03-30 17:20:26,747 >> Saving model checkpoint to M2M-100/random-8000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-30 17:20:26,757 >> Configuration saved in M2M-100/random-8000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-30 17:20:26,758 >> Configuration saved in M2M-100/random-8000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-30 17:20:31,715 >> Model weights saved in M2M-100/random-8000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-30 17:20:31,718 >> tokenizer config file saved in M2M-100/random-8000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-30 17:20:31,719 >> Special tokens file saved in M2M-100/random-8000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-30 17:20:31,719 >> added tokens file saved in M2M-100/random-8000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n"," 14% 852/6000 [10:37<1:02:16,  1.38it/s]"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/random/en-zu/train_8000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/random-8000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":40},"executionInfo":{"elapsed":384,"status":"ok","timestamp":1742841373006,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"50vt5hMgup68","outputId":"c15b6840-cc17-4795-9c5e-cd665ff7e771"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/mgk6bytk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7b0e028ec280>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=f\"africomet_{language}-16000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6040345,"status":"ok","timestamp":1742854293687,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"mlfAMTxMtvOi","outputId":"c4b6c114-bc82-417b-b43b-414cff0bfb11"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-24 20:30:59.240818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-24 20:30:59.262569: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-24 20:30:59.269096: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-24 20:30:59.285716: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-24 20:31:00.377607: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/24/2025 20:31:05 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/24/2025 20:31:05 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-16000/runs/Mar24_20-31-04_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-16000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-16000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-8dd97eb64194ed63\n","03/24/2025 20:31:05 - INFO - datasets.builder - Using custom data configuration default-8dd97eb64194ed63\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/24/2025 20:31:05 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/24/2025 20:31:05 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/24/2025 20:31:05 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/24/2025 20:31:05 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/24/2025 20:31:05 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-24 20:31:05,437 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-24 20:31:05,440 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-24 20:31:05,536 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-24 20:31:05,537 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 20:31:05,537 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 20:31:05,537 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 20:31:05,537 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 20:31:05,537 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 20:31:05,537 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 20:31:05,538 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-24 20:31:05,538 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-24 20:31:05,538 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-24 20:31:05,539 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-24 20:31:06,479 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-24 20:31:06,543 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-24 20:31:06,599 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-24 20:31:06,653 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-24 20:31:06,653 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-24 20:31:06,753 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-24 20:31:06,753 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4f73fd23cbbc2044.arrow\n","03/24/2025 20:31:08 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4f73fd23cbbc2044.arrow\n","[INFO|safetensors_conversion.py:74] 2025-03-24 20:31:08,156 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-61f573cc25eca874.arrow\n","03/24/2025 20:31:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-61f573cc25eca874.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-29c03a83c2242779.arrow\n","03/24/2025 20:31:10 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8dd97eb64194ed63/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-29c03a83c2242779.arrow\n","[INFO|trainer.py:2407] 2025-03-24 20:31:20,529 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-24 20:31:20,529 >>   Num examples = 16,000\n","[INFO|trainer.py:2409] 2025-03-24 20:31:20,529 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-24 20:31:20,529 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-24 20:31:20,529 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-24 20:31:20,529 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-24 20:31:20,529 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-24 20:31:20,529 >>   Total optimization steps = 6,000\n","[INFO|trainer.py:2416] 2025-03-24 20:31:20,530 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-24 20:31:20,535 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250324_203120-9plrl2t2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-16000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/9plrl2t2\u001b[0m\n","  0% 0/6000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.4095, 'grad_norm': 7.838458061218262, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}\n","  8% 500/6000 [06:06<1:05:05,  1.41it/s][INFO|trainer.py:3944] 2025-03-24 20:37:27,987 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-24 20:37:27,993 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 20:37:27,994 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 20:37:39,577 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 20:37:39,581 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 20:37:39,581 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 20:37:39,582 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.8933, 'grad_norm': 7.528745174407959, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n"," 17% 1000/6000 [12:44<1:03:40,  1.31it/s][INFO|trainer.py:3944] 2025-03-24 20:44:05,802 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-24 20:44:05,804 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 20:44:05,805 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 20:44:17,372 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 20:44:17,377 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 20:44:17,377 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 20:44:17,378 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.595, 'grad_norm': 7.852253437042236, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}\n"," 25% 1500/6000 [19:19<55:10,  1.36it/s][INFO|trainer.py:3944] 2025-03-24 20:50:40,695 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-24 20:50:40,697 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 20:50:40,698 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 20:50:52,293 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 20:50:52,296 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 20:50:52,297 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 20:50:52,297 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.4125, 'grad_norm': 7.076495170593262, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"," 33% 2000/6000 [25:56<49:34,  1.34it/s][INFO|trainer.py:3944] 2025-03-24 20:57:18,139 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-24 20:57:18,141 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 20:57:18,142 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 20:57:30,080 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 20:57:30,085 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 20:57:30,085 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 20:57:30,086 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.8227, 'grad_norm': 7.7632927894592285, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}\n"," 42% 2500/6000 [32:32<42:15,  1.38it/s][INFO|trainer.py:3944] 2025-03-24 21:03:54,185 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-24 21:03:54,187 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 21:03:54,187 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 21:04:05,824 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 21:04:05,828 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 21:04:05,829 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 21:04:05,829 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.7638, 'grad_norm': 5.975804805755615, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n"," 50% 3000/6000 [39:09<36:57,  1.35it/s][INFO|trainer.py:3944] 2025-03-24 21:10:30,609 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-24 21:10:30,611 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 21:10:30,612 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 21:10:42,195 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 21:10:42,200 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 21:10:42,200 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 21:10:42,201 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.7293, 'grad_norm': 8.255613327026367, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}\n"," 58% 3500/6000 [45:46<30:21,  1.37it/s][INFO|trainer.py:3944] 2025-03-24 21:17:08,004 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500\n","[INFO|configuration_utils.py:423] 2025-03-24 21:17:08,006 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 21:17:08,006 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 21:17:19,577 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 21:17:19,581 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 21:17:19,582 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 21:17:19,582 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-3500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.6633, 'grad_norm': 6.3068084716796875, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 4000/6000 [52:21<25:20,  1.32it/s][INFO|trainer.py:3944] 2025-03-24 21:23:42,892 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000\n","[INFO|configuration_utils.py:423] 2025-03-24 21:23:42,893 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 21:23:42,894 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 21:23:54,487 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 21:23:54,491 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 21:23:54,492 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 21:23:54,492 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.2192, 'grad_norm': 6.925149440765381, 'learning_rate': 1.25e-05, 'epoch': 2.25}\n"," 75% 4500/6000 [58:58<18:14,  1.37it/s][INFO|trainer.py:3944] 2025-03-24 21:30:19,677 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500\n","[INFO|configuration_utils.py:423] 2025-03-24 21:30:19,679 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 21:30:19,680 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 21:30:31,315 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 21:30:31,320 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 21:30:31,320 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 21:30:31,321 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-4500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.1985, 'grad_norm': 7.397052764892578, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"," 83% 5000/6000 [1:05:35<12:11,  1.37it/s][INFO|trainer.py:3944] 2025-03-24 21:36:56,410 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000\n","[INFO|configuration_utils.py:423] 2025-03-24 21:36:56,412 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 21:36:56,413 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 21:37:07,989 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 21:37:07,993 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 21:37:07,994 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 21:37:07,994 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.1924, 'grad_norm': 6.54964542388916, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}\n"," 92% 5500/6000 [1:12:10<05:54,  1.41it/s][INFO|trainer.py:3944] 2025-03-24 21:43:32,040 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500\n","[INFO|configuration_utils.py:423] 2025-03-24 21:43:32,042 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 21:43:32,043 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 21:43:43,613 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 21:43:43,617 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 21:43:43,617 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 21:43:43,618 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-5500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.133, 'grad_norm': 7.093844413757324, 'learning_rate': 0.0, 'epoch': 3.0}\n","100% 6000/6000 [1:18:46<00:00,  1.37it/s][INFO|trainer.py:3944] 2025-03-24 21:50:08,325 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000\n","[INFO|configuration_utils.py:423] 2025-03-24 21:50:08,327 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 21:50:08,328 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 21:50:19,860 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 21:50:19,864 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 21:50:19,865 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 21:50:19,865 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/checkpoint-6000/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-24 21:50:37,813 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 4757.283, 'train_samples_per_second': 10.09, 'train_steps_per_second': 1.261, 'train_loss': 2.9193775431315103, 'epoch': 3.0}\n","100% 6000/6000 [1:19:16<00:00,  1.26it/s]\n","[INFO|trainer.py:3944] 2025-03-24 21:50:37,826 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-16000\n","[INFO|configuration_utils.py:423] 2025-03-24 21:50:37,874 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-24 21:50:37,875 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-16000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-24 21:50:49,389 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-16000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-24 21:50:49,392 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-24 21:50:49,393 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-24 21:50:49,393 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-16000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  3307792GF\n","  train_loss               =     2.9194\n","  train_runtime            = 1:19:17.28\n","  train_samples            =      16000\n","  train_samples_per_second =      10.09\n","  train_steps_per_second   =      1.261\n","03/24/2025 21:50:49 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-24 21:50:49,539 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-24 21:50:49,539 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-24 21:50:49,540 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [10:05<00:00,  4.84s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =      6.828\n","  eval_chrf               =    39.3323\n","  eval_gen_len            =    47.6409\n","  eval_loss               =     2.5585\n","  eval_runtime            = 0:10:12.76\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.627\n","  eval_steps_per_second   =      0.204\n","03/24/2025 22:01:02 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-24 22:01:02,309 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-24 22:01:02,309 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-24 22:01:02,309 >>   Batch size = 8\n","100% 127/127 [10:05<00:00,  4.77s/it]\n","***** predict metrics *****\n","  predict_bleu               =     6.6496\n","  predict_chrf               =    39.2591\n","  predict_gen_len            =    48.7925\n","  predict_loss               =     2.6011\n","  predict_runtime            = 0:10:11.78\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.654\n","  predict_steps_per_second   =      0.208\n","[INFO|modelcard.py:449] 2025-03-24 22:11:30,043 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 6.828}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_16000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-16000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"executionInfo":{"elapsed":11866,"status":"ok","timestamp":1742911736304,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"XBleg4H92hdW","outputId":"c303b375-be90-43c3-fe3c-b380203f7d14"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.19.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20250325_140850-izuqi7qg</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/paderborn-university/machine%20translation/runs/izuqi7qg' target=\"_blank\">africomet-plus-16000</a></strong> to <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/paderborn-university/machine%20translation/runs/izuqi7qg' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation/runs/izuqi7qg</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/izuqi7qg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7eaef1840460>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=f\"africomet_{language}-plus-16000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7831119,"status":"ok","timestamp":1742919567410,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"gKyoQfzk2hcC","outputId":"94054959-4b8b-4cc0-8724-96461f72876d"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-25 14:09:03.795306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-25 14:09:03.834865: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-25 14:09:03.847835: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-25 14:09:03.870450: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-25 14:09:05.011393: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/25/2025 14:09:10 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/25/2025 14:09:10 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-plus-16000/runs/Mar25_14-09-10_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-plus-16000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-plus-16000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-378fdd38804a47f4\n","03/25/2025 14:09:11 - INFO - datasets.builder - Using custom data configuration default-378fdd38804a47f4\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/25/2025 14:09:11 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Generating dataset json (/root/.cache/huggingface/datasets/json/default-378fdd38804a47f4/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/25/2025 14:09:11 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-378fdd38804a47f4/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-378fdd38804a47f4/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","03/25/2025 14:09:11 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-378fdd38804a47f4/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","Downloading took 0.0 min\n","03/25/2025 14:09:11 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","03/25/2025 14:09:11 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Generating train split\n","03/25/2025 14:09:11 - INFO - datasets.builder - Generating train split\n","Generating train split: 21737 examples [00:00, 312235.74 examples/s]\n","Generating validation split\n","03/25/2025 14:09:11 - INFO - datasets.builder - Generating validation split\n","Generating validation split: 997 examples [00:00, 216366.80 examples/s]\n","Generating test split\n","03/25/2025 14:09:11 - INFO - datasets.builder - Generating test split\n","Generating test split: 1012 examples [00:00, 227582.20 examples/s]\n","Unable to verify splits sizes.\n","03/25/2025 14:09:11 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-378fdd38804a47f4/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","03/25/2025 14:09:11 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-378fdd38804a47f4/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","[INFO|configuration_utils.py:699] 2025-03-25 14:09:11,566 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-25 14:09:11,569 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-25 14:09:11,662 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-25 14:09:11,663 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 14:09:11,665 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 14:09:11,665 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 14:09:11,665 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 14:09:11,665 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 14:09:11,665 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 14:09:11,665 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 14:09:11,665 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-25 14:09:11,666 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-25 14:09:11,667 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-25 14:09:12,644 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-25 14:09:13,335 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-25 14:09:13,508 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-25 14:09:13,508 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-25 14:09:13,603 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-25 14:09:13,603 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-25 14:09:13,839 >> Attempting to create safetensors variant\n","[INFO|safetensors_conversion.py:74] 2025-03-25 14:09:14,339 >> Safetensors PR exists\n","Running tokenizer on train dataset:   0% 0/21737 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-378fdd38804a47f4/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-369fa5a20b3bb9bf.arrow\n","03/25/2025 14:09:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-378fdd38804a47f4/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-369fa5a20b3bb9bf.arrow\n","Running tokenizer on train dataset: 100% 21737/21737 [00:08<00:00, 2537.58 examples/s]\n","Running tokenizer on validation dataset:   0% 0/997 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-378fdd38804a47f4/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-ae57c68f2106ee19.arrow\n","03/25/2025 14:09:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-378fdd38804a47f4/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-ae57c68f2106ee19.arrow\n","Running tokenizer on validation dataset: 100% 997/997 [00:00<00:00, 1714.77 examples/s]\n","Running tokenizer on prediction dataset:   0% 0/1012 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-378fdd38804a47f4/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c153cad90fccc4f5.arrow\n","03/25/2025 14:09:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-378fdd38804a47f4/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c153cad90fccc4f5.arrow\n","Running tokenizer on prediction dataset: 100% 1012/1012 [00:00<00:00, 1741.90 examples/s]\n","[INFO|trainer.py:2407] 2025-03-25 14:09:37,563 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-25 14:09:37,563 >>   Num examples = 21,737\n","[INFO|trainer.py:2409] 2025-03-25 14:09:37,563 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-25 14:09:37,563 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-25 14:09:37,563 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-25 14:09:37,563 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-25 14:09:37,563 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-25 14:09:37,563 >>   Total optimization steps = 8,154\n","[INFO|trainer.py:2416] 2025-03-25 14:09:37,564 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-25 14:09:37,570 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250325_140937-wssz3oc8\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-plus-16000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/wssz3oc8\u001b[0m\n","  0% 0/8154 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.1998, 'grad_norm': 8.517952919006348, 'learning_rate': 4.6934020112828065e-05, 'epoch': 0.18}\n","  6% 500/8154 [06:30<1:34:11,  1.35it/s][INFO|trainer.py:3944] 2025-03-25 14:16:08,491 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-25 14:16:08,497 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 14:16:08,498 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 14:16:13,665 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 14:16:13,668 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 14:16:13,669 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 14:16:13,669 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.6407, 'grad_norm': 10.210153579711914, 'learning_rate': 4.386804022565612e-05, 'epoch': 0.37}\n"," 12% 1000/8154 [13:20<1:33:30,  1.28it/s][INFO|trainer.py:3944] 2025-03-25 14:22:58,713 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-25 14:22:58,715 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 14:22:58,716 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 14:23:03,627 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 14:23:03,630 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 14:23:03,630 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 14:23:03,631 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.393, 'grad_norm': 6.730628490447998, 'learning_rate': 4.080206033848418e-05, 'epoch': 0.55}\n"," 18% 1500/8154 [20:09<1:19:15,  1.40it/s][INFO|trainer.py:3944] 2025-03-25 14:29:48,091 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-25 14:29:48,093 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 14:29:48,094 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 14:29:52,925 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 14:29:52,928 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 14:29:52,928 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 14:29:52,929 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.1976, 'grad_norm': 8.2925386428833, 'learning_rate': 3.773608045131224e-05, 'epoch': 0.74}\n"," 25% 2000/8154 [26:56<1:15:08,  1.36it/s][INFO|trainer.py:3944] 2025-03-25 14:36:34,573 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-25 14:36:34,575 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 14:36:34,575 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 14:36:39,578 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 14:36:39,584 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 14:36:39,586 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 14:36:39,586 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.0706, 'grad_norm': 7.903889179229736, 'learning_rate': 3.46701005641403e-05, 'epoch': 0.92}\n"," 31% 2500/8154 [33:43<1:10:12,  1.34it/s][INFO|trainer.py:3944] 2025-03-25 14:43:22,230 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-25 14:43:22,232 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 14:43:22,233 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 14:43:27,092 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 14:43:27,095 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 14:43:27,095 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 14:43:27,096 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.7313, 'grad_norm': 6.11224889755249, 'learning_rate': 3.160412067696836e-05, 'epoch': 1.1}\n"," 37% 3000/8154 [40:28<1:07:18,  1.28it/s][INFO|trainer.py:3944] 2025-03-25 14:50:06,846 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-25 14:50:06,848 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 14:50:06,849 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 14:50:11,710 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 14:50:11,714 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 14:50:11,715 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 14:50:11,715 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-3000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.5117, 'grad_norm': 6.583982467651367, 'learning_rate': 2.853814078979642e-05, 'epoch': 1.29}\n"," 43% 3500/8154 [47:14<58:55,  1.32it/s][INFO|trainer.py:3944] 2025-03-25 14:56:53,283 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-3500\n","[INFO|configuration_utils.py:423] 2025-03-25 14:56:53,285 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-3500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 14:56:53,285 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-3500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 14:56:58,125 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-3500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 14:56:58,128 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 14:56:58,129 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-3500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 14:56:58,129 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-3500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4415, 'grad_norm': 6.475767612457275, 'learning_rate': 2.5472160902624482e-05, 'epoch': 1.47}\n"," 49% 4000/8154 [54:00<53:53,  1.28it/s][INFO|trainer.py:3944] 2025-03-25 15:03:39,271 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-4000\n","[INFO|configuration_utils.py:423] 2025-03-25 15:03:39,272 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-4000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 15:03:39,273 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-4000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 15:03:44,157 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 15:03:44,161 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 15:03:44,162 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-4000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 15:03:44,162 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-4000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4201, 'grad_norm': 6.162585735321045, 'learning_rate': 2.240618101545254e-05, 'epoch': 1.66}\n"," 55% 4500/8154 [1:00:52<46:53,  1.30it/s][INFO|trainer.py:3944] 2025-03-25 15:10:30,659 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-4500\n","[INFO|configuration_utils.py:423] 2025-03-25 15:10:30,660 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-4500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 15:10:30,661 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-4500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 15:10:35,584 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-4500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 15:10:35,587 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 15:10:35,587 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-4500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 15:10:35,588 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-4500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.386, 'grad_norm': 6.274016857147217, 'learning_rate': 1.93402011282806e-05, 'epoch': 1.84}\n"," 61% 5000/8154 [1:07:42<42:07,  1.25it/s][INFO|trainer.py:3944] 2025-03-25 15:17:21,234 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-5000\n","[INFO|configuration_utils.py:423] 2025-03-25 15:17:21,236 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-5000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 15:17:21,237 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-5000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 15:17:26,147 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-5000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 15:17:26,149 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 15:17:26,150 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-5000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 15:17:26,150 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-5000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.2919, 'grad_norm': 6.371399402618408, 'learning_rate': 1.627422124110866e-05, 'epoch': 2.02}\n"," 67% 5500/8154 [1:14:31<33:23,  1.32it/s][INFO|trainer.py:3944] 2025-03-25 15:24:09,536 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-5500\n","[INFO|configuration_utils.py:423] 2025-03-25 15:24:09,537 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-5500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 15:24:09,538 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-5500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 15:24:14,427 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-5500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 15:24:14,431 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 15:24:14,432 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-5500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 15:24:14,432 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-5500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9772, 'grad_norm': 6.803675651550293, 'learning_rate': 1.3208241353936717e-05, 'epoch': 2.21}\n"," 74% 6000/8154 [1:21:23<29:54,  1.20it/s][INFO|trainer.py:3944] 2025-03-25 15:31:01,731 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-6000\n","[INFO|configuration_utils.py:423] 2025-03-25 15:31:01,733 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-6000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 15:31:01,734 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-6000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 15:31:06,638 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-6000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 15:31:06,641 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 15:31:06,641 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-6000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 15:31:06,641 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-6000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9646, 'grad_norm': 5.226804733276367, 'learning_rate': 1.0142261466764778e-05, 'epoch': 2.39}\n"," 80% 6500/8154 [1:28:11<23:30,  1.17it/s][INFO|trainer.py:3944] 2025-03-25 15:37:49,931 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-6500\n","[INFO|configuration_utils.py:423] 2025-03-25 15:37:49,933 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-6500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 15:37:49,934 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-6500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 15:37:54,866 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-6500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 15:37:54,870 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 15:37:54,871 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-6500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 15:37:54,871 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-6500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.955, 'grad_norm': 7.372836589813232, 'learning_rate': 7.076281579592839e-06, 'epoch': 2.58}\n"," 86% 7000/8154 [1:34:59<16:05,  1.20it/s][INFO|trainer.py:3944] 2025-03-25 15:44:38,215 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-7000\n","[INFO|configuration_utils.py:423] 2025-03-25 15:44:38,217 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-7000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 15:44:38,218 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-7000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 15:44:43,089 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-7000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 15:44:43,092 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 15:44:43,093 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-7000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 15:44:43,093 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-7000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9332, 'grad_norm': 6.598888397216797, 'learning_rate': 4.0103016924208985e-06, 'epoch': 2.76}\n"," 92% 7500/8154 [1:41:47<08:02,  1.35it/s][INFO|trainer.py:3944] 2025-03-25 15:51:26,116 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-7500\n","[INFO|configuration_utils.py:423] 2025-03-25 15:51:26,118 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-7500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 15:51:26,118 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-7500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 15:51:31,069 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-7500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 15:51:31,072 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 15:51:31,073 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-7500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 15:51:31,074 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-7500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9073, 'grad_norm': 6.615671157836914, 'learning_rate': 9.443218052489575e-07, 'epoch': 2.94}\n"," 98% 8000/8154 [1:48:35<02:02,  1.26it/s][INFO|trainer.py:3944] 2025-03-25 15:58:14,327 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-8000\n","[INFO|configuration_utils.py:423] 2025-03-25 15:58:14,328 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-8000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 15:58:14,329 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-8000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 15:58:19,260 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-8000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 15:58:19,263 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 15:58:19,264 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-8000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 15:58:19,264 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-8000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 8154/8154 [1:50:53<00:00,  1.61it/s][INFO|trainer.py:3944] 2025-03-25 16:00:31,997 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-8154\n","[INFO|configuration_utils.py:423] 2025-03-25 16:00:31,999 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-8154/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 16:00:32,000 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-8154/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 16:00:37,217 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-8154/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 16:00:37,221 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-8154/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 16:00:37,221 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-8154/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 16:00:37,222 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/checkpoint-8154/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-25 16:00:50,903 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 6673.3392, 'train_samples_per_second': 9.772, 'train_steps_per_second': 1.222, 'train_loss': 2.6130231530285655, 'epoch': 3.0}\n","100% 8154/8154 [1:51:12<00:00,  1.22it/s]\n","[INFO|trainer.py:3944] 2025-03-25 16:00:50,911 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-16000\n","[INFO|configuration_utils.py:423] 2025-03-25 16:00:50,913 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 16:00:50,914 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-16000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 16:00:59,894 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-16000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 16:00:59,897 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 16:00:59,898 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 16:00:59,898 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-16000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  6691516GF\n","  train_loss               =      2.613\n","  train_runtime            = 1:51:13.33\n","  train_samples            =      21737\n","  train_samples_per_second =      9.772\n","  train_steps_per_second   =      1.222\n","03/25/2025 16:01:00 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-25 16:01:00,053 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-25 16:01:00,054 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-25 16:01:00,054 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [08:45<00:00,  4.20s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     9.8859\n","  eval_chrf               =    47.0916\n","  eval_gen_len            =    45.9188\n","  eval_loss               =     2.0985\n","  eval_runtime            = 0:08:52.54\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.872\n","  eval_steps_per_second   =      0.235\n","03/25/2025 16:09:52 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-25 16:09:52,613 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-25 16:09:52,613 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-25 16:09:52,613 >>   Batch size = 8\n","100% 127/127 [09:09<00:00,  4.32s/it]\n","***** predict metrics *****\n","  predict_bleu               =     9.9987\n","  predict_chrf               =    47.1693\n","  predict_gen_len            =    47.2164\n","  predict_loss               =     2.1239\n","  predict_runtime            = 0:09:15.96\n","  predict_samples            =       1012\n","  predict_samples_per_second =       1.82\n","  predict_steps_per_second   =      0.228\n","[INFO|modelcard.py:449] 2025-03-25 16:19:23,949 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 9.8859}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/trainer_16000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-plus-16000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SlaWlrx1-cgd"},"outputs":[],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=f\"random_{language}-16000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMsO6Cm7-cgd"},"outputs":[],"source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/random/en-zu/train_16000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/random-16000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":40},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1742919567411,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"VcvXjtLvoqP4","outputId":"c5fd3b69-a4cf-4d3a-933e-192a767660b5"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/izuqi7qg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7eaef1840460>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=f\"africomet_{language}-32000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10448520,"status":"ok","timestamp":1742930015924,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"xLXHxNu2o2rj","outputId":"9d11371a-6a95-4448-ebc8-ea91ca2d4994"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-25 16:19:33.341961: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-25 16:19:33.364116: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-25 16:19:33.370617: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-25 16:19:33.387447: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-25 16:19:34.497603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/25/2025 16:19:39 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/25/2025 16:19:39 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-32000/runs/Mar25_16-19-39_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-32000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-32000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-93c76eb23e51e985\n","03/25/2025 16:19:39 - INFO - datasets.builder - Using custom data configuration default-93c76eb23e51e985\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/25/2025 16:19:39 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/25/2025 16:19:39 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/25/2025 16:19:39 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/25/2025 16:19:39 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/25/2025 16:19:39 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-25 16:19:39,715 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-25 16:19:39,718 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-25 16:19:39,805 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-25 16:19:39,806 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 16:19:39,807 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 16:19:39,807 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 16:19:39,807 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 16:19:39,807 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 16:19:39,807 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 16:19:39,807 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 16:19:39,807 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-25 16:19:39,807 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-25 16:19:39,808 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-25 16:19:40,798 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-25 16:19:40,869 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-25 16:19:40,918 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-25 16:19:40,981 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-25 16:19:40,981 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-25 16:19:41,073 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-25 16:19:41,074 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-25 16:19:41,569 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e7e4ea04dc2a8f5f.arrow\n","03/25/2025 16:19:42 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e7e4ea04dc2a8f5f.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-caa473c3d2478b37.arrow\n","03/25/2025 16:19:43 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-caa473c3d2478b37.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1700a6a31f02dd2e.arrow\n","03/25/2025 16:19:45 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-93c76eb23e51e985/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1700a6a31f02dd2e.arrow\n","[INFO|trainer.py:2407] 2025-03-25 16:19:55,010 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-25 16:19:55,010 >>   Num examples = 32,000\n","[INFO|trainer.py:2409] 2025-03-25 16:19:55,010 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-25 16:19:55,010 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-25 16:19:55,010 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-25 16:19:55,010 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-25 16:19:55,010 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-25 16:19:55,010 >>   Total optimization steps = 12,000\n","[INFO|trainer.py:2416] 2025-03-25 16:19:55,011 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-25 16:19:55,019 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250325_161955-ljrnw7bw\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-32000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/ljrnw7bw\u001b[0m\n","  0% 0/12000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.3917, 'grad_norm': 7.4095845222473145, 'learning_rate': 4.791666666666667e-05, 'epoch': 0.12}\n","  4% 500/12000 [05:57<2:16:38,  1.40it/s][INFO|trainer.py:3944] 2025-03-25 16:25:53,660 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-25 16:25:53,668 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 16:25:53,669 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 16:26:05,121 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 16:26:05,125 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 16:26:05,125 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 16:26:05,125 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.8961, 'grad_norm': 7.585800647735596, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}\n","  8% 1000/12000 [12:24<2:14:20,  1.36it/s][INFO|trainer.py:3944] 2025-03-25 16:32:20,156 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-25 16:32:20,167 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 16:32:20,167 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 16:32:31,762 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 16:32:31,766 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 16:32:31,766 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 16:32:31,767 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.6336, 'grad_norm': 6.686267375946045, 'learning_rate': 4.375e-05, 'epoch': 0.38}\n"," 12% 1500/12000 [18:51<1:59:50,  1.46it/s][INFO|trainer.py:3944] 2025-03-25 16:38:46,976 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-25 16:38:46,980 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 16:38:46,981 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 16:38:58,511 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 16:38:58,515 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 16:38:58,515 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 16:38:58,516 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.4467, 'grad_norm': 7.441803455352783, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n"," 17% 2000/12000 [25:19<1:58:44,  1.40it/s][INFO|trainer.py:3944] 2025-03-25 16:45:15,277 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-25 16:45:15,280 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 16:45:15,280 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 16:45:26,840 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 16:45:26,844 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 16:45:26,844 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 16:45:26,845 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.3128, 'grad_norm': 6.87680721282959, 'learning_rate': 3.958333333333333e-05, 'epoch': 0.62}\n"," 21% 2500/12000 [31:43<1:48:00,  1.47it/s][INFO|trainer.py:3944] 2025-03-25 16:51:39,845 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-25 16:51:39,847 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 16:51:39,848 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 16:51:51,408 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 16:51:51,413 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 16:51:51,413 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 16:51:51,414 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.2138, 'grad_norm': 6.000250339508057, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}\n"," 25% 3000/12000 [38:09<1:46:24,  1.41it/s][INFO|trainer.py:3944] 2025-03-25 16:58:04,939 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-25 16:58:04,944 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 16:58:04,945 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 16:58:16,570 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 16:58:16,574 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 16:58:16,575 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 16:58:16,575 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.1438, 'grad_norm': 6.375298976898193, 'learning_rate': 3.541666666666667e-05, 'epoch': 0.88}\n"," 29% 3500/12000 [44:35<1:46:42,  1.33it/s][INFO|trainer.py:3944] 2025-03-25 17:04:31,462 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3500\n","[INFO|configuration_utils.py:423] 2025-03-25 17:04:31,466 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 17:04:31,467 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 17:04:43,010 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 17:04:43,014 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 17:04:43,015 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 17:04:43,015 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-3500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.0542, 'grad_norm': 5.641180992126465, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"," 33% 4000/12000 [51:00<1:37:16,  1.37it/s][INFO|trainer.py:3944] 2025-03-25 17:10:56,415 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4000\n","[INFO|configuration_utils.py:423] 2025-03-25 17:10:56,418 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 17:10:56,418 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 17:11:07,911 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 17:11:07,915 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 17:11:07,916 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 17:11:07,916 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.5598, 'grad_norm': 7.050191402435303, 'learning_rate': 3.125e-05, 'epoch': 1.12}\n"," 38% 4500/12000 [57:25<1:26:58,  1.44it/s][INFO|trainer.py:3944] 2025-03-25 17:17:21,366 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4500\n","[INFO|configuration_utils.py:423] 2025-03-25 17:17:21,371 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 17:17:21,372 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 17:17:32,881 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 17:17:32,885 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 17:17:32,885 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 17:17:32,885 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-4500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.5932, 'grad_norm': 5.900742530822754, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}\n"," 42% 5000/12000 [1:03:49<1:24:13,  1.39it/s][INFO|trainer.py:3944] 2025-03-25 17:23:45,892 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5000\n","[INFO|configuration_utils.py:423] 2025-03-25 17:23:45,895 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 17:23:45,896 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 17:23:57,524 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 17:23:57,528 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 17:23:57,528 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 17:23:57,529 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.5722, 'grad_norm': 5.611404895782471, 'learning_rate': 2.7083333333333332e-05, 'epoch': 1.38}\n"," 46% 5500/12000 [1:10:15<1:23:04,  1.30it/s][INFO|trainer.py:3944] 2025-03-25 17:30:11,468 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5500\n","[INFO|configuration_utils.py:423] 2025-03-25 17:30:11,470 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 17:30:11,471 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 17:30:22,995 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 17:30:22,999 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 17:30:22,999 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 17:30:23,000 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-5500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.512, 'grad_norm': 6.898386478424072, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n"," 50% 6000/12000 [1:16:40<1:16:04,  1.31it/s][INFO|trainer.py:3944] 2025-03-25 17:36:36,072 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6000\n","[INFO|configuration_utils.py:423] 2025-03-25 17:36:36,081 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 17:36:36,082 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 17:36:47,675 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 17:36:47,679 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 17:36:47,680 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 17:36:47,680 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4927, 'grad_norm': 8.672322273254395, 'learning_rate': 2.2916666666666667e-05, 'epoch': 1.62}\n"," 54% 6500/12000 [1:23:01<1:03:26,  1.44it/s][INFO|trainer.py:3944] 2025-03-25 17:42:57,306 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6500\n","[INFO|configuration_utils.py:423] 2025-03-25 17:42:57,310 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 17:42:57,311 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 17:43:08,823 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 17:43:08,827 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 17:43:08,827 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 17:43:08,828 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-6500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4865, 'grad_norm': 5.917010307312012, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}\n"," 58% 7000/12000 [1:29:26<1:00:32,  1.38it/s][INFO|trainer.py:3944] 2025-03-25 17:49:22,211 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7000\n","[INFO|configuration_utils.py:423] 2025-03-25 17:49:22,213 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 17:49:22,214 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 17:49:33,727 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 17:49:33,731 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 17:49:33,731 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 17:49:33,732 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.455, 'grad_norm': 7.390806198120117, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.88}\n"," 62% 7500/12000 [1:35:52<54:19,  1.38it/s][INFO|trainer.py:3944] 2025-03-25 17:55:48,802 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7500\n","[INFO|configuration_utils.py:423] 2025-03-25 17:55:48,807 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 17:55:48,807 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 17:56:00,361 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 17:56:00,365 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 17:56:00,366 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 17:56:00,366 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-7500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4068, 'grad_norm': 6.27310037612915, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"," 67% 8000/12000 [1:42:20<45:00,  1.48it/s][INFO|trainer.py:3944] 2025-03-25 18:02:16,596 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8000\n","[INFO|configuration_utils.py:423] 2025-03-25 18:02:16,599 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 18:02:16,600 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 18:02:28,194 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 18:02:28,198 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 18:02:28,199 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 18:02:28,199 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.0253, 'grad_norm': 7.235912322998047, 'learning_rate': 1.4583333333333335e-05, 'epoch': 2.12}\n"," 71% 8500/12000 [1:48:46<40:30,  1.44it/s][INFO|trainer.py:3944] 2025-03-25 18:08:42,766 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8500\n","[INFO|configuration_utils.py:423] 2025-03-25 18:08:42,771 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 18:08:42,772 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 18:08:54,352 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 18:08:54,358 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 18:08:54,359 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 18:08:54,359 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-8500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.0176, 'grad_norm': 5.883730411529541, 'learning_rate': 1.25e-05, 'epoch': 2.25}\n"," 75% 9000/12000 [1:55:12<33:59,  1.47it/s][INFO|trainer.py:3944] 2025-03-25 18:15:08,249 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9000\n","[INFO|configuration_utils.py:423] 2025-03-25 18:15:08,252 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 18:15:08,253 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 18:15:19,834 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 18:15:19,838 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 18:15:19,839 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 18:15:19,839 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.0277, 'grad_norm': 6.255934238433838, 'learning_rate': 1.0416666666666668e-05, 'epoch': 2.38}\n"," 79% 9500/12000 [2:01:38<29:50,  1.40it/s][INFO|trainer.py:3944] 2025-03-25 18:21:34,871 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9500\n","[INFO|configuration_utils.py:423] 2025-03-25 18:21:34,875 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 18:21:34,876 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 18:21:46,403 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 18:21:46,407 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 18:21:46,407 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 18:21:46,408 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-9500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.0241, 'grad_norm': 7.130779266357422, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"," 83% 10000/12000 [2:08:04<24:32,  1.36it/s][INFO|trainer.py:3944] 2025-03-25 18:28:00,442 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10000\n","[INFO|configuration_utils.py:423] 2025-03-25 18:28:00,452 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 18:28:00,453 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 18:28:12,072 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 18:28:12,076 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 18:28:12,076 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 18:28:12,077 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.987, 'grad_norm': 6.365339756011963, 'learning_rate': 6.25e-06, 'epoch': 2.62}\n"," 88% 10500/12000 [2:14:30<18:01,  1.39it/s][INFO|trainer.py:3944] 2025-03-25 18:34:26,005 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10500\n","[INFO|configuration_utils.py:423] 2025-03-25 18:34:26,007 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 18:34:26,008 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 18:34:37,592 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 18:34:37,596 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 18:34:37,597 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 18:34:37,597 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-10500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.0068, 'grad_norm': 5.302623748779297, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}\n"," 92% 11000/12000 [2:20:55<12:41,  1.31it/s][INFO|trainer.py:3944] 2025-03-25 18:40:51,821 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11000\n","[INFO|configuration_utils.py:423] 2025-03-25 18:40:51,824 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 18:40:51,825 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 18:41:03,398 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 18:41:03,403 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 18:41:03,403 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 18:41:03,404 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9606, 'grad_norm': 7.518202304840088, 'learning_rate': 2.0833333333333334e-06, 'epoch': 2.88}\n"," 96% 11500/12000 [2:27:20<05:39,  1.47it/s][INFO|trainer.py:3944] 2025-03-25 18:47:16,788 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11500\n","[INFO|configuration_utils.py:423] 2025-03-25 18:47:16,790 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 18:47:16,791 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 18:47:28,359 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 18:47:28,363 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 18:47:28,364 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 18:47:28,364 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-11500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.978, 'grad_norm': 6.595338821411133, 'learning_rate': 0.0, 'epoch': 3.0}\n","100% 12000/12000 [2:33:46<00:00,  1.43it/s][INFO|trainer.py:3944] 2025-03-25 18:53:42,401 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000/checkpoint-12000\n","[INFO|configuration_utils.py:423] 2025-03-25 18:53:42,406 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-12000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 18:53:42,407 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-12000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 18:53:53,987 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-12000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 18:53:53,992 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 18:53:53,992 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-12000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 18:53:53,993 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/checkpoint-12000/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-25 18:54:12,226 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 9257.2148, 'train_samples_per_second': 10.37, 'train_steps_per_second': 1.296, 'train_loss': 2.67492138671875, 'epoch': 3.0}\n","100% 12000/12000 [2:34:16<00:00,  1.30it/s]\n","[INFO|trainer.py:3944] 2025-03-25 18:54:12,234 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-32000\n","[INFO|configuration_utils.py:423] 2025-03-25 18:54:12,236 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 18:54:12,237 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-32000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 18:54:23,876 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-32000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 18:54:23,879 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-32000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 18:54:23,879 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 18:54:23,880 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-32000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  6605052GF\n","  train_loss               =     2.6749\n","  train_runtime            = 2:34:17.21\n","  train_samples            =      32000\n","  train_samples_per_second =      10.37\n","  train_steps_per_second   =      1.296\n","03/25/2025 18:54:24 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-25 18:54:24,024 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-25 18:54:24,024 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-25 18:54:24,024 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [09:04<00:00,  4.35s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     9.0061\n","  eval_chrf               =     44.614\n","  eval_gen_len            =    45.3139\n","  eval_loss               =     2.2372\n","  eval_runtime            = 0:09:09.80\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.813\n","  eval_steps_per_second   =      0.227\n","03/25/2025 19:03:33 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-25 19:03:33,835 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-25 19:03:33,835 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-25 19:03:33,835 >>   Batch size = 8\n","100% 127/127 [09:36<00:00,  4.54s/it]\n","***** predict metrics *****\n","  predict_bleu               =     8.6982\n","  predict_chrf               =     44.371\n","  predict_gen_len            =    46.6621\n","  predict_loss               =     2.2735\n","  predict_runtime            = 0:09:43.34\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.735\n","  predict_steps_per_second   =      0.218\n","[INFO|modelcard.py:449] 2025-03-25 19:13:32,635 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 9.0061}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 python /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_32000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-32000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":40},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1742930015925,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"FJl5ZjM7oqOB","outputId":"ab18aaf9-3bf0-4456-9fbc-25b7198bcf8d"},"outputs":[{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/izuqi7qg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7eaef1840460>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=f\"africomet_{language}-plus-32000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12355548,"status":"ok","timestamp":1742942371463,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"},"user_tz":-120},"id":"N-_bLDUyo858","outputId":"7de3af99-28cf-4177-ee4d-c5aa33087be3"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-25 19:13:41.814633: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-25 19:13:41.837092: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-25 19:13:41.843644: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-25 19:13:41.859436: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-25 19:13:42.944176: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/25/2025 19:13:47 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/25/2025 19:13:47 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/wmt22-cometkiwi-da-plus-32000/runs/Mar25_19-13-47_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/wmt22-cometkiwi-da-plus-32000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/wmt22-cometkiwi-da-plus-32000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-bfabaa4e2c76cc07\n","03/25/2025 19:13:47 - INFO - datasets.builder - Using custom data configuration default-bfabaa4e2c76cc07\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/25/2025 19:13:47 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Generating dataset json (/root/.cache/huggingface/datasets/json/default-bfabaa4e2c76cc07/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/25/2025 19:13:47 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-bfabaa4e2c76cc07/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-bfabaa4e2c76cc07/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","03/25/2025 19:13:47 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-bfabaa4e2c76cc07/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","Downloading took 0.0 min\n","03/25/2025 19:13:47 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","03/25/2025 19:13:47 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Generating train split\n","03/25/2025 19:13:47 - INFO - datasets.builder - Generating train split\n","Generating train split: 37737 examples [00:00, 482053.84 examples/s]\n","Generating validation split\n","03/25/2025 19:13:47 - INFO - datasets.builder - Generating validation split\n","Generating validation split: 997 examples [00:00, 218059.19 examples/s]\n","Generating test split\n","03/25/2025 19:13:47 - INFO - datasets.builder - Generating test split\n","Generating test split: 1012 examples [00:00, 226139.35 examples/s]\n","Unable to verify splits sizes.\n","03/25/2025 19:13:47 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-bfabaa4e2c76cc07/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","03/25/2025 19:13:47 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-bfabaa4e2c76cc07/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","[INFO|configuration_utils.py:699] 2025-03-25 19:13:48,114 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-25 19:13:48,116 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-25 19:13:48,218 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-25 19:13:48,218 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 19:13:48,219 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 19:13:48,219 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 19:13:48,219 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 19:13:48,219 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 19:13:48,219 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 19:13:48,219 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-25 19:13:48,220 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-25 19:13:48,220 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-25 19:13:48,221 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-25 19:13:49,189 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-25 19:13:49,258 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-25 19:13:49,325 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-25 19:13:49,377 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-25 19:13:49,377 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-25 19:13:49,532 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-25 19:13:49,533 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-25 19:13:49,686 >> Safetensors PR exists\n","Running tokenizer on train dataset:   0% 0/37737 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-bfabaa4e2c76cc07/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a66523ab1ce03dba.arrow\n","03/25/2025 19:13:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-bfabaa4e2c76cc07/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-a66523ab1ce03dba.arrow\n","Running tokenizer on train dataset: 100% 37737/37737 [00:13<00:00, 2862.97 examples/s]\n","Running tokenizer on validation dataset:   0% 0/997 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-bfabaa4e2c76cc07/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-6c75600c1f3c2124.arrow\n","03/25/2025 19:14:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-bfabaa4e2c76cc07/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-6c75600c1f3c2124.arrow\n","Running tokenizer on validation dataset: 100% 997/997 [00:00<00:00, 1860.91 examples/s]\n","Running tokenizer on prediction dataset:   0% 0/1012 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-bfabaa4e2c76cc07/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0164deae71dec659.arrow\n","03/25/2025 19:14:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-bfabaa4e2c76cc07/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0164deae71dec659.arrow\n","Running tokenizer on prediction dataset: 100% 1012/1012 [00:00<00:00, 1790.32 examples/s]\n","[INFO|trainer.py:2407] 2025-03-25 19:14:17,710 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-25 19:14:17,710 >>   Num examples = 37,737\n","[INFO|trainer.py:2409] 2025-03-25 19:14:17,710 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-25 19:14:17,710 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2412] 2025-03-25 19:14:17,711 >>   Training with DataParallel so batch size has been adjusted to: 8\n","[INFO|trainer.py:2413] 2025-03-25 19:14:17,711 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:2414] 2025-03-25 19:14:17,711 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-25 19:14:17,711 >>   Total optimization steps = 14,154\n","[INFO|trainer.py:2416] 2025-03-25 19:14:17,712 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-25 19:14:17,718 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250325_191417-306lsu1k\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/wmt22-cometkiwi-da-plus-32000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/306lsu1k\u001b[0m\n","  0% 0/14154 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 4.2975, 'grad_norm': 6.715846538543701, 'learning_rate': 4.8233714850925535e-05, 'epoch': 0.11}\n","  4% 500/14154 [06:16<2:38:33,  1.44it/s][INFO|trainer.py:3944] 2025-03-25 19:20:35,450 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-25 19:20:35,458 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 19:20:35,459 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 19:20:40,520 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 19:20:40,523 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 19:20:40,524 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 19:20:40,524 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.7687, 'grad_norm': 7.544182777404785, 'learning_rate': 4.646742970185107e-05, 'epoch': 0.21}\n","  7% 1000/14154 [12:51<2:50:48,  1.28it/s][INFO|trainer.py:3944] 2025-03-25 19:27:09,857 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-25 19:27:09,858 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 19:27:09,859 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 19:27:14,800 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 19:27:14,803 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 19:27:14,803 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 19:27:14,804 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.5093, 'grad_norm': 8.106263160705566, 'learning_rate': 4.47011445527766e-05, 'epoch': 0.32}\n"," 11% 1500/14154 [19:27<2:31:33,  1.39it/s][INFO|trainer.py:3944] 2025-03-25 19:33:45,815 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-1500\n","[INFO|configuration_utils.py:423] 2025-03-25 19:33:45,817 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 19:33:45,818 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 19:33:50,777 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-1500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 19:33:50,781 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 19:33:50,781 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-1500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 19:33:50,781 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-1500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.319, 'grad_norm': 5.6159892082214355, 'learning_rate': 4.293485940370214e-05, 'epoch': 0.42}\n"," 14% 2000/14154 [26:01<2:31:03,  1.34it/s][INFO|trainer.py:3944] 2025-03-25 19:40:20,157 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-2000\n","[INFO|configuration_utils.py:423] 2025-03-25 19:40:20,159 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 19:40:20,159 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 19:40:25,167 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 19:40:25,170 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 19:40:25,170 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 19:40:25,171 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-2000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.1969, 'grad_norm': 5.770010471343994, 'learning_rate': 4.1168574254627665e-05, 'epoch': 0.53}\n"," 18% 2500/14154 [32:40<2:33:13,  1.27it/s][INFO|trainer.py:3944] 2025-03-25 19:46:58,924 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-2500\n","[INFO|configuration_utils.py:423] 2025-03-25 19:46:58,926 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-2500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 19:46:58,927 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-2500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 19:47:03,844 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-2500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 19:47:03,847 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 19:47:03,847 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-2500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 19:47:03,847 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.0922, 'grad_norm': 6.036867618560791, 'learning_rate': 3.9402289105553204e-05, 'epoch': 0.64}\n"," 21% 3000/14154 [39:16<2:30:41,  1.23it/s][INFO|trainer.py:3944] 2025-03-25 19:53:34,749 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-3000\n","[INFO|configuration_utils.py:423] 2025-03-25 19:53:34,750 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 19:53:34,751 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-3000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 19:53:39,718 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-3000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 19:53:39,721 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 19:53:39,722 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 19:53:39,722 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-3000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.9954, 'grad_norm': 5.9100847244262695, 'learning_rate': 3.763600395647874e-05, 'epoch': 0.74}\n"," 25% 3500/14154 [45:54<2:12:00,  1.35it/s][INFO|trainer.py:3944] 2025-03-25 20:00:13,018 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-3500\n","[INFO|configuration_utils.py:423] 2025-03-25 20:00:13,020 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-3500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 20:00:13,021 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-3500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 20:00:17,976 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-3500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 20:00:17,979 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 20:00:17,979 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-3500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 20:00:17,980 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-3500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.9101, 'grad_norm': 6.589414119720459, 'learning_rate': 3.586971880740427e-05, 'epoch': 0.85}\n"," 28% 4000/14154 [52:32<2:04:45,  1.36it/s][INFO|trainer.py:3944] 2025-03-25 20:06:51,121 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-4000\n","[INFO|configuration_utils.py:423] 2025-03-25 20:06:51,123 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-4000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 20:06:51,124 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-4000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 20:06:56,084 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-4000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 20:06:56,087 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 20:06:56,087 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-4000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 20:06:56,088 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-4000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.8448, 'grad_norm': 5.959824562072754, 'learning_rate': 3.41034336583298e-05, 'epoch': 0.95}\n"," 32% 4500/14154 [59:05<1:57:22,  1.37it/s][INFO|trainer.py:3944] 2025-03-25 20:13:24,118 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-4500\n","[INFO|configuration_utils.py:423] 2025-03-25 20:13:24,120 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-4500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 20:13:24,121 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-4500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 20:13:29,052 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-4500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 20:13:29,055 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 20:13:29,056 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-4500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 20:13:29,056 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-4500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.632, 'grad_norm': 5.6130805015563965, 'learning_rate': 3.233714850925534e-05, 'epoch': 1.06}\n"," 35% 5000/14154 [1:05:42<1:54:35,  1.33it/s][INFO|trainer.py:3944] 2025-03-25 20:20:01,414 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-5000\n","[INFO|configuration_utils.py:423] 2025-03-25 20:20:01,416 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-5000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 20:20:01,416 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-5000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 20:20:06,338 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-5000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 20:20:06,340 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 20:20:06,341 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-5000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 20:20:06,341 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-5000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.4145, 'grad_norm': 5.4745402336120605, 'learning_rate': 3.057086336018087e-05, 'epoch': 1.17}\n"," 39% 5500/14154 [1:12:21<1:45:52,  1.36it/s][INFO|trainer.py:3944] 2025-03-25 20:26:39,775 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-5500\n","[INFO|configuration_utils.py:423] 2025-03-25 20:26:39,777 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-5500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 20:26:39,778 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-5500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 20:26:44,739 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-5500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 20:26:44,742 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 20:26:44,742 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-5500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 20:26:44,742 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-5500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3837, 'grad_norm': 6.860598564147949, 'learning_rate': 2.8804578211106403e-05, 'epoch': 1.27}\n"," 42% 6000/14154 [1:18:56<1:43:35,  1.31it/s][INFO|trainer.py:3944] 2025-03-25 20:33:15,545 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-6000\n","[INFO|configuration_utils.py:423] 2025-03-25 20:33:15,547 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-6000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 20:33:15,548 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-6000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 20:33:20,569 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-6000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 20:33:20,572 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 20:33:20,573 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-6000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 20:33:20,573 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-6000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3654, 'grad_norm': 6.220436096191406, 'learning_rate': 2.7038293062031932e-05, 'epoch': 1.38}\n"," 46% 6500/14154 [1:25:31<1:29:57,  1.42it/s][INFO|trainer.py:3944] 2025-03-25 20:39:50,200 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-6500\n","[INFO|configuration_utils.py:423] 2025-03-25 20:39:50,201 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-6500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 20:39:50,202 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-6500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 20:39:55,151 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-6500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 20:39:55,154 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 20:39:55,154 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-6500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 20:39:55,154 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-6500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.352, 'grad_norm': 4.684757709503174, 'learning_rate': 2.5272007912957468e-05, 'epoch': 1.48}\n"," 49% 7000/14154 [1:32:07<1:33:26,  1.28it/s][INFO|trainer.py:3944] 2025-03-25 20:46:26,397 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-7000\n","[INFO|configuration_utils.py:423] 2025-03-25 20:46:26,399 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-7000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 20:46:26,400 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-7000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 20:46:31,408 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-7000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 20:46:31,411 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 20:46:31,412 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-7000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 20:46:31,412 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-7000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3241, 'grad_norm': 5.857235431671143, 'learning_rate': 2.3505722763883004e-05, 'epoch': 1.59}\n"," 53% 7500/14154 [1:38:45<1:23:42,  1.32it/s][INFO|trainer.py:3944] 2025-03-25 20:53:04,473 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-7500\n","[INFO|configuration_utils.py:423] 2025-03-25 20:53:04,475 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-7500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 20:53:04,476 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-7500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 20:53:09,576 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-7500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 20:53:09,580 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 20:53:09,580 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-7500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 20:53:09,581 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-7500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3243, 'grad_norm': 6.37507438659668, 'learning_rate': 2.1739437614808536e-05, 'epoch': 1.7}\n"," 57% 8000/14154 [1:45:23<1:13:57,  1.39it/s][INFO|trainer.py:3944] 2025-03-25 20:59:42,421 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-8000\n","[INFO|configuration_utils.py:423] 2025-03-25 20:59:42,423 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-8000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 20:59:42,424 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-8000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 20:59:47,477 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-8000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 20:59:47,480 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 20:59:47,481 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-8000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 20:59:47,481 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-8000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.2824, 'grad_norm': 7.17487096786499, 'learning_rate': 1.997315246573407e-05, 'epoch': 1.8}\n"," 60% 8500/14154 [1:52:00<1:08:49,  1.37it/s][INFO|trainer.py:3944] 2025-03-25 21:06:19,318 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-8500\n","[INFO|configuration_utils.py:423] 2025-03-25 21:06:19,320 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-8500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 21:06:19,320 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-8500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 21:06:24,366 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-8500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 21:06:24,370 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 21:06:24,371 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-8500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 21:06:24,371 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-8500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.2445, 'grad_norm': 5.2771782875061035, 'learning_rate': 1.8206867316659605e-05, 'epoch': 1.91}\n"," 64% 9000/14154 [1:58:40<1:06:19,  1.30it/s][INFO|trainer.py:3944] 2025-03-25 21:12:58,804 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-9000\n","[INFO|configuration_utils.py:423] 2025-03-25 21:12:58,806 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-9000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 21:12:58,806 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-9000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 21:13:03,929 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-9000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 21:13:03,932 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 21:13:03,932 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-9000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 21:13:03,933 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-9000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.2237, 'grad_norm': 4.4121904373168945, 'learning_rate': 1.6440582167585137e-05, 'epoch': 2.01}\n"," 67% 9500/14154 [2:05:20<1:00:44,  1.28it/s][INFO|trainer.py:3944] 2025-03-25 21:19:38,677 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-9500\n","[INFO|configuration_utils.py:423] 2025-03-25 21:19:38,679 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-9500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 21:19:38,679 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-9500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 21:19:43,767 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-9500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 21:19:43,770 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 21:19:43,771 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-9500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 21:19:43,771 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-9500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9209, 'grad_norm': 6.2525529861450195, 'learning_rate': 1.4674297018510668e-05, 'epoch': 2.12}\n"," 71% 10000/14154 [2:11:58<49:07,  1.41it/s][INFO|trainer.py:3944] 2025-03-25 21:26:16,998 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-10000\n","[INFO|configuration_utils.py:423] 2025-03-25 21:26:17,000 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-10000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 21:26:17,001 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-10000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 21:26:21,999 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-10000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 21:26:22,002 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 21:26:22,003 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-10000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 21:26:22,003 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-10000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9225, 'grad_norm': 6.404486179351807, 'learning_rate': 1.29080118694362e-05, 'epoch': 2.23}\n"," 74% 10500/14154 [2:18:38<42:52,  1.42it/s][INFO|trainer.py:3944] 2025-03-25 21:32:56,881 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-10500\n","[INFO|configuration_utils.py:423] 2025-03-25 21:32:56,883 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-10500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 21:32:56,884 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-10500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 21:33:01,962 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-10500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 21:33:01,965 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 21:33:01,966 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-10500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 21:33:01,966 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-10500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.9089, 'grad_norm': 6.911657810211182, 'learning_rate': 1.1141726720361735e-05, 'epoch': 2.33}\n"," 78% 11000/14154 [2:25:15<38:57,  1.35it/s][INFO|trainer.py:3944] 2025-03-25 21:39:34,491 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-11000\n","[INFO|configuration_utils.py:423] 2025-03-25 21:39:34,493 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-11000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 21:39:34,494 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-11000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 21:39:39,541 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-11000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 21:39:39,545 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 21:39:39,545 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-11000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 21:39:39,546 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-11000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.8582, 'grad_norm': 6.200482368469238, 'learning_rate': 9.375441571287269e-06, 'epoch': 2.44}\n"," 81% 11500/14154 [2:31:54<32:36,  1.36it/s][INFO|trainer.py:3944] 2025-03-25 21:46:13,100 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-11500\n","[INFO|configuration_utils.py:423] 2025-03-25 21:46:13,102 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-11500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 21:46:13,102 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-11500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 21:46:18,178 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-11500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 21:46:18,181 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 21:46:18,182 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-11500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 21:46:18,182 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-11500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.8793, 'grad_norm': 6.740275859832764, 'learning_rate': 7.609156422212803e-06, 'epoch': 2.54}\n"," 85% 12000/14154 [2:38:33<27:17,  1.32it/s][INFO|trainer.py:3944] 2025-03-25 21:52:51,772 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-12000\n","[INFO|configuration_utils.py:423] 2025-03-25 21:52:51,775 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-12000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 21:52:51,776 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-12000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 21:52:56,808 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-12000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 21:52:56,813 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 21:52:56,813 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-12000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 21:52:56,814 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-12000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.8518, 'grad_norm': 5.539434432983398, 'learning_rate': 5.842871273138336e-06, 'epoch': 2.65}\n"," 88% 12500/14154 [2:45:13<21:10,  1.30it/s][INFO|trainer.py:3944] 2025-03-25 21:59:32,234 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-12500\n","[INFO|configuration_utils.py:423] 2025-03-25 21:59:32,236 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-12500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 21:59:32,236 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-12500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 21:59:37,372 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-12500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 21:59:37,375 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 21:59:37,376 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-12500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 21:59:37,377 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-12500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.8459, 'grad_norm': 5.0667195320129395, 'learning_rate': 4.076586124063869e-06, 'epoch': 2.76}\n"," 92% 13000/14154 [2:51:50<14:25,  1.33it/s][INFO|trainer.py:3944] 2025-03-25 22:06:08,990 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-13000\n","[INFO|configuration_utils.py:423] 2025-03-25 22:06:08,992 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-13000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 22:06:08,992 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-13000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 22:06:13,997 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-13000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 22:06:14,001 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 22:06:14,001 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-13000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 22:06:14,002 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-13000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.8555, 'grad_norm': 5.735628128051758, 'learning_rate': 2.3103009749894023e-06, 'epoch': 2.86}\n"," 95% 13500/14154 [2:58:24<07:59,  1.36it/s][INFO|trainer.py:3944] 2025-03-25 22:12:43,378 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-13500\n","[INFO|configuration_utils.py:423] 2025-03-25 22:12:43,380 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-13500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 22:12:43,381 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-13500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 22:12:48,442 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-13500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 22:12:48,446 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 22:12:48,447 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-13500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 22:12:48,447 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-13500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 1.8393, 'grad_norm': 6.125715732574463, 'learning_rate': 5.440158259149357e-07, 'epoch': 2.97}\n"," 99% 14000/14154 [3:04:57<01:52,  1.37it/s][INFO|trainer.py:3944] 2025-03-25 22:19:16,477 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-14000\n","[INFO|configuration_utils.py:423] 2025-03-25 22:19:16,479 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-14000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 22:19:16,480 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-14000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 22:19:21,509 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-14000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 22:19:21,512 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 22:19:21,513 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-14000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 22:19:21,513 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-14000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 14154/14154 [3:07:08<00:00,  1.65it/s][INFO|trainer.py:3944] 2025-03-25 22:21:27,567 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-14154\n","[INFO|configuration_utils.py:423] 2025-03-25 22:21:27,570 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-14154/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 22:21:27,570 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-14154/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 22:21:32,780 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-14154/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 22:21:32,783 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-14154/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 22:21:32,783 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-14154/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 22:21:32,784 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/checkpoint-14154/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-25 22:21:46,635 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 11248.9232, 'train_samples_per_second': 10.064, 'train_steps_per_second': 1.258, 'train_loss': 2.505351406311406, 'epoch': 3.0}\n","100% 14154/14154 [3:07:28<00:00,  1.26it/s]\n","[INFO|trainer.py:3944] 2025-03-25 22:21:46,643 >> Saving model checkpoint to M2M-100/wmt22-cometkiwi-da-plus-32000\n","[INFO|configuration_utils.py:423] 2025-03-25 22:21:46,645 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-25 22:21:46,646 >> Configuration saved in M2M-100/wmt22-cometkiwi-da-plus-32000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-25 22:21:55,625 >> Model weights saved in M2M-100/wmt22-cometkiwi-da-plus-32000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-25 22:21:55,628 >> tokenizer config file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-25 22:21:55,629 >> Special tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-25 22:21:55,629 >> added tokens file saved in M2M-100/wmt22-cometkiwi-da-plus-32000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               = 10220982GF\n","  train_loss               =     2.5054\n","  train_runtime            = 3:07:28.92\n","  train_samples            =      37737\n","  train_samples_per_second =     10.064\n","  train_steps_per_second   =      1.258\n","03/25/2025 22:21:55 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-25 22:21:55,788 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-25 22:21:55,788 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-25 22:21:55,788 >>   Batch size = 8\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 125/125 [08:08<00:00,  3.91s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =    10.8442\n","  eval_chrf               =    49.2251\n","  eval_gen_len            =    44.5065\n","  eval_loss               =     1.9316\n","  eval_runtime            = 0:08:15.49\n","  eval_samples            =        997\n","  eval_samples_per_second =      2.012\n","  eval_steps_per_second   =      0.252\n","03/25/2025 22:30:11 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-25 22:30:11,290 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-25 22:30:11,291 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-25 22:30:11,291 >>   Batch size = 8\n","100% 127/127 [08:54<00:00,  4.21s/it]\n","***** predict metrics *****\n","  predict_bleu               =    10.8899\n","  predict_chrf               =     48.944\n","  predict_gen_len            =    46.2767\n","  predict_loss               =     1.9676\n","  predict_runtime            = 0:09:00.94\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.871\n","  predict_steps_per_second   =      0.235\n","[INFO|modelcard.py:449] 2025-03-25 22:39:28,064 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 10.8442}]}\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/trainer_32000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/wmt22-cometkiwi-da-plus-32000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_F_RGd3to84r"},"outputs":[],"source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=f\"random_{language}-32000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"52Z6bMP_-cg-"},"outputs":[],"source":["!CUDA_VISIBLE_DEVICES=0,1 /content/AIMS-NLP-Project/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/random/en-zu/train_32000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/random-32000 \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}