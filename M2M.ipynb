{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP1vn8CLZMqCdyETvoE/dmY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import wandb"],"metadata":{"id":"vqsa3QW1weGy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Baseline Run"],"metadata":{"id":"qZQLQPZjy2QO"}},{"cell_type":"code","source":["# Log in with your API key\n","wandb.login(key=\"7f13d9fe09d0856f7c12099a27ccda7aa15c8afd\")\n","\n","# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"baseline\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"id":"aggyfHIUR2zp","executionInfo":{"status":"ok","timestamp":1741101907124,"user_tz":-120,"elapsed":4135,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"b713db02-10fa-41d5-d530-789a9c7479be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250304_152505-db1p07uv</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/paderborn-university/machine%20translation/runs/db1p07uv' target=\"_blank\">baseline</a></strong> to <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/paderborn-university/machine%20translation/runs/db1p07uv' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation/runs/db1p07uv</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/db1p07uv?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7c1e38bc88b0>"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/mafand/en-zu/merged.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100-baseline \\\n","    --per_device_train_batch_size=8 \\\n","    --per_device_eval_batch_size=8 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvM-Zf4jUO81","outputId":"cdd7e59e-8d10-40e3-fff6-6d61be26abbd","executionInfo":{"status":"ok","timestamp":1741104856723,"user_tz":-120,"elapsed":825747,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-04 15:30:06.825603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-04 15:30:06.846923: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-04 15:30:06.853542: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-04 15:30:06.870457: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-04 15:30:07.874354: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/04/2025 15:30:10 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/04/2025 15:30:10 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100-baseline/runs/Mar04_15-30-10_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100-baseline,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=8,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100-baseline,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-e5f93da27ac26768\n","03/04/2025 15:30:10 - INFO - datasets.builder - Using custom data configuration default-e5f93da27ac26768\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/04/2025 15:30:10 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/04/2025 15:30:10 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-e5f93da27ac26768/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/04/2025 15:30:10 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-e5f93da27ac26768/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-e5f93da27ac26768/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/04/2025 15:30:10 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-e5f93da27ac26768/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-e5f93da27ac26768/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/04/2025 15:30:10 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-e5f93da27ac26768/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-04 15:30:10,606 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-04 15:30:10,607 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-04 15:30:10,707 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-04 15:30:10,708 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-04 15:30:10,709 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-04 15:30:10,709 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-04 15:30:10,709 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-04 15:30:10,709 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-04 15:30:10,709 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-04 15:30:10,709 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-04 15:30:10,709 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-04 15:30:10,709 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-04 15:30:10,710 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-04 15:30:11,603 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-04 15:30:11,670 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-04 15:30:11,724 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-04 15:30:11,778 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-04 15:30:11,778 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-04 15:30:11,866 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-04 15:30:11,867 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-04 15:30:12,092 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e5f93da27ac26768/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b6e265a2abb058a2.arrow\n","03/04/2025 15:30:13 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e5f93da27ac26768/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b6e265a2abb058a2.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e5f93da27ac26768/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-45bd53168f168c04.arrow\n","03/04/2025 15:30:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e5f93da27ac26768/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-45bd53168f168c04.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e5f93da27ac26768/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-376af58a54ac8074.arrow\n","03/04/2025 15:30:15 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e5f93da27ac26768/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-376af58a54ac8074.arrow\n","[INFO|trainer.py:2407] 2025-03-04 15:30:18,024 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-04 15:30:18,024 >>   Num examples = 5,737\n","[INFO|trainer.py:2409] 2025-03-04 15:30:18,024 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-04 15:30:18,024 >>   Instantaneous batch size per device = 8\n","[INFO|trainer.py:2412] 2025-03-04 15:30:18,024 >>   Training with DataParallel so batch size has been adjusted to: 16\n","[INFO|trainer.py:2413] 2025-03-04 15:30:18,024 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:2414] 2025-03-04 15:30:18,024 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-04 15:30:18,024 >>   Total optimization steps = 1,077\n","[INFO|trainer.py:2416] 2025-03-04 15:30:18,026 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-04 15:30:18,031 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250304_153018-8401dyzh\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100-baseline\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/8401dyzh\u001b[0m\n","  0% 0/1077 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.3255, 'grad_norm': 5.437372207641602, 'learning_rate': 2.678737233054782e-05, 'epoch': 1.39}\n"," 46% 500/1077 [10:31<11:08,  1.16s/it][INFO|trainer.py:3944] 2025-03-04 15:40:50,493 >> Saving model checkpoint to M2M-100-baseline/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-04 15:40:50,502 >> Configuration saved in M2M-100-baseline/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-04 15:40:50,503 >> Configuration saved in M2M-100-baseline/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-04 15:40:55,371 >> Model weights saved in M2M-100-baseline/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-04 15:40:55,374 >> tokenizer config file saved in M2M-100-baseline/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-04 15:40:55,374 >> Special tokens file saved in M2M-100-baseline/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-04 15:40:55,375 >> added tokens file saved in M2M-100-baseline/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3609, 'grad_norm': 5.426982879638672, 'learning_rate': 3.574744661095636e-06, 'epoch': 2.79}\n"," 93% 1000/1077 [21:20<01:38,  1.28s/it][INFO|trainer.py:3944] 2025-03-04 15:51:39,624 >> Saving model checkpoint to M2M-100-baseline/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-04 15:51:39,625 >> Configuration saved in M2M-100-baseline/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-04 15:51:39,626 >> Configuration saved in M2M-100-baseline/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-04 15:51:44,499 >> Model weights saved in M2M-100-baseline/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-04 15:51:44,502 >> tokenizer config file saved in M2M-100-baseline/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-04 15:51:44,503 >> Special tokens file saved in M2M-100-baseline/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-04 15:51:44,503 >> added tokens file saved in M2M-100-baseline/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 1077/1077 [23:15<00:00,  1.09s/it][INFO|trainer.py:3944] 2025-03-04 15:53:34,754 >> Saving model checkpoint to M2M-100-baseline/checkpoint-1077\n","[INFO|configuration_utils.py:423] 2025-03-04 15:53:34,756 >> Configuration saved in M2M-100-baseline/checkpoint-1077/config.json\n","[INFO|configuration_utils.py:909] 2025-03-04 15:53:34,757 >> Configuration saved in M2M-100-baseline/checkpoint-1077/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-04 15:53:39,758 >> Model weights saved in M2M-100-baseline/checkpoint-1077/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-04 15:53:39,762 >> tokenizer config file saved in M2M-100-baseline/checkpoint-1077/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-04 15:53:39,763 >> Special tokens file saved in M2M-100-baseline/checkpoint-1077/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-04 15:53:39,763 >> added tokens file saved in M2M-100-baseline/checkpoint-1077/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-04 15:53:53,300 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 1415.2748, 'train_samples_per_second': 12.161, 'train_steps_per_second': 0.761, 'train_loss': 2.7940998555557974, 'epoch': 3.0}\n","100% 1077/1077 [23:34<00:00,  1.31s/it]\n","[INFO|trainer.py:3944] 2025-03-04 15:53:53,312 >> Saving model checkpoint to M2M-100-baseline\n","[INFO|configuration_utils.py:423] 2025-03-04 15:53:53,314 >> Configuration saved in M2M-100-baseline/config.json\n","[INFO|configuration_utils.py:909] 2025-03-04 15:53:53,314 >> Configuration saved in M2M-100-baseline/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-04 15:54:02,258 >> Model weights saved in M2M-100-baseline/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-04 15:54:02,263 >> tokenizer config file saved in M2M-100-baseline/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-04 15:54:02,263 >> Special tokens file saved in M2M-100-baseline/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-04 15:54:02,264 >> added tokens file saved in M2M-100-baseline/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  2920989GF\n","  train_loss               =     2.7941\n","  train_runtime            = 0:23:35.27\n","  train_samples            =       5737\n","  train_samples_per_second =     12.161\n","  train_steps_per_second   =      0.761\n","03/04/2025 15:54:02 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-04 15:54:02,408 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-04 15:54:02,409 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-04 15:54:02,409 >>   Batch size = 16\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 63/63 [09:16<00:00,  8.84s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     6.6097\n","  eval_gen_len            =    47.7282\n","  eval_loss               =     2.6394\n","  eval_runtime            = 0:09:28.65\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.753\n","  eval_steps_per_second   =      0.111\n","03/04/2025 16:03:31 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-04 16:03:31,073 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-04 16:03:31,073 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-04 16:03:31,073 >>   Batch size = 16\n","100% 64/64 [10:13<00:00,  9.59s/it]\n","***** predict metrics *****\n","  predict_bleu               =     6.7163\n","  predict_gen_len            =    49.7431\n","  predict_loss               =     2.6772\n","  predict_runtime            = 0:10:25.95\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.617\n","  predict_steps_per_second   =      0.102\n","[INFO|modelcard.py:449] 2025-03-04 16:14:12,618 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 6.6097}]}\n"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"hbNrB5boVIgU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740396255536,"user_tz":-120,"elapsed":8,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"8ff025dd-fa3b-44f5-aaf5-d4c3f5e13bed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Feb 24 11:24:14 2025       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   71C    P0              33W /  70W |      0MiB / 15360MiB |     55%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n","| N/A   54C    P0              32W /  70W |      0MiB / 15360MiB |     47%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]}]}