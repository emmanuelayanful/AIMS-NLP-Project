{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMF+PNjXPi2gf4/eJJVYlWw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import wandb"],"metadata":{"id":"vqsa3QW1weGy","executionInfo":{"status":"ok","timestamp":1741604750188,"user_tz":-120,"elapsed":729,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Baseline Run"],"metadata":{"id":"qZQLQPZjy2QO"}},{"cell_type":"code","source":["# Log in with your API key\n","wandb.login(key=\"7f13d9fe09d0856f7c12099a27ccda7aa15c8afd\")\n","\n","# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"baseline\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"id":"aggyfHIUR2zp","executionInfo":{"status":"ok","timestamp":1741604754179,"user_tz":-120,"elapsed":3994,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"e5b62dda-d797-4934-b35c-d64442875556"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250310_110552-e2to2kqk</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/paderborn-university/machine%20translation/runs/e2to2kqk' target=\"_blank\">baseline</a></strong> to <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/paderborn-university/machine%20translation' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/paderborn-university/machine%20translation/runs/e2to2kqk' target=\"_blank\">https://wandb.ai/paderborn-university/machine%20translation/runs/e2to2kqk</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/e2to2kqk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7fc2a4c73fd0>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/mafand/en-zu/merged.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/baseline \\\n","    --per_device_train_batch_size=8 \\\n","    --per_device_eval_batch_size=8 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvM-Zf4jUO81","outputId":"c97aea64-538c-484c-a064-bd6386223367","executionInfo":{"status":"ok","timestamp":1741607411646,"user_tz":-120,"elapsed":2125833,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-10 11:05:58.804755: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-10 11:05:58.827453: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-10 11:05:58.834235: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-10 11:05:58.850793: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-10 11:05:59.871321: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/10/2025 11:06:02 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/10/2025 11:06:02 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/baseline/runs/Mar10_11-06-02_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/baseline,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=8,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/baseline,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-fcf11d994dda434e\n","03/10/2025 11:06:02 - INFO - datasets.builder - Using custom data configuration default-fcf11d994dda434e\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/10/2025 11:06:02 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","03/10/2025 11:06:02 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 11:06:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/10/2025 11:06:02 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","03/10/2025 11:06:02 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:699] 2025-03-10 11:06:02,695 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 11:06:02,696 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-10 11:06:02,798 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 11:06:02,799 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 11:06:02,800 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 11:06:02,800 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 11:06:02,800 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 11:06:02,800 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 11:06:02,800 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 11:06:02,800 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 11:06:02,800 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-10 11:06:02,800 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 11:06:02,801 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-10 11:06:03,754 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-10 11:06:03,817 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-10 11:06:03,862 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4972] 2025-03-10 11:06:03,920 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-10 11:06:03,920 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-10 11:06:04,011 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-10 11:06:04,011 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-10 11:06:04,237 >> Safetensors PR exists\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-500813e1b41c1f29.arrow\n","03/10/2025 11:06:05 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-500813e1b41c1f29.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-24d83da570939502.arrow\n","03/10/2025 11:06:06 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-24d83da570939502.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c4510aa2c36da9e3.arrow\n","03/10/2025 11:06:08 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-fcf11d994dda434e/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c4510aa2c36da9e3.arrow\n","[INFO|trainer.py:2407] 2025-03-10 11:06:10,381 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-10 11:06:10,381 >>   Num examples = 5,737\n","[INFO|trainer.py:2409] 2025-03-10 11:06:10,381 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-10 11:06:10,381 >>   Instantaneous batch size per device = 8\n","[INFO|trainer.py:2412] 2025-03-10 11:06:10,382 >>   Training with DataParallel so batch size has been adjusted to: 16\n","[INFO|trainer.py:2413] 2025-03-10 11:06:10,382 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:2414] 2025-03-10 11:06:10,382 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-10 11:06:10,382 >>   Total optimization steps = 1,077\n","[INFO|trainer.py:2416] 2025-03-10 11:06:10,383 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-10 11:06:10,388 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_110610-zkjobc1e\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/baseline\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/zkjobc1e\u001b[0m\n","  0% 0/1077 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 3.3254, 'grad_norm': 5.521811008453369, 'learning_rate': 2.678737233054782e-05, 'epoch': 1.39}\n"," 46% 500/1077 [10:34<11:14,  1.17s/it][INFO|trainer.py:3944] 2025-03-10 11:16:46,007 >> Saving model checkpoint to M2M-100/baseline/checkpoint-500\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-10 11:16:46,019 >> Configuration saved in M2M-100/baseline/checkpoint-500/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 11:16:46,020 >> Configuration saved in M2M-100/baseline/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 11:16:50,966 >> Model weights saved in M2M-100/baseline/checkpoint-500/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 11:16:50,970 >> tokenizer config file saved in M2M-100/baseline/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 11:16:50,970 >> Special tokens file saved in M2M-100/baseline/checkpoint-500/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 11:16:50,971 >> added tokens file saved in M2M-100/baseline/checkpoint-500/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","{'loss': 2.3628, 'grad_norm': 5.180380344390869, 'learning_rate': 3.574744661095636e-06, 'epoch': 2.79}\n"," 93% 1000/1077 [21:30<01:40,  1.31s/it][INFO|trainer.py:3944] 2025-03-10 11:27:42,294 >> Saving model checkpoint to M2M-100/baseline/checkpoint-1000\n","[INFO|configuration_utils.py:423] 2025-03-10 11:27:42,296 >> Configuration saved in M2M-100/baseline/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 11:27:42,296 >> Configuration saved in M2M-100/baseline/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 11:27:47,301 >> Model weights saved in M2M-100/baseline/checkpoint-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 11:27:47,307 >> tokenizer config file saved in M2M-100/baseline/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 11:27:47,308 >> Special tokens file saved in M2M-100/baseline/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 11:27:47,308 >> added tokens file saved in M2M-100/baseline/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 1077/1077 [23:26<00:00,  1.11s/it][INFO|trainer.py:3944] 2025-03-10 11:29:38,409 >> Saving model checkpoint to M2M-100/baseline/checkpoint-1077\n","[INFO|configuration_utils.py:423] 2025-03-10 11:29:38,411 >> Configuration saved in M2M-100/baseline/checkpoint-1077/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 11:29:38,412 >> Configuration saved in M2M-100/baseline/checkpoint-1077/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 11:29:43,536 >> Model weights saved in M2M-100/baseline/checkpoint-1077/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 11:29:43,541 >> tokenizer config file saved in M2M-100/baseline/checkpoint-1077/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 11:29:43,541 >> Special tokens file saved in M2M-100/baseline/checkpoint-1077/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 11:29:43,542 >> added tokens file saved in M2M-100/baseline/checkpoint-1077/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-10 11:29:57,013 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 1426.6309, 'train_samples_per_second': 12.064, 'train_steps_per_second': 0.755, 'train_loss': 2.794963018475801, 'epoch': 3.0}\n","100% 1077/1077 [23:45<00:00,  1.32s/it]\n","[INFO|trainer.py:3944] 2025-03-10 11:29:57,026 >> Saving model checkpoint to M2M-100/baseline\n","[INFO|configuration_utils.py:423] 2025-03-10 11:29:57,027 >> Configuration saved in M2M-100/baseline/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 11:29:57,028 >> Configuration saved in M2M-100/baseline/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 11:30:06,015 >> Model weights saved in M2M-100/baseline/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 11:30:06,018 >> tokenizer config file saved in M2M-100/baseline/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 11:30:06,019 >> Special tokens file saved in M2M-100/baseline/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 11:30:06,019 >> added tokens file saved in M2M-100/baseline/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =  2920989GF\n","  train_loss               =      2.795\n","  train_runtime            = 0:23:46.63\n","  train_samples            =       5737\n","  train_samples_per_second =     12.064\n","  train_steps_per_second   =      0.755\n","03/10/2025 11:30:06 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-10 11:30:06,164 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-10 11:30:06,165 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-10 11:30:06,165 >>   Batch size = 16\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 63/63 [09:09<00:00,  8.72s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =      6.547\n","  eval_gen_len            =    47.5697\n","  eval_loss               =     2.6394\n","  eval_runtime            = 0:09:21.28\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.776\n","  eval_steps_per_second   =      0.112\n","03/10/2025 11:39:27 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-10 11:39:27,462 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-10 11:39:27,462 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-10 11:39:27,462 >>   Batch size = 16\n","100% 64/64 [10:08<00:00,  9.51s/it]\n","***** predict metrics *****\n","  predict_bleu               =     6.3611\n","  predict_gen_len            =     49.419\n","  predict_loss               =      2.682\n","  predict_runtime            = 0:10:22.84\n","  predict_samples            =       1012\n","  predict_samples_per_second =      1.625\n","  predict_steps_per_second   =      0.103\n","[INFO|modelcard.py:449] 2025-03-10 11:50:08,169 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 6.547}]}\n"]}]},{"cell_type":"code","source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet-1000\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Mp6AahGfufdK","executionInfo":{"status":"ok","timestamp":1741607411846,"user_tz":-120,"elapsed":208,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"85415bb1-b51d-417e-d0fe-d779ed5487c2"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/e2to2kqk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7fc2a4c73fd0>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_1000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-1000 \\\n","    --per_device_train_batch_size=8 \\\n","    --per_device_eval_batch_size=8 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"id":"hbNrB5boVIgU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741608839176,"user_tz":-120,"elapsed":1427533,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"777d39d1-0321-403e-87b9-03c993fa5a12"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-10 11:50:18.514541: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-10 11:50:18.540735: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-10 11:50:18.548234: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-10 11:50:18.567331: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-10 11:50:21.253884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/10/2025 11:50:26 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/10/2025 11:50:26 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-1000/runs/Mar10_11-50-26_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-1000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=8,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-1000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-d22542853ecd2369\n","03/10/2025 11:50:26 - INFO - datasets.builder - Using custom data configuration default-d22542853ecd2369\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/10/2025 11:50:26 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Generating dataset json (/root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/10/2025 11:50:26 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","03/10/2025 11:50:26 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","Downloading took 0.0 min\n","03/10/2025 11:50:26 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","03/10/2025 11:50:26 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Generating train split\n","03/10/2025 11:50:26 - INFO - datasets.builder - Generating train split\n","Generating train split: 1000 examples [00:00, 88265.83 examples/s]\n","Generating validation split\n","03/10/2025 11:50:26 - INFO - datasets.builder - Generating validation split\n","Generating validation split: 997 examples [00:00, 115830.73 examples/s]\n","Generating test split\n","03/10/2025 11:50:26 - INFO - datasets.builder - Generating test split\n","Generating test split: 1012 examples [00:00, 175906.99 examples/s]\n","Unable to verify splits sizes.\n","03/10/2025 11:50:26 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","03/10/2025 11:50:26 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","[INFO|configuration_utils.py:699] 2025-03-10 11:50:27,092 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 11:50:27,094 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-10 11:50:27,192 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 11:50:27,193 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 11:50:27,194 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 11:50:27,194 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 11:50:27,194 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 11:50:27,194 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 11:50:27,194 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 11:50:27,194 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 11:50:27,194 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-10 11:50:27,194 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 11:50:27,195 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-10 11:50:28,377 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-10 11:50:28,453 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-10 11:50:28,579 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|safetensors_conversion.py:61] 2025-03-10 11:50:28,581 >> Attempting to create safetensors variant\n","[INFO|modeling_utils.py:4980] 2025-03-10 11:50:28,581 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-03-10 11:50:28,674 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-10 11:50:28,674 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-10 11:50:29,049 >> Safetensors PR exists\n","Running tokenizer on train dataset:   0% 0/1000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e70b427f4acd1a90.arrow\n","03/10/2025 11:50:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e70b427f4acd1a90.arrow\n","Running tokenizer on train dataset: 100% 1000/1000 [00:00<00:00, 2570.51 examples/s]\n","Running tokenizer on validation dataset:   0% 0/997 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1e1e9cb1ba5e0af7.arrow\n","03/10/2025 11:50:32 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1e1e9cb1ba5e0af7.arrow\n","Running tokenizer on validation dataset: 100% 997/997 [00:00<00:00, 1491.85 examples/s]\n","Running tokenizer on prediction dataset:   0% 0/1012 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b37e93e0dc9f95ce.arrow\n","03/10/2025 11:50:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d22542853ecd2369/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b37e93e0dc9f95ce.arrow\n","Running tokenizer on prediction dataset: 100% 1012/1012 [00:00<00:00, 1477.64 examples/s]\n","[INFO|trainer.py:2407] 2025-03-10 11:50:37,691 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-10 11:50:37,691 >>   Num examples = 1,000\n","[INFO|trainer.py:2409] 2025-03-10 11:50:37,691 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-10 11:50:37,691 >>   Instantaneous batch size per device = 8\n","[INFO|trainer.py:2412] 2025-03-10 11:50:37,691 >>   Training with DataParallel so batch size has been adjusted to: 16\n","[INFO|trainer.py:2413] 2025-03-10 11:50:37,691 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:2414] 2025-03-10 11:50:37,691 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-10 11:50:37,691 >>   Total optimization steps = 189\n","[INFO|trainer.py:2416] 2025-03-10 11:50:37,693 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-10 11:50:37,700 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_115037-cwhsqinf\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-1000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/cwhsqinf\u001b[0m\n","  0% 0/189 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 189/189 [02:58<00:00,  1.13it/s][INFO|trainer.py:3944] 2025-03-10 11:53:37,386 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-1000/checkpoint-189\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-10 11:53:37,393 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-189/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 11:53:37,394 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-189/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 11:53:43,085 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-189/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 11:53:43,089 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-189/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 11:53:43,089 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-189/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 11:53:43,090 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/checkpoint-189/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-10 11:53:56,404 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 198.7119, 'train_samples_per_second': 15.097, 'train_steps_per_second': 0.951, 'train_loss': 3.857828453104332, 'epoch': 3.0}\n","100% 189/189 [03:17<00:00,  1.04s/it]\n","[INFO|trainer.py:3944] 2025-03-10 11:53:56,416 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-1000\n","[INFO|configuration_utils.py:423] 2025-03-10 11:53:56,418 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 11:53:56,419 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-1000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 11:54:05,374 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-1000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 11:54:05,377 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 11:54:05,377 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 11:54:05,378 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-1000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =   247758GF\n","  train_loss               =     3.8578\n","  train_runtime            = 0:03:18.71\n","  train_samples            =       1000\n","  train_samples_per_second =     15.097\n","  train_steps_per_second   =      0.951\n","03/10/2025 11:54:05 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-10 11:54:05,527 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-10 11:54:05,528 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-10 11:54:05,528 >>   Batch size = 16\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 63/63 [12:12<00:00, 11.62s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     2.3677\n","  eval_gen_len            =    54.2207\n","  eval_loss               =     3.9458\n","  eval_runtime            = 0:12:28.16\n","  eval_samples            =        997\n","  eval_samples_per_second =      1.333\n","  eval_steps_per_second   =      0.084\n","03/10/2025 12:06:33 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-10 12:06:33,702 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-10 12:06:33,702 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-10 12:06:33,702 >>   Batch size = 16\n"," 53% 34/64 [06:50<05:24, 10.80s/it]Traceback (most recent call last):\n","  File \"/content/transformers/examples/pytorch/translation/run_translation.py\", line 697, in <module>\n","    main()\n","  File \"/content/transformers/examples/pytorch/translation/run_translation.py\", line 646, in main\n","    predict_results = trainer.predict(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_seq2seq.py\", line 261, in predict\n","    return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 4185, in predict\n","    output = eval_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 4301, in evaluation_loop\n","    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_seq2seq.py\", line 333, in prediction_step\n","    generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2258, in generate\n","    result = self._beam_search(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 3559, in _beam_search\n","    model_kwargs[\"past_key_values\"] = self._temporary_reorder_cache(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 3320, in _temporary_reorder_cache\n","    past_key_values = self._reorder_cache(past_key_values, beam_idx)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/m2m_100/modeling_m2m_100.py\", line 1628, in _reorder_cache\n","    tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/m2m_100/modeling_m2m_100.py\", line 1628, in <genexpr>\n","    tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n","torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 15.06 MiB is free. Process 35317 has 14.73 GiB memory in use. Of the allocated memory 13.93 GiB is allocated by PyTorch, and 604.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"," 53% 34/64 [07:07<06:16, 12.56s/it]\n"]}]},{"cell_type":"code","source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet-2000\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"oAhoqvCbuk06","executionInfo":{"status":"ok","timestamp":1741608839374,"user_tz":-120,"elapsed":203,"user":{"displayName":"Emmanuel Kwame AYANFUL","userId":"02540403979605827324"}},"outputId":"3eca0e0a-f99e-459f-b0c1-0529c2a3c2df"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/paderborn-university/machine%20translation/runs/e2to2kqk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7fc2a4c73fd0>"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_2000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-2000 \\\n","    --per_device_train_batch_size=8 \\\n","    --per_device_eval_batch_size=8 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S0Vip1YNtpQn","outputId":"a1d2cc17-f7e2-4fdf-8d7b-f085a58f42ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-10 12:14:04.102165: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-03-10 12:14:04.127473: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-03-10 12:14:04.134773: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-10 12:14:04.153264: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-10 12:14:05.512016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","03/10/2025 12:14:08 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n","03/10/2025 12:14:08 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=2,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=M2M-100/africomet-qe-stl-1.1-2000/runs/Mar10_12-14-08_bcdd196dbaf2,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=M2M-100/africomet-qe-stl-1.1-2000,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=8,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=M2M-100/africomet-qe-stl-1.1-2000,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tp_size=0,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-63fa3ea70d279152\n","03/10/2025 12:14:08 - INFO - datasets.builder - Using custom data configuration default-63fa3ea70d279152\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","03/10/2025 12:14:08 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Generating dataset json (/root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","03/10/2025 12:14:09 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","03/10/2025 12:14:09 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n","Downloading took 0.0 min\n","03/10/2025 12:14:09 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","03/10/2025 12:14:09 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Generating train split\n","03/10/2025 12:14:09 - INFO - datasets.builder - Generating train split\n","Generating train split: 2000 examples [00:00, 221545.74 examples/s]\n","Generating validation split\n","03/10/2025 12:14:09 - INFO - datasets.builder - Generating validation split\n","Generating validation split: 997 examples [00:00, 121543.99 examples/s]\n","Generating test split\n","03/10/2025 12:14:09 - INFO - datasets.builder - Generating test split\n","Generating test split: 1012 examples [00:00, 184943.39 examples/s]\n","Unable to verify splits sizes.\n","03/10/2025 12:14:09 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","03/10/2025 12:14:09 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n","[INFO|configuration_utils.py:699] 2025-03-10 12:14:09,200 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 12:14:09,202 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|configuration_utils.py:699] 2025-03-10 12:14:09,297 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 12:14:09,298 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 12:14:09,298 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 12:14:09,298 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 12:14:09,299 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 12:14:09,299 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 12:14:09,299 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 12:14:09,299 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2050] 2025-03-10 12:14:09,299 >> loading file chat_template.jinja from cache at None\n","[INFO|configuration_utils.py:699] 2025-03-10 12:14:09,299 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n","[INFO|configuration_utils.py:771] 2025-03-10 12:14:09,300 >> Model config M2M100Config {\n","  \"_name_or_path\": \"facebook/m2m100_418M\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"relu\",\n","  \"architectures\": [\n","    \"M2M100ForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.05,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.05,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"m2m_100\",\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"transformers_version\": \"4.50.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128112\n","}\n","\n","[INFO|modeling_utils.py:3984] 2025-03-10 12:14:10,297 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n","[INFO|configuration_utils.py:1140] 2025-03-10 12:14:10,370 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|modeling_utils.py:4972] 2025-03-10 12:14:10,480 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4980] 2025-03-10 12:14:10,480 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n","[INFO|safetensors_conversion.py:61] 2025-03-10 12:14:10,488 >> Attempting to create safetensors variant\n","[INFO|configuration_utils.py:1095] 2025-03-10 12:14:10,576 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-10 12:14:10,577 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1\n","}\n","\n","[INFO|safetensors_conversion.py:74] 2025-03-10 12:14:10,924 >> Safetensors PR exists\n","Running tokenizer on train dataset:   0% 0/2000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-239a0c8168036379.arrow\n","03/10/2025 12:14:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-239a0c8168036379.arrow\n","Running tokenizer on train dataset: 100% 2000/2000 [00:00<00:00, 2853.77 examples/s]\n","Running tokenizer on validation dataset:   0% 0/997 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4e37c8bc3d2bbba6.arrow\n","03/10/2025 12:14:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4e37c8bc3d2bbba6.arrow\n","Running tokenizer on validation dataset: 100% 997/997 [00:00<00:00, 1403.66 examples/s]\n","Running tokenizer on prediction dataset:   0% 0/1012 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-283cd450680fa67d.arrow\n","03/10/2025 12:14:17 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-63fa3ea70d279152/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-283cd450680fa67d.arrow\n","Running tokenizer on prediction dataset: 100% 1012/1012 [00:00<00:00, 1395.64 examples/s]\n","[INFO|trainer.py:2407] 2025-03-10 12:14:20,178 >> ***** Running training *****\n","[INFO|trainer.py:2408] 2025-03-10 12:14:20,178 >>   Num examples = 2,000\n","[INFO|trainer.py:2409] 2025-03-10 12:14:20,178 >>   Num Epochs = 3\n","[INFO|trainer.py:2410] 2025-03-10 12:14:20,178 >>   Instantaneous batch size per device = 8\n","[INFO|trainer.py:2412] 2025-03-10 12:14:20,178 >>   Training with DataParallel so batch size has been adjusted to: 16\n","[INFO|trainer.py:2413] 2025-03-10 12:14:20,178 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:2414] 2025-03-10 12:14:20,179 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2415] 2025-03-10 12:14:20,179 >>   Total optimization steps = 375\n","[INFO|trainer.py:2416] 2025-03-10 12:14:20,180 >>   Number of trainable parameters = 483,905,536\n","[INFO|integration_utils.py:817] 2025-03-10 12:14:20,186 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250310_121420-ml7dhg5a\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mM2M-100/africomet-qe-stl-1.1-2000\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/paderborn-university/huggingface/runs/ml7dhg5a\u001b[0m\n","  0% 0/375 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 375/375 [05:49<00:00,  1.09it/s][INFO|trainer.py:3944] 2025-03-10 12:20:10,349 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-2000/checkpoint-375\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","[INFO|configuration_utils.py:423] 2025-03-10 12:20:10,356 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-375/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 12:20:10,357 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-375/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 12:20:15,548 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-375/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 12:20:15,551 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-375/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 12:20:15,552 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-375/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 12:20:15,552 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/checkpoint-375/added_tokens.json\n","[INFO|trainer.py:2659] 2025-03-10 12:20:29,030 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 368.8499, 'train_samples_per_second': 16.267, 'train_steps_per_second': 1.017, 'train_loss': 3.761904296875, 'epoch': 3.0}\n","100% 375/375 [06:07<00:00,  1.02it/s]\n","[INFO|trainer.py:3944] 2025-03-10 12:20:29,035 >> Saving model checkpoint to M2M-100/africomet-qe-stl-1.1-2000\n","[INFO|configuration_utils.py:423] 2025-03-10 12:20:29,037 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/config.json\n","[INFO|configuration_utils.py:909] 2025-03-10 12:20:29,038 >> Configuration saved in M2M-100/africomet-qe-stl-1.1-2000/generation_config.json\n","[INFO|modeling_utils.py:3040] 2025-03-10 12:20:37,973 >> Model weights saved in M2M-100/africomet-qe-stl-1.1-2000/model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-03-10 12:20:37,976 >> tokenizer config file saved in M2M-100/africomet-qe-stl-1.1-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-03-10 12:20:37,977 >> Special tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2562] 2025-03-10 12:20:37,977 >> added tokens file saved in M2M-100/africomet-qe-stl-1.1-2000/added_tokens.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  total_flos               =   489683GF\n","  train_loss               =     3.7619\n","  train_runtime            = 0:06:08.84\n","  train_samples            =       2000\n","  train_samples_per_second =     16.267\n","  train_steps_per_second   =      1.017\n","03/10/2025 12:20:38 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:4260] 2025-03-10 12:20:38,128 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4262] 2025-03-10 12:20:38,128 >>   Num examples = 997\n","[INFO|trainer.py:4265] 2025-03-10 12:20:38,129 >>   Batch size = 16\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n","100% 63/63 [12:02<00:00, 11.47s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_bleu               =     2.2533\n","  eval_gen_len            =     54.993\n","  eval_loss               =     3.7365\n","  eval_runtime            = 0:12:18.44\n","  eval_samples            =        997\n","  eval_samples_per_second =       1.35\n","  eval_steps_per_second   =      0.085\n","03/10/2025 12:32:56 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:4260] 2025-03-10 12:32:56,580 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4262] 2025-03-10 12:32:56,580 >>   Num examples = 1012\n","[INFO|trainer.py:4265] 2025-03-10 12:32:56,580 >>   Batch size = 16\n"," 89% 57/64 [12:00<01:31, 13.12s/it]"]}]},{"cell_type":"code","source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet-4000\")"],"metadata":{"id":"0v4YeYRgumqu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_4000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-4000 \\\n","    --per_device_train_batch_size=8 \\\n","    --per_device_eval_batch_size=8 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"id":"yGUYnc2wtrby"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet-8000\")"],"metadata":{"id":"hKmXZ3xLuoYh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_8000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-8000 \\\n","    --per_device_train_batch_size=8 \\\n","    --per_device_eval_batch_size=8 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"id":"TZSOQFxvttWv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize WandB\n","wandb.init(project=\"machine translation\", name=\"africomet-16000\")"],"metadata":{"id":"50vt5hMgup68"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0,1 python transformers/examples/pytorch/translation/run_translation.py \\\n","    --model_name_or_path facebook/m2m100_418M \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --source_lang en \\\n","    --target_lang zu \\\n","    --train_file /content/AIMS-NLP-Project/data/wmt22_african/africomet-qe-stl-1.1/en-zu/train_16000.json \\\n","    --validation_file /content/AIMS-NLP-Project/data/flores/en-zu/dev.json \\\n","    --test_file /content/AIMS-NLP-Project/data/flores/en-zu/devtest.json \\\n","    --num_beams 10 \\\n","    --output_dir M2M-100/africomet-qe-stl-1.1-16000 \\\n","    --per_device_train_batch_size=8 \\\n","    --per_device_eval_batch_size=8 \\\n","    --overwrite_output_dir \\\n","    --predict_with_generate"],"metadata":{"id":"mlfAMTxMtvOi"},"execution_count":null,"outputs":[]}]}