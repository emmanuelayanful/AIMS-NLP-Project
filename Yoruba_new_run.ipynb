{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb349528-a2f2-4735-a873-1b58cc618a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext jsonlines transformers datasets unbabel-comet wandb evaluate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d20ca3-4e41-4462-9b06-650546224f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r /home/emmanuelka/transformers/examples/pytorch/translation/requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7915eb6c-44ba-437d-b58e-d4c7647ce66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import runpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "441dfe6d-d970-4b2e-b8f3-eb017cadecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"AIMS-NLP-Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3374587-bf76-4723-8ae0-ebd6bf5fe394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 30 13:50:08 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   48C    P8               9W /  70W |      5MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4                       On  | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   77C    P0              52W /  70W |  11869MiB / 15360MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    1   N/A  N/A     53760      C   .../emmanuelka/jupyter-env/bin/python3    11864MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2769dd2-b4c3-4553-a0cb-cf15ef88dd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device count: 2\n",
      "Current device: 0\n",
      "Device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c942deac-67d3-47f2-b210-a58f0147b42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Processing allenai/nllb | eng_Latn-yor_Latn | top_k=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Logging into Hugging Face.\n",
      "Processing language pair: eng_Latn-yor_Latn\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Dataset for the eng_Latn-yor_Latn loaded!\n",
      "Fetching 4 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 30671.33it/s]\n",
      "Encoder model frozen.\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 72657.58 examples/s]\n",
      "Selecting top sentences with McGill-NLP/ssa-comet-mtl model after LID filtering.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:26<00:00,  4.76it/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 22093.31 examples/s]\n",
      "Processing completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Processing allenai/nllb | eng_Latn-yor_Latn | top_k=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging into Hugging Face.\n",
      "Processing language pair: eng_Latn-yor_Latn\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Dataset for the eng_Latn-yor_Latn loaded!\n",
      "Fetching 4 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 43919.41it/s]\n",
      "Encoder model frozen.\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:00<00:00, 102150.61 examples/s]\n",
      "Selecting top sentences with McGill-NLP/ssa-comet-mtl model after LID filtering.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:53<00:00,  4.70it/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 23759.86 examples/s]\n",
      "Processing completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Processing allenai/nllb | eng_Latn-yor_Latn | top_k=4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging into Hugging Face.\n",
      "Processing language pair: eng_Latn-yor_Latn\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Dataset for the eng_Latn-yor_Latn loaded!\n",
      "Fetching 4 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 34169.48it/s]\n",
      "Encoder model frozen.\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:00<00:00, 102054.61 examples/s]\n",
      "Selecting top sentences with McGill-NLP/ssa-comet-mtl model after LID filtering.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:59<00:00,  4.31it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 244/244 [00:56<00:00,  4.35it/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:00<00:00, 23829.44 examples/s]\n",
      "Processing completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Processing allenai/nllb | eng_Latn-yor_Latn | top_k=8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging into Hugging Face.\n",
      "Processing language pair: eng_Latn-yor_Latn\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Dataset for the eng_Latn-yor_Latn loaded!\n",
      "Fetching 4 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 38479.85it/s]\n",
      "Encoder model frozen.\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 101252.38 examples/s]\n",
      "Selecting top sentences with McGill-NLP/ssa-comet-mtl model after LID filtering.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:00<00:00,  4.25it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:59<00:00,  4.34it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:58<00:00,  4.35it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 232/232 [00:54<00:00,  4.28it/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:00<00:00, 23583.68 examples/s]\n",
      "Processing completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Processing allenai/nllb | eng_Latn-yor_Latn | top_k=16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging into Hugging Face.\n",
      "Processing language pair: eng_Latn-yor_Latn\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Dataset for the eng_Latn-yor_Latn loaded!\n",
      "Fetching 4 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 35246.25it/s]\n",
      "Encoder model frozen.\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32000/32000 [00:00<00:00, 116979.41 examples/s]\n",
      "Selecting top sentences with McGill-NLP/ssa-comet-mtl model after LID filtering.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:00<00:00,  4.26it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:59<00:00,  4.31it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:57<00:00,  4.42it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:59<00:00,  4.32it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:59<00:00,  4.34it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:59<00:00,  4.34it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 208/208 [00:47<00:00,  4.34it/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 24218.57 examples/s]\n",
      "Processing completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Processing allenai/nllb | eng_Latn-yor_Latn | top_k=32000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging into Hugging Face.\n",
      "Processing language pair: eng_Latn-yor_Latn\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Dataset for the eng_Latn-yor_Latn loaded!\n",
      "Fetching 4 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 36711.63it/s]\n",
      "Encoder model frozen.\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64000/64000 [00:00<00:00, 117071.60 examples/s]\n",
      "Selecting top sentences with McGill-NLP/ssa-comet-mtl model after LID filtering.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:00<00:00,  4.23it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:59<00:00,  4.32it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:58<00:00,  4.34it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:58<00:00,  4.37it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:57<00:00,  4.44it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:57<00:00,  4.47it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:58<00:00,  4.37it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:58<00:00,  4.37it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:57<00:00,  4.42it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:58<00:00,  4.36it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:58<00:00,  4.38it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:58<00:00,  4.37it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:59<00:00,  4.32it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:58<00:00,  4.39it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:58<00:00,  4.38it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 160/160 [00:35<00:00,  4.45it/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32000/32000 [00:01<00:00, 23777.98 examples/s]\n",
      "Processing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Language pairs and dataset sources (with dataset-specific codes)\n",
    "language_pairs = {\n",
    "    \"allenai/nllb\": [\n",
    "        # (\"ewe_Latn\", \"fra_Latn\", \"Ewe\", \"French\"),\n",
    "        # (\"eng_Latn\", \"twi_Latn\", \"English\", \"Twi\"),\n",
    "        # (\"eng_Latn\", \"hau_Latn\", \"English\", \"Hausa\"),\n",
    "        # (\"eng_Latn\", \"ibo_Latn\", \"English\", \"Igbo\"),\n",
    "        (\"eng_Latn\", \"yor_Latn\", \"English\", \"Yoruba\"),\n",
    "        # (\"eng_Latn\", \"zul_Latn\", \"English\", \"Zulu\")\n",
    "    # ],\n",
    "    # \"facebook/flores\": [\n",
    "    #     # (\"fra_Latn\", \"ewe_Latn\", \"French\", \"Ewe\"),\n",
    "    #     # (\"eng_Latn\", \"twi_Latn\", \"English\", \"Twi\"),\n",
    "    #     # (\"eng_Latn\", \"hau_Latn\", \"English\", \"Hausa\"),\n",
    "    #     # (\"eng_Latn\", \"ibo_Latn\", \"English\", \"Igbo\"),\n",
    "    #     # (\"eng_Latn\", \"yor_Latn\", \"English\", \"Yoruba\"),\n",
    "    #     (\"eng_Latn\", \"zul_Latn\", \"English\", \"Zulu\")\n",
    "    # ],\n",
    "    # \"masakhane/mafand\": [\n",
    "    #     # (\"fr\", \"ewe\", \"French\", \"Ewe\"),\n",
    "    #     # (\"en\", \"twi\", \"English\", \"Twi\"),\n",
    "    #     # (\"en\", \"hau\", \"English\", \"Hausa\"),\n",
    "    #     # (\"en\", \"ibo\", \"English\", \"Igbo\"),\n",
    "    #     # (\"en\", \"yor\", \"English\", \"Yoruba\"),\n",
    "    #     (\"en\", \"zul\", \"English\", \"Zulu\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "top_k_values = [1000 * 2 ** i for i in range(6)]\n",
    "HF_TOKEN = \"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\"\n",
    "DEVICE = \"cuda\"\n",
    "COMET_MODEL = \"McGill-NLP/ssa-comet-mtl\"\n",
    "#COMET_MODEL = \"masakhane/africomet-qe-stl-1.1\"\n",
    "#COMET_MODEL = \"None\"\n",
    "\n",
    "base_path = Path(\"data\")\n",
    "\n",
    "# Download and preprocess data\n",
    "for dataset, pairs in language_pairs.items():\n",
    "    for src, tgt, src_name, tgt_name in pairs:\n",
    "        if dataset.startswith(\"allenai/nllb\"):\n",
    "            for top_k in top_k_values:\n",
    "                print(f\"\\n>>> Processing {dataset} | {src}-{tgt} | top_k={top_k}\")\n",
    "                sys.argv = [\n",
    "                    \"dataPreprocess.py\",\n",
    "                    \"--output_dir\", str(base_path),\n",
    "                    \"--dataset_name\", dataset,\n",
    "                    \"--source_lang\", src,\n",
    "                    \"--source_lang_name\", src_name,\n",
    "                    \"--target_lang\", tgt,\n",
    "                    \"--target_lang_name\", tgt_name,\n",
    "                    \"--hf_token\", HF_TOKEN,\n",
    "                    \"--batch_size\", \"4096\",\n",
    "                    \"--top_k_train\", str(top_k),\n",
    "                    \"--device\", DEVICE,\n",
    "                    \"--comet_model\", COMET_MODEL\n",
    "                ]\n",
    "                runpy.run_path(\"/home/emmanuelka/AIMS-NLP-Project/dataPreprocess.py\", run_name=\"__main__\")\n",
    "\n",
    "        else:\n",
    "            print(f\"\\n>>> Processing {dataset} | {src}-{tgt}\")\n",
    "            sys.argv = [\n",
    "                \"dataPreprocess.py\",\n",
    "                \"--output_dir\", str(base_path),\n",
    "                \"--dataset_name\", dataset,\n",
    "                \"--source_lang\", src,\n",
    "                \"--source_lang_name\", src_name,\n",
    "                \"--target_lang\", tgt,\n",
    "                \"--target_lang_name\", tgt_name,\n",
    "                \"--hf_token\", HF_TOKEN,\n",
    "                \"--batch_size\", \"4096\",\n",
    "                \"--device\", DEVICE,\n",
    "                \"--comet_model\", COMET_MODEL\n",
    "            ]\n",
    "\n",
    "            runpy.run_path(\"/home/emmanuelka/AIMS-NLP-Project/dataPreprocess.py\", run_name=\"__main__\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6428e8ff-eef3-4cf0-a95c-0fb842061d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 2000, 4000, 8000, 16000, 32000]\n",
      "data/nllb/ssa-comet-mtl/en-yo/trainer_1000.json\n",
      "\n",
      ">>> Merging data/nllb/ssa-comet-mtl/en-yo/train_1000.json with baseline data/mafand/en-yo/merged.json into data/nllb/ssa-comet-mtl/en-yo/trainer_1000.json\n",
      "data/nllb/ssa-comet-mtl/en-yo/trainer_2000.json\n",
      "\n",
      ">>> Merging data/nllb/ssa-comet-mtl/en-yo/train_2000.json with baseline data/mafand/en-yo/merged.json into data/nllb/ssa-comet-mtl/en-yo/trainer_2000.json\n",
      "data/nllb/ssa-comet-mtl/en-yo/trainer_4000.json\n",
      "\n",
      ">>> Merging data/nllb/ssa-comet-mtl/en-yo/train_4000.json with baseline data/mafand/en-yo/merged.json into data/nllb/ssa-comet-mtl/en-yo/trainer_4000.json\n",
      "data/nllb/ssa-comet-mtl/en-yo/trainer_8000.json\n",
      "\n",
      ">>> Merging data/nllb/ssa-comet-mtl/en-yo/train_8000.json with baseline data/mafand/en-yo/merged.json into data/nllb/ssa-comet-mtl/en-yo/trainer_8000.json\n",
      "data/nllb/ssa-comet-mtl/en-yo/trainer_16000.json\n",
      "\n",
      ">>> Merging data/nllb/ssa-comet-mtl/en-yo/train_16000.json with baseline data/mafand/en-yo/merged.json into data/nllb/ssa-comet-mtl/en-yo/trainer_16000.json\n",
      "data/nllb/ssa-comet-mtl/en-yo/trainer_32000.json\n",
      "\n",
      ">>> Merging data/nllb/ssa-comet-mtl/en-yo/train_32000.json with baseline data/mafand/en-yo/merged.json into data/nllb/ssa-comet-mtl/en-yo/trainer_32000.json\n"
     ]
    }
   ],
   "source": [
    "from dataPreprocess import merge_jsonlines\n",
    "\n",
    "# Merge with baseline mafand data\n",
    "top_k_values = [1000 * 2 ** i for i in range(6)]\n",
    "print(top_k_values)\n",
    "for lang in [\"en-yo\"]:\n",
    "    output_path = base_path / \"nllb\" / \"ssa-comet-mtl\" / f\"{lang}\"\n",
    "    baseline_file = base_path / \"mafand\" / f\"{lang}\" / \"merged.json\"\n",
    "    input_files = [output_path / f\"train_{top_k}.json\" for top_k in top_k_values]\n",
    "\n",
    "    for input_file in input_files:\n",
    "        trainer_file = output_path / f\"trainer_{input_file.stem.split('_')[-1]}.json\"\n",
    "        print(trainer_file)\n",
    "        print(f\"\\n>>> Merging {input_file} with baseline {baseline_file} into {trainer_file}\")\n",
    "        merge_jsonlines([str(baseline_file), str(input_file)], str(trainer_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb33b9e9-64b9-49a5-bb0f-28f7feed31d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Processing allenai/nllb | eng_Latn-yor_Latn | top_k=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Logging into Hugging Face.\n",
      "Processing language pair: eng_Latn-yor_Latn\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Dataset for the eng_Latn-yor_Latn loaded!\n",
      "No COMET-QE model provided. Using random sampling with LID filtering.\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 75312.50 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 11304.09 examples/s]\n",
      "Processing completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Processing allenai/nllb | eng_Latn-yor_Latn | top_k=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging into Hugging Face.\n",
      "Processing language pair: eng_Latn-yor_Latn\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Dataset for the eng_Latn-yor_Latn loaded!\n",
      "No COMET-QE model provided. Using random sampling with LID filtering.\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 76560.75 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 11766.60 examples/s]\n",
      "Processing completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Processing allenai/nllb | eng_Latn-yor_Latn | top_k=4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging into Hugging Face.\n",
      "Processing language pair: eng_Latn-yor_Latn\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Dataset for the eng_Latn-yor_Latn loaded!\n",
      "No COMET-QE model provided. Using random sampling with LID filtering.\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:00<00:00, 99808.54 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:00<00:00, 11736.84 examples/s]\n",
      "Processing completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Processing allenai/nllb | eng_Latn-yor_Latn | top_k=8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging into Hugging Face.\n",
      "Processing language pair: eng_Latn-yor_Latn\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Dataset for the eng_Latn-yor_Latn loaded!\n",
      "No COMET-QE model provided. Using random sampling with LID filtering.\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:00<00:00, 106773.18 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:00<00:00, 8561.20 examples/s]\n",
      "Processing completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Processing allenai/nllb | eng_Latn-yor_Latn | top_k=16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging into Hugging Face.\n",
      "Processing language pair: eng_Latn-yor_Latn\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Dataset for the eng_Latn-yor_Latn loaded!\n",
      "No COMET-QE model provided. Using random sampling with LID filtering.\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 41088.25 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16000/16000 [00:01<00:00, 10074.94 examples/s]\n",
      "Processing completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Processing allenai/nllb | eng_Latn-yor_Latn | top_k=32000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging into Hugging Face.\n",
      "Processing language pair: eng_Latn-yor_Latn\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Dataset for the eng_Latn-yor_Latn loaded!\n",
      "No COMET-QE model provided. Using random sampling with LID filtering.\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32000/32000 [00:00<00:00, 114890.31 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32000/32000 [00:03<00:00, 9804.55 examples/s]\n",
      "Processing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Language pairs and dataset sources (with dataset-specific codes)\n",
    "language_pairs = {\n",
    "    \"allenai/nllb\": [\n",
    "        # (\"ewe_Latn\", \"fra_Latn\", \"Ewe\", \"French\"),\n",
    "        # (\"eng_Latn\", \"twi_Latn\", \"English\", \"Twi\"),\n",
    "        # (\"eng_Latn\", \"hau_Latn\", \"English\", \"Hausa\"),\n",
    "        # (\"eng_Latn\", \"ibo_Latn\", \"English\", \"Igbo\"),\n",
    "        (\"eng_Latn\", \"yor_Latn\", \"English\", \"Yoruba\"),\n",
    "        # (\"eng_Latn\", \"zul_Latn\", \"English\", \"Zulu\")\n",
    "    # ],\n",
    "    # \"facebook/flores\": [\n",
    "    #     # (\"fra_Latn\", \"ewe_Latn\", \"French\", \"Ewe\"),\n",
    "    #     # (\"eng_Latn\", \"twi_Latn\", \"English\", \"Twi\"),\n",
    "    #     # (\"eng_Latn\", \"hau_Latn\", \"English\", \"Hausa\"),\n",
    "    #     # (\"eng_Latn\", \"ibo_Latn\", \"English\", \"Igbo\"),\n",
    "    #     # (\"eng_Latn\", \"yor_Latn\", \"English\", \"Yoruba\"),\n",
    "    #     (\"eng_Latn\", \"zul_Latn\", \"English\", \"Zulu\")\n",
    "    # ],\n",
    "    # \"masakhane/mafand\": [\n",
    "    #     # (\"fr\", \"ewe\", \"French\", \"Ewe\"),\n",
    "    #     # (\"en\", \"twi\", \"English\", \"Twi\"),\n",
    "    #     # (\"en\", \"hau\", \"English\", \"Hausa\"),\n",
    "    #     # (\"en\", \"ibo\", \"English\", \"Igbo\"),\n",
    "    #     # (\"en\", \"yor\", \"English\", \"Yoruba\"),\n",
    "    #     (\"en\", \"zul\", \"English\", \"Zulu\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "top_k_values = [1000 * 2 ** i for i in range(6)]\n",
    "HF_TOKEN = \"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\"\n",
    "DEVICE = \"cuda\"\n",
    "# COMET_MODEL = \"McGill-NLP/ssa-comet-mtl\"\n",
    "# COMET_MODEL = \"masakhane/africomet-qe-stl-1.1\"\n",
    "COMET_MODEL = \"None\"\n",
    "\n",
    "base_path = Path(\"data\")\n",
    "\n",
    "# Download and preprocess data\n",
    "for dataset, pairs in language_pairs.items():\n",
    "    for src, tgt, src_name, tgt_name in pairs:\n",
    "        if dataset.startswith(\"allenai/nllb\"):\n",
    "            for top_k in top_k_values:\n",
    "                print(f\"\\n>>> Processing {dataset} | {src}-{tgt} | top_k={top_k}\")\n",
    "                sys.argv = [\n",
    "                    \"dataPreprocess.py\",\n",
    "                    \"--output_dir\", str(base_path),\n",
    "                    \"--dataset_name\", dataset,\n",
    "                    \"--source_lang\", src,\n",
    "                    \"--source_lang_name\", src_name,\n",
    "                    \"--target_lang\", tgt,\n",
    "                    \"--target_lang_name\", tgt_name,\n",
    "                    \"--hf_token\", HF_TOKEN,\n",
    "                    \"--batch_size\", \"4096\",\n",
    "                    \"--top_k_train\", str(top_k),\n",
    "                    \"--device\", DEVICE,\n",
    "                    \"--comet_model\", COMET_MODEL\n",
    "                ]\n",
    "                runpy.run_path(\"/home/emmanuelka/AIMS-NLP-Project/dataPreprocess.py\", run_name=\"__main__\")\n",
    "\n",
    "        else:\n",
    "            print(f\"\\n>>> Processing {dataset} | {src}-{tgt}\")\n",
    "            sys.argv = [\n",
    "                \"dataPreprocess.py\",\n",
    "                \"--output_dir\", str(base_path),\n",
    "                \"--dataset_name\", dataset,\n",
    "                \"--source_lang\", src,\n",
    "                \"--source_lang_name\", src_name,\n",
    "                \"--target_lang\", tgt,\n",
    "                \"--target_lang_name\", tgt_name,\n",
    "                \"--hf_token\", HF_TOKEN,\n",
    "                \"--batch_size\", \"4096\",\n",
    "                \"--device\", DEVICE,\n",
    "                \"--comet_model\", COMET_MODEL\n",
    "            ]\n",
    "\n",
    "            runpy.run_path(\"/home/emmanuelka/AIMS-NLP-Project/dataPreprocess.py\", run_name=\"__main__\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d4db0-2514-4d95-8776-de50007ede09",
   "metadata": {},
   "source": [
    "### Africomet with new comet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a61437-0d41-4766-b523-a983207f782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00b2a17c-2c0a-4742-bc63-d27c9d0df64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/emmanuelka/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log in with your API key\n",
    "wandb.login(key=\"7f13d9fe09d0856f7c12099a27ccda7aa15c8afd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eeaa0ce-7c77-4adb-bafa-785742d1a634",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0529 21:41:26.547000 18426 torch/distributed/run.py:766] \n",
      "W0529 21:41:26.547000 18426 torch/distributed/run.py:766] *****************************************\n",
      "W0529 21:41:26.547000 18426 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0529 21:41:26.547000 18426 torch/distributed/run.py:766] *****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29/2025 21:41:31 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "05/29/2025 21:41:31 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa/runs/May29_21-41-31_yoruba-vm,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "05/29/2025 21:41:31 - INFO - datasets.builder - Using custom data configuration default-82ae416fa505f913\n",
      "05/29/2025 21:41:31 - INFO - datasets.info - Loading Dataset Infos from /home/emmanuelka/jupyter-env/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "05/29/2025 21:41:31 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "05/29/2025 21:41:31 - INFO - datasets.info - Loading Dataset info from /home/emmanuelka/.cache/huggingface/datasets/json/default-82ae416fa505f913/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
      "05/29/2025 21:41:31 - INFO - datasets.builder - Found cached dataset json (/home/emmanuelka/.cache/huggingface/datasets/json/default-82ae416fa505f913/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "05/29/2025 21:41:31 - INFO - datasets.info - Loading Dataset info from /home/emmanuelka/.cache/huggingface/datasets/json/default-82ae416fa505f913/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
      "05/29/2025 21:41:31 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-82ae416fa505f913\n",
      "Loading Dataset Infos from /home/emmanuelka/jupyter-env/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/emmanuelka/.cache/huggingface/datasets/json/default-82ae416fa505f913/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
      "Found cached dataset json (/home/emmanuelka/.cache/huggingface/datasets/json/default-82ae416fa505f913/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "Loading Dataset info from /home/emmanuelka/.cache/huggingface/datasets/json/default-82ae416fa505f913/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
      "[INFO|configuration_utils.py:712] 2025-05-29 21:41:31,933 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-29 21:41:31,936 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:712] 2025-05-29 21:41:31,969 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-29 21:41:31,971 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 21:41:32,005 >> loading file vocab.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 21:41:32,005 >> loading file sentencepiece.bpe.model from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 21:41:32,005 >> loading file tokenizer_config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 21:41:32,005 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 21:41:32,005 >> loading file special_tokens_map.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 21:41:32,005 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 21:41:32,005 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:712] 2025-05-29 21:41:32,005 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-29 21:41:32,006 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1270] 2025-05-29 21:41:32,860 >> loading weights file pytorch_model.bin from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:1122] 2025-05-29 21:41:32,862 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "[INFO|safetensors_conversion.py:61] 2025-05-29 21:41:32,913 >> Attempting to create safetensors variant\n",
      "[INFO|safetensors_conversion.py:74] 2025-05-29 21:41:33,130 >> Safetensors PR exists\n",
      "[INFO|modeling_utils.py:5233] 2025-05-29 21:41:34,358 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:5241] 2025-05-29 21:41:34,358 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:1077] 2025-05-29 21:41:34,391 >> loading configuration file generation_config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
      "[INFO|configuration_utils.py:1122] 2025-05-29 21:41:34,392 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "[rank1]:[W529 21:41:34.133075063 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-82ae416fa505f913/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-535d4a298ddc681d.arrow\n",
      "[rank0]:[W529 21:41:35.056267422 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29/2025 21:41:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-82ae416fa505f913/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-535d4a298ddc681d.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-82ae416fa505f913/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-19431431c4f6f9c2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29/2025 21:41:37 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-82ae416fa505f913/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-19431431c4f6f9c2.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-82ae416fa505f913/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e7322d79555995ac.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29/2025 21:41:37 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-82ae416fa505f913/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e7322d79555995ac.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:756] 2025-05-29 21:41:40,144 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2409] 2025-05-29 21:41:41,431 >> ***** Running training *****\n",
      "[INFO|trainer.py:2410] 2025-05-29 21:41:41,431 >>   Num examples = 1,000\n",
      "[INFO|trainer.py:2411] 2025-05-29 21:41:41,432 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2412] 2025-05-29 21:41:41,432 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2415] 2025-05-29 21:41:41,432 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2416] 2025-05-29 21:41:41,432 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2417] 2025-05-29 21:41:41,432 >>   Total optimization steps = 375\n",
      "[INFO|trainer.py:2418] 2025-05-29 21:41:41,433 >>   Number of trainable parameters = 483,905,536\n",
      "[INFO|integration_utils.py:832] 2025-05-29 21:41:41,555 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: emmanuelka (paderborn-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.11\n",
      "wandb: Run data is saved locally in /home/emmanuelka/AIMS-NLP-Project/wandb/run-20250529_214141-zn1h6g57\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run /home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa\n",
      "wandb: ⭐️ View project at https://wandb.ai/paderborn-university/huggingface\n",
      "wandb: 🚀 View run at https://wandb.ai/paderborn-university/huggingface/runs/zn1h6g57\n",
      "  0%|          | 0/375 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-05-29 21:41:42,683 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "[rank1]:[W529 21:41:42.218203623 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[WARNING|logging.py:328] 2025-05-29 21:41:42,821 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "[rank0]:[W529 21:41:42.377473418 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "100%|██████████| 375/375 [04:44<00:00,  1.32it/s][INFO|trainer.py:3993] 2025-05-29 21:46:27,124 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa/checkpoint-375\n",
      "/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/transformers-4.53.0.dev0-py3.11.egg/transformers/modeling_utils.py:3585: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:438] 2025-05-29 21:46:27,130 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa/checkpoint-375/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-29 21:46:27,130 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa/checkpoint-375/generation_config.json\n",
      "[INFO|modeling_utils.py:3845] 2025-05-29 21:46:30,680 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa/checkpoint-375/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-29 21:46:30,683 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa/checkpoint-375/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-29 21:46:30,684 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa/checkpoint-375/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-29 21:46:30,685 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa/checkpoint-375/added_tokens.json\n",
      "[INFO|trainer.py:2676] 2025-05-29 21:46:52,048 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 375/375 [05:09<00:00,  1.21it/s]\n",
      "[INFO|trainer.py:3993] 2025-05-29 21:46:52,052 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa\n",
      "[INFO|configuration_utils.py:438] 2025-05-29 21:46:52,054 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-29 21:46:52,055 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 310.6156, 'train_samples_per_second': 9.658, 'train_steps_per_second': 1.207, 'train_loss': 3.333336263020833, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-29 21:47:04,403 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-29 21:47:04,407 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-29 21:47:04,408 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-29 21:47:04,408 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_1000_ssa/added_tokens.json\n",
      "[INFO|trainer.py:4327] 2025-05-29 21:47:04,588 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4329] 2025-05-29 21:47:04,588 >>   Num examples = 997\n",
      "[INFO|trainer.py:4332] 2025-05-29 21:47:04,589 >>   Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               =   202836GF\n",
      "  train_loss               =     3.3333\n",
      "  train_runtime            = 0:05:10.61\n",
      "  train_samples            =       1000\n",
      "  train_samples_per_second =      9.658\n",
      "  train_steps_per_second   =      1.207\n",
      "05/29/2025 21:47:04 - INFO - __main__ - *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [06:09<00:00,  2.96s/it]\n",
      "[INFO|trainer.py:4327] 2025-05-29 21:53:17,460 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:4329] 2025-05-29 21:53:17,460 >>   Num examples = 1012\n",
      "[INFO|trainer.py:4332] 2025-05-29 21:53:17,460 >>   Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_bleu               =     2.1008\n",
      "  eval_chrf               =    14.8177\n",
      "  eval_gen_len            =     62.883\n",
      "  eval_loss               =     4.3764\n",
      "  eval_runtime            = 0:06:12.86\n",
      "  eval_samples            =        997\n",
      "  eval_samples_per_second =      2.674\n",
      "  eval_steps_per_second   =      0.335\n",
      "05/29/2025 21:53:17 - INFO - __main__ - *** Predict ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [06:21<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** predict metrics *****\n",
      "  predict_bleu               =     1.9361\n",
      "  predict_chrf               =    14.0593\n",
      "  predict_gen_len            =    64.8425\n",
      "  predict_loss               =     4.3616\n",
      "  predict_runtime            = 0:06:24.83\n",
      "  predict_samples            =       1012\n",
      "  predict_samples_per_second =       2.63\n",
      "  predict_steps_per_second   =       0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modelcard.py:450] 2025-05-29 21:59:54,340 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 2.1008}]}\n",
      "[rank0]:[W529 21:59:54.258837026 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "W0529 21:59:57.971000 20573 torch/distributed/run.py:766] \n",
      "W0529 21:59:57.971000 20573 torch/distributed/run.py:766] *****************************************\n",
      "W0529 21:59:57.971000 20573 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0529 21:59:57.971000 20573 torch/distributed/run.py:766] *****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29/2025 22:00:03 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "05/29/2025 22:00:03 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/runs/May29_22-00-02_yoruba-vm,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "05/29/2025 22:00:03 - INFO - datasets.builder - Using custom data configuration default-5d07e8f41906549d\n",
      "05/29/2025 22:00:03 - INFO - datasets.info - Loading Dataset Infos from /home/emmanuelka/jupyter-env/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "05/29/2025 22:00:03 - INFO - datasets.builder - Generating dataset json (/home/emmanuelka/.cache/huggingface/datasets/json/default-5d07e8f41906549d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "05/29/2025 22:00:03 - INFO - datasets.builder - Downloading and preparing dataset json/default to /home/emmanuelka/.cache/huggingface/datasets/json/default-5d07e8f41906549d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
      "05/29/2025 22:00:03 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "05/29/2025 22:00:03 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "05/29/2025 22:00:03 - INFO - datasets.builder - Generating train split\n",
      "05/29/2025 22:00:03 - INFO - datasets.builder - Generating validation split\n",
      "05/29/2025 22:00:03 - INFO - datasets.builder - Generating test split\n",
      "05/29/2025 22:00:03 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "05/29/2025 22:00:03 - INFO - datasets.builder - Dataset json downloaded and prepared to /home/emmanuelka/.cache/huggingface/datasets/json/default-5d07e8f41906549d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
      "05/29/2025 22:00:03 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5d07e8f41906549d\n",
      "Loading Dataset Infos from /home/emmanuelka/jupyter-env/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "Generating dataset json (/home/emmanuelka/.cache/huggingface/datasets/json/default-5d07e8f41906549d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "Downloading and preparing dataset json/default to /home/emmanuelka/.cache/huggingface/datasets/json/default-5d07e8f41906549d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "Generating train split: 2000 examples [00:00, 350138.07 examples/s]\n",
      "Generating validation split\n",
      "Generating validation split: 997 examples [00:00, 144736.30 examples/s]\n",
      "Generating test split\n",
      "Generating test split: 1012 examples [00:00, 212859.72 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /home/emmanuelka/.cache/huggingface/datasets/json/default-5d07e8f41906549d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
      "[INFO|configuration_utils.py:712] 2025-05-29 22:00:03,255 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-29 22:00:03,258 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:712] 2025-05-29 22:00:03,311 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-29 22:00:03,312 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 22:00:03,362 >> loading file vocab.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 22:00:03,362 >> loading file sentencepiece.bpe.model from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 22:00:03,362 >> loading file tokenizer_config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 22:00:03,362 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 22:00:03,362 >> loading file special_tokens_map.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 22:00:03,362 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 22:00:03,362 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:712] 2025-05-29 22:00:03,362 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-29 22:00:03,364 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1270] 2025-05-29 22:00:04,236 >> loading weights file pytorch_model.bin from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:1122] 2025-05-29 22:00:04,238 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "[INFO|safetensors_conversion.py:61] 2025-05-29 22:00:04,291 >> Attempting to create safetensors variant\n",
      "[INFO|safetensors_conversion.py:74] 2025-05-29 22:00:04,526 >> Safetensors PR exists\n",
      "[INFO|modeling_utils.py:5233] 2025-05-29 22:00:05,749 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:5241] 2025-05-29 22:00:05,750 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:1077] 2025-05-29 22:00:05,789 >> loading configuration file generation_config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
      "[INFO|configuration_utils.py:1122] 2025-05-29 22:00:05,789 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "[rank1]:[W529 22:00:06.603679277 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Running tokenizer on train dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-5d07e8f41906549d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-68d0d38e07313780.arrow\n",
      "Running tokenizer on train dataset:  50%|█████     | 1000/2000 [00:00<00:00, 3456.28 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29/2025 22:00:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-5d07e8f41906549d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-68d0d38e07313780.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset: 100%|██████████| 2000/2000 [00:00<00:00, 3543.08 examples/s]\n",
      "[rank0]:[W529 22:00:07.997794104 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Running tokenizer on validation dataset:   0%|          | 0/997 [00:00<?, ? examples/s]Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-5d07e8f41906549d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0854816b92813d1d.arrow\n",
      "Running tokenizer on validation dataset: 100%|██████████| 997/997 [00:00<00:00, 1978.59 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29/2025 22:00:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-5d07e8f41906549d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0854816b92813d1d.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on prediction dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-5d07e8f41906549d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-81948814007ebaf5.arrow\n",
      "Running tokenizer on prediction dataset: 100%|██████████| 1012/1012 [00:00<00:00, 1898.78 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29/2025 22:00:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-5d07e8f41906549d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-81948814007ebaf5.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:756] 2025-05-29 22:00:13,045 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2409] 2025-05-29 22:00:14,384 >> ***** Running training *****\n",
      "[INFO|trainer.py:2410] 2025-05-29 22:00:14,384 >>   Num examples = 2,000\n",
      "[INFO|trainer.py:2411] 2025-05-29 22:00:14,385 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2412] 2025-05-29 22:00:14,385 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2415] 2025-05-29 22:00:14,385 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2416] 2025-05-29 22:00:14,385 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2417] 2025-05-29 22:00:14,385 >>   Total optimization steps = 750\n",
      "[INFO|trainer.py:2418] 2025-05-29 22:00:14,386 >>   Number of trainable parameters = 483,905,536\n",
      "[INFO|integration_utils.py:832] 2025-05-29 22:00:14,505 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: emmanuelka (paderborn-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.11\n",
      "wandb: Run data is saved locally in /home/emmanuelka/AIMS-NLP-Project/wandb/run-20250529_220014-xgk1o6rt\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa\n",
      "wandb: ⭐️ View project at https://wandb.ai/paderborn-university/huggingface\n",
      "wandb: 🚀 View run at https://wandb.ai/paderborn-university/huggingface/runs/xgk1o6rt\n",
      "  0%|          | 0/750 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-05-29 22:00:15,516 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "[WARNING|logging.py:328] 2025-05-29 22:00:15,548 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "[rank0]:[W529 22:00:15.056376888 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank1]:[W529 22:00:15.089888513 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      " 67%|██████▋   | 500/750 [06:18<03:07,  1.33it/s][INFO|trainer.py:3993] 2025-05-29 22:06:33,873 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/checkpoint-500\n",
      "/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/transformers-4.53.0.dev0-py3.11.egg/transformers/modeling_utils.py:3585: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:438] 2025-05-29 22:06:33,876 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/checkpoint-500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-29 22:06:33,877 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/checkpoint-500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.412, 'grad_norm': 5.63731575012207, 'learning_rate': 1.6733333333333335e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-29 22:06:39,488 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/checkpoint-500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-29 22:06:39,491 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-29 22:06:39,492 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/checkpoint-500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-29 22:06:39,492 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/checkpoint-500/added_tokens.json\n",
      "100%|██████████| 750/750 [09:53<00:00,  1.33it/s][INFO|trainer.py:3993] 2025-05-29 22:10:08,913 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/checkpoint-750\n",
      "[INFO|configuration_utils.py:438] 2025-05-29 22:10:08,916 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/checkpoint-750/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-29 22:10:08,917 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/checkpoint-750/generation_config.json\n",
      "[INFO|modeling_utils.py:3845] 2025-05-29 22:10:14,513 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/checkpoint-750/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-29 22:10:14,516 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/checkpoint-750/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-29 22:10:14,517 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/checkpoint-750/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-29 22:10:14,517 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/checkpoint-750/added_tokens.json\n",
      "[INFO|trainer.py:2676] 2025-05-29 22:10:35,856 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 750/750 [10:20<00:00,  1.21it/s]\n",
      "[INFO|trainer.py:3993] 2025-05-29 22:10:35,859 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa\n",
      "[INFO|configuration_utils.py:438] 2025-05-29 22:10:35,861 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-29 22:10:35,862 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 621.4704, 'train_samples_per_second': 9.655, 'train_steps_per_second': 1.207, 'train_loss': 3.0679796549479166, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-29 22:10:48,193 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-29 22:10:48,198 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-29 22:10:48,198 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-29 22:10:48,199 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_2000_ssa/added_tokens.json\n",
      "[INFO|trainer.py:4327] 2025-05-29 22:10:48,320 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4329] 2025-05-29 22:10:48,320 >>   Num examples = 997\n",
      "[INFO|trainer.py:4332] 2025-05-29 22:10:48,320 >>   Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               =   405546GF\n",
      "  train_loss               =      3.068\n",
      "  train_runtime            = 0:10:21.47\n",
      "  train_samples            =       2000\n",
      "  train_samples_per_second =      9.655\n",
      "  train_steps_per_second   =      1.207\n",
      "05/29/2025 22:10:48 - INFO - __main__ - *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [05:36<00:00,  2.69s/it]\n",
      "[INFO|trainer.py:4327] 2025-05-29 22:16:28,603 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:4329] 2025-05-29 22:16:28,603 >>   Num examples = 1012\n",
      "[INFO|trainer.py:4332] 2025-05-29 22:16:28,603 >>   Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_bleu               =     2.3235\n",
      "  eval_chrf               =      15.77\n",
      "  eval_gen_len            =     58.025\n",
      "  eval_loss               =     4.1323\n",
      "  eval_runtime            = 0:05:40.27\n",
      "  eval_samples            =        997\n",
      "  eval_samples_per_second =       2.93\n",
      "  eval_steps_per_second   =      0.367\n",
      "05/29/2025 22:16:28 - INFO - __main__ - *** Predict ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [05:48<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** predict metrics *****\n",
      "  predict_bleu               =     2.0861\n",
      "  predict_chrf               =    15.4885\n",
      "  predict_gen_len            =    59.6949\n",
      "  predict_loss               =     4.1051\n",
      "  predict_runtime            = 0:05:51.52\n",
      "  predict_samples            =       1012\n",
      "  predict_samples_per_second =      2.879\n",
      "  predict_steps_per_second   =      0.361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modelcard.py:450] 2025-05-29 22:22:31,986 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 2.3235}]}\n",
      "[rank0]:[W529 22:22:32.895664602 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "W0529 22:22:36.055000 22661 torch/distributed/run.py:766] \n",
      "W0529 22:22:36.055000 22661 torch/distributed/run.py:766] *****************************************\n",
      "W0529 22:22:36.055000 22661 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0529 22:22:36.055000 22661 torch/distributed/run.py:766] *****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29/2025 22:22:42 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "05/29/2025 22:22:42 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/runs/May29_22-22-42_yoruba-vm,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "05/29/2025 22:22:42 - INFO - datasets.builder - Using custom data configuration default-9430ff7997eed267\n",
      "05/29/2025 22:22:42 - INFO - datasets.info - Loading Dataset Infos from /home/emmanuelka/jupyter-env/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "05/29/2025 22:22:42 - INFO - datasets.builder - Generating dataset json (/home/emmanuelka/.cache/huggingface/datasets/json/default-9430ff7997eed267/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "05/29/2025 22:22:42 - INFO - datasets.builder - Downloading and preparing dataset json/default to /home/emmanuelka/.cache/huggingface/datasets/json/default-9430ff7997eed267/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
      "05/29/2025 22:22:42 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "05/29/2025 22:22:42 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "05/29/2025 22:22:42 - INFO - datasets.builder - Generating train split\n",
      "05/29/2025 22:22:42 - INFO - datasets.builder - Generating validation split\n",
      "05/29/2025 22:22:42 - INFO - datasets.builder - Generating test split\n",
      "05/29/2025 22:22:42 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "05/29/2025 22:22:42 - INFO - datasets.builder - Dataset json downloaded and prepared to /home/emmanuelka/.cache/huggingface/datasets/json/default-9430ff7997eed267/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
      "05/29/2025 22:22:43 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9430ff7997eed267\n",
      "Loading Dataset Infos from /home/emmanuelka/jupyter-env/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "Generating dataset json (/home/emmanuelka/.cache/huggingface/datasets/json/default-9430ff7997eed267/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "Downloading and preparing dataset json/default to /home/emmanuelka/.cache/huggingface/datasets/json/default-9430ff7997eed267/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "Generating train split: 4000 examples [00:00, 439781.28 examples/s]\n",
      "Generating validation split\n",
      "Generating validation split: 997 examples [00:00, 238369.78 examples/s]\n",
      "Generating test split\n",
      "Generating test split: 1012 examples [00:00, 239269.20 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /home/emmanuelka/.cache/huggingface/datasets/json/default-9430ff7997eed267/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
      "[INFO|configuration_utils.py:712] 2025-05-29 22:22:43,023 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-29 22:22:43,027 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:712] 2025-05-29 22:22:43,056 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-29 22:22:43,057 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 22:22:43,101 >> loading file vocab.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 22:22:43,101 >> loading file sentencepiece.bpe.model from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 22:22:43,101 >> loading file tokenizer_config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 22:22:43,101 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 22:22:43,101 >> loading file special_tokens_map.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 22:22:43,101 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-29 22:22:43,101 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:712] 2025-05-29 22:22:43,101 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-29 22:22:43,102 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1270] 2025-05-29 22:22:43,963 >> loading weights file pytorch_model.bin from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:1122] 2025-05-29 22:22:43,965 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "[INFO|safetensors_conversion.py:61] 2025-05-29 22:22:44,018 >> Attempting to create safetensors variant\n",
      "[INFO|safetensors_conversion.py:74] 2025-05-29 22:22:44,182 >> Safetensors PR exists\n",
      "[INFO|modeling_utils.py:5233] 2025-05-29 22:22:47,929 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:5241] 2025-05-29 22:22:47,929 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:1077] 2025-05-29 22:22:47,963 >> loading configuration file generation_config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
      "[INFO|configuration_utils.py:1122] 2025-05-29 22:22:47,964 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "[rank1]:[W529 22:22:48.469056163 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Running tokenizer on train dataset:   0%|          | 0/4000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29/2025 22:22:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-9430ff7997eed267/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b21a8ec73a02a6b0.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-9430ff7997eed267/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b21a8ec73a02a6b0.arrow\n",
      "Running tokenizer on train dataset: 100%|██████████| 4000/4000 [00:01<00:00, 3199.15 examples/s]\n",
      "[rank0]:[W529 22:22:50.846368238 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Running tokenizer on validation dataset:   0%|          | 0/997 [00:00<?, ? examples/s]Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-9430ff7997eed267/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-dfa53a9d5fb2e61b.arrow\n",
      "Running tokenizer on validation dataset: 100%|██████████| 997/997 [00:00<00:00, 2001.11 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29/2025 22:22:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-9430ff7997eed267/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-dfa53a9d5fb2e61b.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on prediction dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-9430ff7997eed267/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-73e1334508c4e412.arrow\n",
      "Running tokenizer on prediction dataset: 100%|██████████| 1012/1012 [00:00<00:00, 1928.62 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/29/2025 22:22:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-9430ff7997eed267/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-73e1334508c4e412.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:756] 2025-05-29 22:22:56,690 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2409] 2025-05-29 22:22:57,197 >> ***** Running training *****\n",
      "[INFO|trainer.py:2410] 2025-05-29 22:22:57,198 >>   Num examples = 4,000\n",
      "[INFO|trainer.py:2411] 2025-05-29 22:22:57,198 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2412] 2025-05-29 22:22:57,198 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2415] 2025-05-29 22:22:57,198 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2416] 2025-05-29 22:22:57,198 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2417] 2025-05-29 22:22:57,198 >>   Total optimization steps = 1,500\n",
      "[INFO|trainer.py:2418] 2025-05-29 22:22:57,199 >>   Number of trainable parameters = 483,905,536\n",
      "[INFO|integration_utils.py:832] 2025-05-29 22:22:57,317 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: emmanuelka (paderborn-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.11\n",
      "wandb: Run data is saved locally in /home/emmanuelka/AIMS-NLP-Project/wandb/run-20250529_222257-gubmzm00\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa\n",
      "wandb: ⭐️ View project at https://wandb.ai/paderborn-university/huggingface\n",
      "wandb: 🚀 View run at https://wandb.ai/paderborn-university/huggingface/runs/gubmzm00\n",
      "  0%|          | 0/1500 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-05-29 22:22:58,303 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "[WARNING|logging.py:328] 2025-05-29 22:22:58,348 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "[rank1]:[W529 22:22:58.840386713 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W529 22:22:58.883960872 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      " 33%|███▎      | 500/1500 [06:17<12:31,  1.33it/s][INFO|trainer.py:3993] 2025-05-29 22:29:15,248 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-500\n",
      "/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/transformers-4.53.0.dev0-py3.11.egg/transformers/modeling_utils.py:3585: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:438] 2025-05-29 22:29:15,252 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-29 22:29:15,252 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6324, 'grad_norm': 9.014321327209473, 'learning_rate': 3.336666666666667e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-29 22:29:20,525 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-29 22:29:20,529 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-29 22:29:20,530 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-29 22:29:20,530 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-500/added_tokens.json\n",
      " 67%|██████▋   | 1000/1500 [12:59<06:15,  1.33it/s][INFO|trainer.py:3993] 2025-05-29 22:35:57,370 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-1000\n",
      "[INFO|configuration_utils.py:438] 2025-05-29 22:35:57,372 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-1000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-29 22:35:57,372 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-1000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6703, 'grad_norm': 7.048520088195801, 'learning_rate': 1.6700000000000003e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-29 22:36:02,158 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-1000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-29 22:36:02,162 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-29 22:36:02,162 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-29 22:36:02,163 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-1000/added_tokens.json\n",
      "100%|██████████| 1500/1500 [19:43<00:00,  1.33it/s][INFO|trainer.py:3993] 2025-05-29 22:42:41,138 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-1500\n",
      "[INFO|configuration_utils.py:438] 2025-05-29 22:42:41,140 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-1500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-29 22:42:41,141 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-1500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1678, 'grad_norm': 7.2590203285217285, 'learning_rate': 3.3333333333333334e-08, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-29 22:42:46,433 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-1500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-29 22:42:46,436 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-29 22:42:46,436 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-29 22:42:46,437 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/checkpoint-1500/added_tokens.json\n",
      "[INFO|trainer.py:2676] 2025-05-29 22:43:07,644 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 1500/1500 [20:09<00:00,  1.24it/s]\n",
      "[INFO|trainer.py:3993] 2025-05-29 22:43:07,648 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa\n",
      "[INFO|configuration_utils.py:438] 2025-05-29 22:43:07,650 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-29 22:43:07,651 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1210.446, 'train_samples_per_second': 9.914, 'train_steps_per_second': 1.239, 'train_loss': 2.8235305989583335, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-29 22:43:19,850 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-29 22:43:19,853 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-29 22:43:19,854 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-29 22:43:19,854 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_4000_ssa/added_tokens.json\n",
      "[INFO|trainer.py:4327] 2025-05-29 22:43:19,975 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4329] 2025-05-29 22:43:19,976 >>   Num examples = 997\n",
      "[INFO|trainer.py:4332] 2025-05-29 22:43:19,976 >>   Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               =   798857GF\n",
      "  train_loss               =     2.8235\n",
      "  train_runtime            = 0:20:10.44\n",
      "  train_samples            =       4000\n",
      "  train_samples_per_second =      9.914\n",
      "  train_steps_per_second   =      1.239\n",
      "05/29/2025 22:43:19 - INFO - __main__ - *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [05:22<00:00,  2.58s/it]\n",
      "[INFO|trainer.py:4327] 2025-05-29 22:48:44,925 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:4329] 2025-05-29 22:48:44,925 >>   Num examples = 1012\n",
      "[INFO|trainer.py:4332] 2025-05-29 22:48:44,925 >>   Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_bleu               =     2.7675\n",
      "  eval_chrf               =    17.8289\n",
      "  eval_gen_len            =     55.088\n",
      "  eval_loss               =     3.8563\n",
      "  eval_runtime            = 0:05:24.94\n",
      "  eval_samples            =        997\n",
      "  eval_samples_per_second =      3.068\n",
      "  eval_steps_per_second   =      0.385\n",
      "05/29/2025 22:48:44 - INFO - __main__ - *** Predict ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [05:37<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** predict metrics *****\n",
      "  predict_bleu               =     2.4734\n",
      "  predict_chrf               =    17.3518\n",
      "  predict_gen_len            =    56.8278\n",
      "  predict_loss               =      3.821\n",
      "  predict_runtime            = 0:05:40.99\n",
      "  predict_samples            =       1012\n",
      "  predict_samples_per_second =      2.968\n",
      "  predict_steps_per_second   =      0.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modelcard.py:450] 2025-05-29 22:54:38,303 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 2.7675}]}\n",
      "[rank0]:[W529 22:54:38.248700774 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "language_pairs = [\"en-yo\"]  # Add more as needed\n",
    "text_nums = [1000 * 2 ** i for i in range(3)]\n",
    "base_dir = \"/home/emmanuelka\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "for lang_pair in language_pairs:\n",
    "    src, tgt = lang_pair.split(\"-\")\n",
    "\n",
    "    for num in text_nums:\n",
    "        train_file = f\"{base_dir}/AIMS-NLP-Project/data/nllb/ssa-comet-mtl/{lang_pair}/train_{num}.json\"\n",
    "        val_file = f\"{base_dir}/AIMS-NLP-Project/data/flores/{lang_pair}/dev.json\"\n",
    "        test_file = f\"{base_dir}/AIMS-NLP-Project/data/flores/{lang_pair}/devtest.json\"\n",
    "        output_dir = f\"{base_dir}/M2M-100/{lang_pair}/africomet_{num}_ssa\"\n",
    "\n",
    "        cmd = [\n",
    "            \"torchrun\",\n",
    "            \"--nproc_per_node=2\",\n",
    "            f\"{base_dir}/AIMS-NLP-Project/run_translation.py\",\n",
    "            \"--model_name_or_path\", \"facebook/m2m100_418M\",\n",
    "            \"--do_train\", \"--do_eval\", \"--do_predict\",\n",
    "            \"--source_lang\", src,\n",
    "            \"--target_lang\", tgt,\n",
    "            \"--train_file\", train_file,\n",
    "            \"--validation_file\", val_file,\n",
    "            \"--test_file\", test_file,\n",
    "            \"--num_beams\", \"10\",\n",
    "            \"--output_dir\", output_dir,\n",
    "            \"--per_device_train_batch_size\", \"4\",\n",
    "            \"--per_device_eval_batch_size\", \"4\",\n",
    "            \"--overwrite_output_dir\",\n",
    "            \"--predict_with_generate\",\n",
    "            \"--fp16\",  # comment this out if your GPU doesn’t support fp16\n",
    "        ]\n",
    "\n",
    "        # Launch multi-GPU run\n",
    "        subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa3bc95a-204b-4d18-ba67-258a1937bfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/emmanuelka/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Log in with your API key\n",
    "wandb.login(key=\"7f13d9fe09d0856f7c12099a27ccda7aa15c8afd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f4e7eb-ae65-4aed-bf53-512a9cf2edab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0530 14:14:21.984000 58328 torch/distributed/run.py:766] \n",
      "W0530 14:14:21.984000 58328 torch/distributed/run.py:766] *****************************************\n",
      "W0530 14:14:21.984000 58328 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0530 14:14:21.984000 58328 torch/distributed/run.py:766] *****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 14:14:27 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "05/30/2025 14:14:28 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "05/30/2025 14:14:28 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/emmanuelka/M2M-100/en-yo/africomet_8000_ssa/runs/May30_14-14-28_yoruba-vm,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/home/emmanuelka/M2M-100/en-yo/africomet_8000_ssa,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/emmanuelka/M2M-100/en-yo/africomet_8000_ssa,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "05/30/2025 14:14:28 - INFO - datasets.builder - Using custom data configuration default-b49dc552a04076a6\n",
      "05/30/2025 14:14:28 - INFO - datasets.info - Loading Dataset Infos from /home/emmanuelka/jupyter-env/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "05/30/2025 14:14:28 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "05/30/2025 14:14:28 - INFO - datasets.info - Loading Dataset info from /home/emmanuelka/.cache/huggingface/datasets/json/default-b49dc552a04076a6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
      "05/30/2025 14:14:28 - INFO - datasets.builder - Found cached dataset json (/home/emmanuelka/.cache/huggingface/datasets/json/default-b49dc552a04076a6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "05/30/2025 14:14:28 - INFO - datasets.info - Loading Dataset info from /home/emmanuelka/.cache/huggingface/datasets/json/default-b49dc552a04076a6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b49dc552a04076a6\n",
      "Loading Dataset Infos from /home/emmanuelka/jupyter-env/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/emmanuelka/.cache/huggingface/datasets/json/default-b49dc552a04076a6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
      "Found cached dataset json (/home/emmanuelka/.cache/huggingface/datasets/json/default-b49dc552a04076a6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "Loading Dataset info from /home/emmanuelka/.cache/huggingface/datasets/json/default-b49dc552a04076a6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
      "[INFO|configuration_utils.py:712] 2025-05-30 14:14:28,462 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-30 14:14:28,466 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:712] 2025-05-30 14:14:28,506 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-30 14:14:28,508 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 14:14:28,541 >> loading file vocab.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 14:14:28,541 >> loading file sentencepiece.bpe.model from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 14:14:28,541 >> loading file tokenizer_config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 14:14:28,542 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 14:14:28,542 >> loading file special_tokens_map.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 14:14:28,542 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 14:14:28,542 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:712] 2025-05-30 14:14:28,543 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-30 14:14:28,544 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1270] 2025-05-30 14:14:29,820 >> loading weights file pytorch_model.bin from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:1122] 2025-05-30 14:14:29,823 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "[INFO|safetensors_conversion.py:61] 2025-05-30 14:14:29,875 >> Attempting to create safetensors variant\n",
      "[INFO|safetensors_conversion.py:74] 2025-05-30 14:14:30,044 >> Safetensors PR exists\n",
      "[rank1]:[W530 14:14:31.843087409 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "[INFO|modeling_utils.py:5233] 2025-05-30 14:14:31,499 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:5241] 2025-05-30 14:14:31,499 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:1077] 2025-05-30 14:14:31,531 >> loading configuration file generation_config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
      "[INFO|configuration_utils.py:1122] 2025-05-30 14:14:31,531 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-b49dc552a04076a6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2957c749aab66b1a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 14:14:32 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-b49dc552a04076a6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2957c749aab66b1a.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W530 14:14:32.140760419 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-b49dc552a04076a6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0c1a77f0dd90cbdb.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 14:14:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-b49dc552a04076a6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0c1a77f0dd90cbdb.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-b49dc552a04076a6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4417474a154b2fbd.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 14:14:36 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-b49dc552a04076a6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4417474a154b2fbd.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:756] 2025-05-30 14:14:39,809 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2409] 2025-05-30 14:14:42,109 >> ***** Running training *****\n",
      "[INFO|trainer.py:2410] 2025-05-30 14:14:42,109 >>   Num examples = 8,000\n",
      "[INFO|trainer.py:2411] 2025-05-30 14:14:42,109 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2412] 2025-05-30 14:14:42,109 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2415] 2025-05-30 14:14:42,109 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2416] 2025-05-30 14:14:42,109 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2417] 2025-05-30 14:14:42,109 >>   Total optimization steps = 3,000\n",
      "[INFO|trainer.py:2418] 2025-05-30 14:14:42,111 >>   Number of trainable parameters = 483,905,536\n",
      "[INFO|integration_utils.py:832] 2025-05-30 14:14:42,394 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/home/emmanuelka/AIMS-NLP-Project/run_translation.py\", line 704, in <module>\n",
      "[rank1]:     main()\n",
      "[rank1]:   File \"/home/emmanuelka/AIMS-NLP-Project/run_translation.py\", line 619, in main\n",
      "[rank1]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
      "[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/transformers-4.53.0.dev0-py3.11.egg/transformers/trainer.py\", line 2240, in train\n",
      "[rank1]:     return inner_training_loop(\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/transformers-4.53.0.dev0-py3.11.egg/transformers/trainer.py\", line 2369, in _inner_training_loop\n",
      "[rank1]:     model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n",
      "[rank1]:                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1438, in prepare\n",
      "[rank1]:     result = tuple(\n",
      "[rank1]:              ^^^^^^\n",
      "[rank1]:   File \"/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1439, in <genexpr>\n",
      "[rank1]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)\n",
      "[rank1]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1281, in _prepare_one\n",
      "[rank1]:     return self.prepare_model(obj, device_placement=device_placement)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1585, in prepare_model\n",
      "[rank1]:     model = torch.nn.parallel.DistributedDataParallel(\n",
      "[rank1]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 850, in __init__\n",
      "[rank1]:     self._ddp_init_helper(\n",
      "[rank1]:   File \"/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1201, in _ddp_init_helper\n",
      "[rank1]:     self.reducer = dist.Reducer(\n",
      "[rank1]:                    ^^^^^^^^^^^^^\n",
      "[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 1 has a total capacity of 14.58 GiB of which 13.62 MiB is free. Process 53760 has 11.59 GiB memory in use. Including non-PyTorch memory, this process has 2.98 GiB memory in use. Of the allocated memory 2.72 GiB is allocated by PyTorch, and 66.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "wandb: Currently logged in as: emmanuelka (paderborn-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.11\n",
      "wandb: Run data is saved locally in /home/emmanuelka/wandb/run-20250530_141442-yb9zfsev\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run /home/emmanuelka/M2M-100/en-yo/africomet_8000_ssa\n",
      "wandb: ⭐️ View project at https://wandb.ai/paderborn-university/huggingface\n",
      "wandb: 🚀 View run at https://wandb.ai/paderborn-university/huggingface/runs/yb9zfsev\n",
      "  0%|          | 0/3000 [00:00<?, ?it/s]W0530 14:14:44.334000 58328 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 58333 closing signal SIGTERM\n",
      "E0530 14:14:44.499000 58328 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 1 (pid: 58334) of binary: /home/emmanuelka/jupyter-env/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/emmanuelka/jupyter-env/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/torch/distributed/run.py\", line 892, in main\n",
      "    run(args)\n",
      "  File \"/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/torch/distributed/run.py\", line 883, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 139, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 270, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/home/emmanuelka/AIMS-NLP-Project/run_translation.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-05-30_14:14:44\n",
      "  host      : yoruba-vm.c.focus-sequencer-418317.internal\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 58334)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "language_pairs = [\"en-yo\"]  # Add more as needed\n",
    "text_nums = [1000 * 2 ** i for i in range(3, 6)]\n",
    "base_dir = \"/home/emmanuelka\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "for lang_pair in language_pairs:\n",
    "    src, tgt = lang_pair.split(\"-\")\n",
    "\n",
    "    for num in text_nums:\n",
    "        train_file = f\"{base_dir}/AIMS-NLP-Project/data/nllb/ssa-comet-mtl/{lang_pair}/train_{num}.json\"\n",
    "        val_file = f\"{base_dir}/AIMS-NLP-Project/data/flores/{lang_pair}/dev.json\"\n",
    "        test_file = f\"{base_dir}/AIMS-NLP-Project/data/flores/{lang_pair}/devtest.json\"\n",
    "        output_dir = f\"{base_dir}/M2M-100/{lang_pair}/africomet_{num}_ssa\"\n",
    "\n",
    "        cmd = [\n",
    "            \"torchrun\",\n",
    "            \"--nproc_per_node=2\",\n",
    "            f\"{base_dir}/AIMS-NLP-Project/run_translation.py\",\n",
    "            \"--model_name_or_path\", \"facebook/m2m100_418M\",\n",
    "            \"--do_train\", \"--do_eval\", \"--do_predict\",\n",
    "            \"--source_lang\", src,\n",
    "            \"--target_lang\", tgt,\n",
    "            \"--train_file\", train_file,\n",
    "            \"--validation_file\", val_file,\n",
    "            \"--test_file\", test_file,\n",
    "            \"--num_beams\", \"10\",\n",
    "            \"--output_dir\", output_dir,\n",
    "            \"--per_device_train_batch_size\", \"4\",\n",
    "            \"--per_device_eval_batch_size\", \"4\",\n",
    "            \"--overwrite_output_dir\",\n",
    "            \"--predict_with_generate\",\n",
    "            \"--fp16\",  # comment this out if your GPU doesn’t support fp16\n",
    "        ]\n",
    "\n",
    "        # Launch multi-GPU run\n",
    "        subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a3376-0123-4536-9b6f-02cb2ae9f0c5",
   "metadata": {},
   "source": [
    "### Africomet + Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24969ca7-6f8f-4160-b307-0ea20a9d60b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/emmanuelka/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Log in with your API key\n",
    "wandb.login(key=\"7f13d9fe09d0856f7c12099a27ccda7aa15c8afd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4074961d-6976-4176-b65a-11ad06842a49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0530 10:34:31.905000 35563 torch/distributed/run.py:766] \n",
      "W0530 10:34:31.905000 35563 torch/distributed/run.py:766] *****************************************\n",
      "W0530 10:34:31.905000 35563 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0530 10:34:31.905000 35563 torch/distributed/run.py:766] *****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 10:34:37 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "05/30/2025 10:34:37 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/runs/May30_10-34-37_yoruba-vm,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "05/30/2025 10:34:37 - INFO - datasets.builder - Using custom data configuration default-d1f9932bb0ea2ca1\n",
      "05/30/2025 10:34:37 - INFO - datasets.info - Loading Dataset Infos from /home/emmanuelka/jupyter-env/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "05/30/2025 10:34:37 - INFO - datasets.builder - Generating dataset json (/home/emmanuelka/.cache/huggingface/datasets/json/default-d1f9932bb0ea2ca1/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "05/30/2025 10:34:37 - INFO - datasets.builder - Downloading and preparing dataset json/default to /home/emmanuelka/.cache/huggingface/datasets/json/default-d1f9932bb0ea2ca1/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
      "05/30/2025 10:34:37 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "05/30/2025 10:34:37 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "05/30/2025 10:34:37 - INFO - datasets.builder - Generating train split\n",
      "05/30/2025 10:34:37 - INFO - datasets.builder - Generating validation split\n",
      "05/30/2025 10:34:37 - INFO - datasets.builder - Generating test split\n",
      "05/30/2025 10:34:37 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "05/30/2025 10:34:37 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "05/30/2025 10:34:37 - INFO - datasets.builder - Dataset json downloaded and prepared to /home/emmanuelka/.cache/huggingface/datasets/json/default-d1f9932bb0ea2ca1/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d1f9932bb0ea2ca1\n",
      "Loading Dataset Infos from /home/emmanuelka/jupyter-env/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "Generating dataset json (/home/emmanuelka/.cache/huggingface/datasets/json/default-d1f9932bb0ea2ca1/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "Downloading and preparing dataset json/default to /home/emmanuelka/.cache/huggingface/datasets/json/default-d1f9932bb0ea2ca1/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "Generating train split: 10746 examples [00:00, 239489.86 examples/s]\n",
      "Generating validation split\n",
      "Generating validation split: 997 examples [00:00, 187327.92 examples/s]\n",
      "Generating test split\n",
      "Generating test split: 1012 examples [00:00, 204491.77 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /home/emmanuelka/.cache/huggingface/datasets/json/default-d1f9932bb0ea2ca1/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
      "[INFO|configuration_utils.py:712] 2025-05-30 10:34:37,796 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-30 10:34:37,800 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:712] 2025-05-30 10:34:37,829 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-30 10:34:37,830 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 10:34:37,885 >> loading file vocab.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 10:34:37,885 >> loading file sentencepiece.bpe.model from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 10:34:37,885 >> loading file tokenizer_config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 10:34:37,885 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 10:34:37,885 >> loading file special_tokens_map.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 10:34:37,885 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 10:34:37,885 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:712] 2025-05-30 10:34:37,885 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-30 10:34:37,886 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1270] 2025-05-30 10:34:38,785 >> loading weights file pytorch_model.bin from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:1122] 2025-05-30 10:34:38,786 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "[INFO|safetensors_conversion.py:61] 2025-05-30 10:34:38,872 >> Attempting to create safetensors variant\n",
      "[INFO|safetensors_conversion.py:74] 2025-05-30 10:34:39,083 >> Safetensors PR exists\n",
      "[INFO|modeling_utils.py:5233] 2025-05-30 10:34:47,781 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:5241] 2025-05-30 10:34:47,781 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:1077] 2025-05-30 10:34:47,816 >> loading configuration file generation_config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
      "[INFO|configuration_utils.py:1122] 2025-05-30 10:34:47,816 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "[rank1]:[W530 10:34:47.231236236 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Running tokenizer on train dataset:   0%|          | 0/10746 [00:00<?, ? examples/s]Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-d1f9932bb0ea2ca1/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-148ffd1e36e25ed5.arrow\n",
      "Running tokenizer on train dataset:   9%|▉         | 1000/10746 [00:00<00:05, 1819.27 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 10:34:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-d1f9932bb0ea2ca1/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-148ffd1e36e25ed5.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset: 100%|██████████| 10746/10746 [00:05<00:00, 1879.63 examples/s]\n",
      "[rank0]:[W530 10:34:54.130820081 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Running tokenizer on validation dataset:   0%|          | 0/997 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 10:34:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-d1f9932bb0ea2ca1/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-76fddc1dbc6048bc.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-d1f9932bb0ea2ca1/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-76fddc1dbc6048bc.arrow\n",
      "Running tokenizer on validation dataset: 100%|██████████| 997/997 [00:00<00:00, 1982.34 examples/s]\n",
      "Running tokenizer on prediction dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-d1f9932bb0ea2ca1/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-25000ef00fda6a1a.arrow\n",
      "Running tokenizer on prediction dataset: 100%|██████████| 1012/1012 [00:00<00:00, 1899.26 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 10:34:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-d1f9932bb0ea2ca1/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-25000ef00fda6a1a.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:756] 2025-05-30 10:35:00,818 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2409] 2025-05-30 10:35:01,875 >> ***** Running training *****\n",
      "[INFO|trainer.py:2410] 2025-05-30 10:35:01,875 >>   Num examples = 10,746\n",
      "[INFO|trainer.py:2411] 2025-05-30 10:35:01,875 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2412] 2025-05-30 10:35:01,875 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2415] 2025-05-30 10:35:01,875 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2416] 2025-05-30 10:35:01,875 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2417] 2025-05-30 10:35:01,875 >>   Total optimization steps = 4,032\n",
      "[INFO|trainer.py:2418] 2025-05-30 10:35:01,876 >>   Number of trainable parameters = 483,905,536\n",
      "[INFO|integration_utils.py:832] 2025-05-30 10:35:01,993 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: emmanuelka (paderborn-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.11\n",
      "wandb: Run data is saved locally in /home/emmanuelka/wandb/run-20250530_103502-xrxw9yb1\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa\n",
      "wandb: ⭐️ View project at https://wandb.ai/paderborn-university/huggingface\n",
      "wandb: 🚀 View run at https://wandb.ai/paderborn-university/huggingface/runs/xrxw9yb1\n",
      "  0%|          | 0/4032 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-05-30 10:35:03,192 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "[WARNING|logging.py:328] 2025-05-30 10:35:03,218 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "[rank1]:[W530 10:35:03.677348351 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W530 10:35:03.704647943 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      " 12%|█▏        | 500/4032 [04:51<34:29,  1.71it/s][INFO|trainer.py:3993] 2025-05-30 10:39:54,486 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-500\n",
      "/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/transformers-4.53.0.dev0-py3.11.egg/transformers/modeling_utils.py:3585: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 10:39:54,491 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 10:39:54,492 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1085, 'grad_norm': 4.989090442657471, 'learning_rate': 4.3824404761904765e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 10:39:58,914 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 10:39:58,917 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 10:39:58,918 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 10:39:58,918 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-500/added_tokens.json\n",
      " 25%|██▍       | 1000/4032 [10:05<29:26,  1.72it/s][INFO|trainer.py:3993] 2025-05-30 10:45:08,106 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-1000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 10:45:08,108 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-1000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 10:45:08,108 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-1000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3454, 'grad_norm': 3.9590747356414795, 'learning_rate': 3.762400793650794e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 10:45:12,445 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-1000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 10:45:12,448 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 10:45:12,449 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 10:45:12,451 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-1000/added_tokens.json\n",
      " 37%|███▋      | 1500/4032 [15:17<24:47,  1.70it/s][INFO|trainer.py:3993] 2025-05-30 10:50:20,648 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-1500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 10:50:20,650 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-1500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 10:50:20,651 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-1500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0131, 'grad_norm': 4.683335781097412, 'learning_rate': 3.142361111111111e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 10:50:24,634 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-1500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 10:50:24,638 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 10:50:24,638 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 10:50:24,639 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-1500/added_tokens.json\n",
      " 50%|████▉     | 2000/4032 [20:30<19:43,  1.72it/s][INFO|trainer.py:3993] 2025-05-30 10:55:33,334 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-2000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 10:55:33,336 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-2000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 10:55:33,336 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-2000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7521, 'grad_norm': 5.0413408279418945, 'learning_rate': 2.5223214285714287e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 10:55:37,475 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-2000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 10:55:37,480 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 10:55:37,480 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 10:55:37,481 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-2000/added_tokens.json\n",
      " 62%|██████▏   | 2500/4032 [25:43<14:52,  1.72it/s][INFO|trainer.py:3993] 2025-05-30 11:00:46,045 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-2500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 11:00:46,048 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-2500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 11:00:46,048 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-2500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.64, 'grad_norm': 4.8214216232299805, 'learning_rate': 1.902281746031746e-05, 'epoch': 1.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 11:00:51,126 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-2500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 11:00:51,130 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 11:00:51,131 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-2500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 11:00:51,131 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-2500/added_tokens.json\n",
      " 74%|███████▍  | 3000/4032 [30:56<09:58,  1.72it/s][INFO|trainer.py:3993] 2025-05-30 11:05:58,836 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-3000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 11:05:58,838 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-3000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 11:05:58,839 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-3000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4924, 'grad_norm': 4.427936553955078, 'learning_rate': 1.2822420634920635e-05, 'epoch': 2.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 11:06:02,884 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-3000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 11:06:02,888 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 11:06:02,889 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-3000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 11:06:02,889 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-3000/added_tokens.json\n",
      " 87%|████████▋ | 3500/4032 [36:08<05:08,  1.72it/s][INFO|trainer.py:3993] 2025-05-30 11:11:10,710 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-3500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 11:11:10,713 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-3500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 11:11:10,713 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-3500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3758, 'grad_norm': 4.419530868530273, 'learning_rate': 6.62202380952381e-06, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 11:11:14,939 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-3500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 11:11:14,943 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 11:11:14,944 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-3500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 11:11:14,944 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-3500/added_tokens.json\n",
      " 99%|█████████▉| 4000/4032 [41:20<00:18,  1.72it/s][INFO|trainer.py:3993] 2025-05-30 11:16:22,734 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-4000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 11:16:22,736 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-4000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 11:16:22,737 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-4000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.368, 'grad_norm': 4.4354352951049805, 'learning_rate': 4.216269841269841e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 11:16:27,128 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-4000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 11:16:27,132 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 11:16:27,132 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-4000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 11:16:27,132 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-4000/added_tokens.json\n",
      "100%|██████████| 4032/4032 [42:00<00:00,  1.74it/s][INFO|trainer.py:3993] 2025-05-30 11:17:03,112 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-4032\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 11:17:03,115 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-4032/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 11:17:03,115 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-4032/generation_config.json\n",
      "[INFO|modeling_utils.py:3845] 2025-05-30 11:17:07,075 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-4032/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 11:17:07,077 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-4032/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 11:17:07,078 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-4032/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 11:17:07,078 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/checkpoint-4032/added_tokens.json\n",
      "[INFO|trainer.py:2676] 2025-05-30 11:17:23,847 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 4032/4032 [42:21<00:00,  1.59it/s]\n",
      "[INFO|trainer.py:3993] 2025-05-30 11:17:23,851 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 11:17:23,853 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 11:17:23,854 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2541.9712, 'train_samples_per_second': 12.682, 'train_steps_per_second': 1.586, 'train_loss': 1.8822951856113614, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 11:17:33,363 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 11:17:33,368 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 11:17:33,368 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 11:17:33,369 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_1000_ssa/added_tokens.json\n",
      "[INFO|trainer.py:4327] 2025-05-30 11:17:33,486 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4329] 2025-05-30 11:17:33,486 >>   Num examples = 997\n",
      "[INFO|trainer.py:4332] 2025-05-30 11:17:33,486 >>   Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               =  3468592GF\n",
      "  train_loss               =     1.8823\n",
      "  train_runtime            = 0:42:21.97\n",
      "  train_samples            =      10746\n",
      "  train_samples_per_second =     12.682\n",
      "  train_steps_per_second   =      1.586\n",
      "05/30/2025 11:17:33 - INFO - __main__ - *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [05:56<00:00,  2.85s/it]\n",
      "[INFO|trainer.py:4327] 2025-05-30 11:23:33,226 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:4329] 2025-05-30 11:23:33,226 >>   Num examples = 1012\n",
      "[INFO|trainer.py:4332] 2025-05-30 11:23:33,226 >>   Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_bleu               =     5.8838\n",
      "  eval_chrf               =    26.6467\n",
      "  eval_gen_len            =     70.834\n",
      "  eval_loss               =     2.9911\n",
      "  eval_runtime            = 0:05:59.73\n",
      "  eval_samples            =        997\n",
      "  eval_samples_per_second =      2.771\n",
      "  eval_steps_per_second   =      0.347\n",
      "05/30/2025 11:23:33 - INFO - __main__ - *** Predict ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [06:23<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** predict metrics *****\n",
      "  predict_bleu               =     6.0881\n",
      "  predict_chrf               =    26.5285\n",
      "  predict_gen_len            =    72.5846\n",
      "  predict_loss               =     2.9758\n",
      "  predict_runtime            = 0:06:26.44\n",
      "  predict_samples            =       1012\n",
      "  predict_samples_per_second =      2.619\n",
      "  predict_steps_per_second   =      0.329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modelcard.py:450] 2025-05-30 11:30:12,011 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 5.8838}]}\n",
      "[rank0]:[W530 11:30:12.847874312 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "W0530 11:30:15.809000 40829 torch/distributed/run.py:766] \n",
      "W0530 11:30:15.809000 40829 torch/distributed/run.py:766] *****************************************\n",
      "W0530 11:30:15.809000 40829 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0530 11:30:15.809000 40829 torch/distributed/run.py:766] *****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 11:30:21 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "05/30/2025 11:30:21 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/runs/May30_11-30-20_yoruba-vm,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "05/30/2025 11:30:21 - INFO - datasets.builder - Using custom data configuration default-483646c343b893fc\n",
      "05/30/2025 11:30:21 - INFO - datasets.info - Loading Dataset Infos from /home/emmanuelka/jupyter-env/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "05/30/2025 11:30:21 - INFO - datasets.builder - Generating dataset json (/home/emmanuelka/.cache/huggingface/datasets/json/default-483646c343b893fc/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "05/30/2025 11:30:21 - INFO - datasets.builder - Downloading and preparing dataset json/default to /home/emmanuelka/.cache/huggingface/datasets/json/default-483646c343b893fc/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
      "05/30/2025 11:30:21 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "05/30/2025 11:30:21 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "05/30/2025 11:30:21 - INFO - datasets.builder - Generating train split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-483646c343b893fc\n",
      "Loading Dataset Infos from /home/emmanuelka/jupyter-env/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "Generating dataset json (/home/emmanuelka/.cache/huggingface/datasets/json/default-483646c343b893fc/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "Downloading and preparing dataset json/default to /home/emmanuelka/.cache/huggingface/datasets/json/default-483646c343b893fc/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "Generating train split: 11746 examples [00:00, 220323.40 examples/s]\n",
      "Generating validation split\n",
      "Generating validation split: 997 examples [00:00, 209757.28 examples/s]\n",
      "Generating test split\n",
      "Generating test split: 1012 examples [00:00, 150375.02 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /home/emmanuelka/.cache/huggingface/datasets/json/default-483646c343b893fc/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
      "[INFO|configuration_utils.py:712] 2025-05-30 11:30:21,615 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-30 11:30:21,618 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:712] 2025-05-30 11:30:21,651 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-30 11:30:21,651 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 11:30:21 - INFO - datasets.builder - Generating validation split\n",
      "05/30/2025 11:30:21 - INFO - datasets.builder - Generating test split\n",
      "05/30/2025 11:30:21 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "05/30/2025 11:30:21 - INFO - datasets.builder - Dataset json downloaded and prepared to /home/emmanuelka/.cache/huggingface/datasets/json/default-483646c343b893fc/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
      "05/30/2025 11:30:21 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 11:30:21,683 >> loading file vocab.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 11:30:21,683 >> loading file sentencepiece.bpe.model from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 11:30:21,683 >> loading file tokenizer_config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 11:30:21,683 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 11:30:21,683 >> loading file special_tokens_map.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 11:30:21,683 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 11:30:21,683 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:712] 2025-05-30 11:30:21,683 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-30 11:30:21,684 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1270] 2025-05-30 11:30:22,561 >> loading weights file pytorch_model.bin from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:1122] 2025-05-30 11:30:22,562 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "[INFO|safetensors_conversion.py:61] 2025-05-30 11:30:22,607 >> Attempting to create safetensors variant\n",
      "[INFO|safetensors_conversion.py:74] 2025-05-30 11:30:23,076 >> Safetensors PR exists\n",
      "[INFO|modeling_utils.py:5233] 2025-05-30 11:30:24,178 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:5241] 2025-05-30 11:30:24,178 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:1077] 2025-05-30 11:30:24,213 >> loading configuration file generation_config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
      "[INFO|configuration_utils.py:1122] 2025-05-30 11:30:24,213 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "[rank1]:[W530 11:30:24.852601059 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Running tokenizer on train dataset:   0%|          | 0/11746 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 11:30:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-483646c343b893fc/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-fa22bb18608686a1.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-483646c343b893fc/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-fa22bb18608686a1.arrow\n",
      "Running tokenizer on train dataset: 100%|██████████| 11746/11746 [00:06<00:00, 1862.63 examples/s]\n",
      "[rank0]:[W530 11:30:31.090742923 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Running tokenizer on validation dataset:   0%|          | 0/997 [00:00<?, ? examples/s]Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-483646c343b893fc/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-81365fd2e68ae28c.arrow\n",
      "Running tokenizer on validation dataset: 100%|██████████| 997/997 [00:00<00:00, 1949.51 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 11:30:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-483646c343b893fc/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-81365fd2e68ae28c.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on prediction dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-483646c343b893fc/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-112e653edd8c35ac.arrow\n",
      "Running tokenizer on prediction dataset: 100%|██████████| 1012/1012 [00:00<00:00, 1706.84 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 11:30:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-483646c343b893fc/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-112e653edd8c35ac.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:756] 2025-05-30 11:30:37,296 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2409] 2025-05-30 11:30:38,497 >> ***** Running training *****\n",
      "[INFO|trainer.py:2410] 2025-05-30 11:30:38,497 >>   Num examples = 11,746\n",
      "[INFO|trainer.py:2411] 2025-05-30 11:30:38,497 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2412] 2025-05-30 11:30:38,497 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2415] 2025-05-30 11:30:38,497 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2416] 2025-05-30 11:30:38,497 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2417] 2025-05-30 11:30:38,497 >>   Total optimization steps = 4,407\n",
      "[INFO|trainer.py:2418] 2025-05-30 11:30:38,498 >>   Number of trainable parameters = 483,905,536\n",
      "[INFO|integration_utils.py:832] 2025-05-30 11:30:38,601 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: emmanuelka (paderborn-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.11\n",
      "wandb: Run data is saved locally in /home/emmanuelka/wandb/run-20250530_113038-kd59ht2s\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa\n",
      "wandb: ⭐️ View project at https://wandb.ai/paderborn-university/huggingface\n",
      "wandb: 🚀 View run at https://wandb.ai/paderborn-university/huggingface/runs/kd59ht2s\n",
      "  0%|          | 0/4407 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-05-30 11:30:39,598 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "[WARNING|logging.py:328] 2025-05-30 11:30:39,603 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "[rank1]:[W530 11:30:39.067651013 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W530 11:30:39.074477017 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      " 11%|█▏        | 500/4407 [04:47<37:12,  1.75it/s][INFO|trainer.py:3993] 2025-05-30 11:35:26,208 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-500\n",
      "/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/transformers-4.53.0.dev0-py3.11.egg/transformers/modeling_utils.py:3585: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 11:35:26,214 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 11:35:26,214 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1873, 'grad_norm': 5.172911167144775, 'learning_rate': 4.433855230315407e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 11:35:30,424 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 11:35:30,428 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 11:35:30,429 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 11:35:30,429 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-500/added_tokens.json\n",
      " 23%|██▎       | 1000/4407 [09:54<33:08,  1.71it/s][INFO|trainer.py:3993] 2025-05-30 11:40:33,501 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-1000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 11:40:33,503 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-1000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 11:40:33,504 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-1000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3876, 'grad_norm': 5.012275218963623, 'learning_rate': 3.866575901974132e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 11:40:38,476 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-1000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 11:40:38,478 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 11:40:38,479 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 11:40:38,479 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-1000/added_tokens.json\n",
      " 34%|███▍      | 1500/4407 [15:02<27:57,  1.73it/s][INFO|trainer.py:3993] 2025-05-30 11:45:41,390 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-1500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 11:45:41,392 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-1500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 11:45:41,392 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-1500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1579, 'grad_norm': 3.7992680072784424, 'learning_rate': 3.2992965736328565e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 11:45:45,796 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-1500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 11:45:45,798 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 11:45:45,799 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 11:45:45,799 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-1500/added_tokens.json\n",
      " 45%|████▌     | 2000/4407 [20:10<22:49,  1.76it/s][INFO|trainer.py:3993] 2025-05-30 11:50:49,717 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-2000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 11:50:49,719 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-2000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 11:50:49,720 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-2000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8127, 'grad_norm': 4.108773708343506, 'learning_rate': 2.7320172452915815e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 11:50:54,784 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-2000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 11:50:54,787 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 11:50:54,787 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 11:50:54,788 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-2000/added_tokens.json\n",
      " 57%|█████▋    | 2500/4407 [25:18<18:12,  1.75it/s][INFO|trainer.py:3993] 2025-05-30 11:55:57,228 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-2500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 11:55:57,230 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-2500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 11:55:57,230 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-2500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7301, 'grad_norm': 3.956315755844116, 'learning_rate': 2.1647379169503064e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 11:56:02,218 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-2500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 11:56:02,222 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 11:56:02,223 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-2500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 11:56:02,223 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-2500/added_tokens.json\n",
      " 68%|██████▊   | 3000/4407 [30:25<13:10,  1.78it/s][INFO|trainer.py:3993] 2025-05-30 12:01:04,927 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-3000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 12:01:04,929 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-3000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 12:01:04,930 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-3000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6353, 'grad_norm': 6.015838146209717, 'learning_rate': 1.597458588609031e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 12:01:09,622 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-3000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 12:01:09,625 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 12:01:09,625 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-3000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 12:01:09,626 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-3000/added_tokens.json\n",
      " 79%|███████▉  | 3500/4407 [35:33<08:35,  1.76it/s][INFO|trainer.py:3993] 2025-05-30 12:06:12,794 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-3500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 12:06:12,796 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-3500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 12:06:12,797 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-3500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4314, 'grad_norm': 3.594961166381836, 'learning_rate': 1.0301792602677559e-05, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 12:06:17,609 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-3500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 12:06:17,611 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 12:06:17,612 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-3500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 12:06:17,612 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-3500/added_tokens.json\n",
      " 91%|█████████ | 4000/4407 [40:41<03:52,  1.75it/s][INFO|trainer.py:3993] 2025-05-30 12:11:20,831 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-4000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 12:11:20,832 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-4000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 12:11:20,833 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-4000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4093, 'grad_norm': 4.862606525421143, 'learning_rate': 4.628999319264806e-06, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 12:11:25,464 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-4000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 12:11:25,467 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 12:11:25,467 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-4000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 12:11:25,468 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-4000/added_tokens.json\n",
      "100%|██████████| 4407/4407 [44:56<00:00,  1.72it/s][INFO|trainer.py:3993] 2025-05-30 12:15:35,266 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-4407\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 12:15:35,268 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-4407/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 12:15:35,268 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-4407/generation_config.json\n",
      "[INFO|modeling_utils.py:3845] 2025-05-30 12:15:40,080 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-4407/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 12:15:40,082 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-4407/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 12:15:40,083 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-4407/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 12:15:40,083 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/checkpoint-4407/added_tokens.json\n",
      "[INFO|trainer.py:2676] 2025-05-30 12:15:56,832 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 4407/4407 [45:17<00:00,  1.62it/s]\n",
      "[INFO|trainer.py:3993] 2025-05-30 12:15:56,836 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 12:15:56,840 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 12:15:56,840 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2718.3337, 'train_samples_per_second': 12.963, 'train_steps_per_second': 1.621, 'train_loss': 1.9139956618642926, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 12:16:06,426 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 12:16:06,431 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 12:16:06,432 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 12:16:06,432 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_2000_ssa/added_tokens.json\n",
      "[INFO|trainer.py:4327] 2025-05-30 12:16:06,623 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4329] 2025-05-30 12:16:06,623 >>   Num examples = 997\n",
      "[INFO|trainer.py:4332] 2025-05-30 12:16:06,623 >>   Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               =  3721570GF\n",
      "  train_loss               =      1.914\n",
      "  train_runtime            = 0:45:18.33\n",
      "  train_samples            =      11746\n",
      "  train_samples_per_second =     12.963\n",
      "  train_steps_per_second   =      1.621\n",
      "05/30/2025 12:16:06 - INFO - __main__ - *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [06:11<00:00,  2.97s/it]\n",
      "[INFO|trainer.py:4327] 2025-05-30 12:22:21,232 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:4329] 2025-05-30 12:22:21,232 >>   Num examples = 1012\n",
      "[INFO|trainer.py:4332] 2025-05-30 12:22:21,232 >>   Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_bleu               =     5.7368\n",
      "  eval_chrf               =    26.2146\n",
      "  eval_gen_len            =     72.502\n",
      "  eval_loss               =     2.9396\n",
      "  eval_runtime            = 0:06:14.68\n",
      "  eval_samples            =        997\n",
      "  eval_samples_per_second =      2.661\n",
      "  eval_steps_per_second   =      0.334\n",
      "05/30/2025 12:22:21 - INFO - __main__ - *** Predict ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [06:32<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** predict metrics *****\n",
      "  predict_bleu               =     5.6967\n",
      "  predict_chrf               =    25.5805\n",
      "  predict_gen_len            =    75.1969\n",
      "  predict_loss               =     2.9269\n",
      "  predict_runtime            = 0:06:36.59\n",
      "  predict_samples            =       1012\n",
      "  predict_samples_per_second =      2.552\n",
      "  predict_steps_per_second   =       0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modelcard.py:450] 2025-05-30 12:29:10,416 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 5.7368}]}\n",
      "[rank0]:[W530 12:29:10.235646086 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "W0530 12:29:14.181000 46844 torch/distributed/run.py:766] \n",
      "W0530 12:29:14.181000 46844 torch/distributed/run.py:766] *****************************************\n",
      "W0530 12:29:14.181000 46844 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0530 12:29:14.181000 46844 torch/distributed/run.py:766] *****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 12:29:19 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "05/30/2025 12:29:19 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/runs/May30_12-29-19_yoruba-vm,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "05/30/2025 12:29:19 - INFO - datasets.builder - Using custom data configuration default-b70d2174b754354a\n",
      "05/30/2025 12:29:19 - INFO - datasets.info - Loading Dataset Infos from /home/emmanuelka/jupyter-env/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "05/30/2025 12:29:19 - INFO - datasets.builder - Generating dataset json (/home/emmanuelka/.cache/huggingface/datasets/json/default-b70d2174b754354a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "05/30/2025 12:29:19 - INFO - datasets.builder - Downloading and preparing dataset json/default to /home/emmanuelka/.cache/huggingface/datasets/json/default-b70d2174b754354a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
      "05/30/2025 12:29:19 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "05/30/2025 12:29:19 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "05/30/2025 12:29:19 - INFO - datasets.builder - Generating train split\n",
      "05/30/2025 12:29:19 - INFO - datasets.builder - Generating validation split\n",
      "05/30/2025 12:29:19 - INFO - datasets.builder - Generating test split\n",
      "05/30/2025 12:29:19 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "05/30/2025 12:29:19 - INFO - datasets.builder - Dataset json downloaded and prepared to /home/emmanuelka/.cache/huggingface/datasets/json/default-b70d2174b754354a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b70d2174b754354a\n",
      "Loading Dataset Infos from /home/emmanuelka/jupyter-env/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "Generating dataset json (/home/emmanuelka/.cache/huggingface/datasets/json/default-b70d2174b754354a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "Downloading and preparing dataset json/default to /home/emmanuelka/.cache/huggingface/datasets/json/default-b70d2174b754354a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "Generating train split: 13746 examples [00:00, 286756.14 examples/s]\n",
      "Generating validation split\n",
      "Generating validation split: 997 examples [00:00, 177191.57 examples/s]\n",
      "Generating test split\n",
      "Generating test split: 1012 examples [00:00, 208192.84 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /home/emmanuelka/.cache/huggingface/datasets/json/default-b70d2174b754354a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
      "[INFO|configuration_utils.py:712] 2025-05-30 12:29:19,947 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-30 12:29:19,950 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:712] 2025-05-30 12:29:19,980 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-30 12:29:19,981 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 12:29:19 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 12:29:20,015 >> loading file vocab.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 12:29:20,015 >> loading file sentencepiece.bpe.model from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 12:29:20,015 >> loading file tokenizer_config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 12:29:20,015 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 12:29:20,015 >> loading file special_tokens_map.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 12:29:20,015 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 12:29:20,015 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:712] 2025-05-30 12:29:20,016 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-30 12:29:20,017 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1270] 2025-05-30 12:29:20,852 >> loading weights file pytorch_model.bin from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:1122] 2025-05-30 12:29:20,854 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "[INFO|safetensors_conversion.py:61] 2025-05-30 12:29:20,899 >> Attempting to create safetensors variant\n",
      "[INFO|safetensors_conversion.py:74] 2025-05-30 12:29:21,347 >> Safetensors PR exists\n",
      "[INFO|modeling_utils.py:5233] 2025-05-30 12:29:22,353 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:5241] 2025-05-30 12:29:22,353 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:1077] 2025-05-30 12:29:22,390 >> loading configuration file generation_config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
      "[INFO|configuration_utils.py:1122] 2025-05-30 12:29:22,391 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "[rank1]:[W530 12:29:22.812506822 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Running tokenizer on train dataset:   0%|          | 0/13746 [00:00<?, ? examples/s]Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-b70d2174b754354a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d3c95f909a2b544a.arrow\n",
      "Running tokenizer on train dataset:   7%|▋         | 1000/13746 [00:00<00:06, 1862.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 12:29:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-b70d2174b754354a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d3c95f909a2b544a.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset: 100%|██████████| 13746/13746 [00:06<00:00, 2130.27 examples/s]\n",
      "[rank0]:[W530 12:29:30.393668855 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Running tokenizer on validation dataset:   0%|          | 0/997 [00:00<?, ? examples/s]Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-b70d2174b754354a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-3a3ae185c4ab8d14.arrow\n",
      "Running tokenizer on validation dataset: 100%|██████████| 997/997 [00:00<00:00, 1895.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 12:29:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-b70d2174b754354a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-3a3ae185c4ab8d14.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on prediction dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-b70d2174b754354a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d8fefa2a90cd68ac.arrow\n",
      "Running tokenizer on prediction dataset: 100%|██████████| 1012/1012 [00:00<00:00, 1918.81 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 12:29:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-b70d2174b754354a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d8fefa2a90cd68ac.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:756] 2025-05-30 12:29:35,681 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2409] 2025-05-30 12:29:36,899 >> ***** Running training *****\n",
      "[INFO|trainer.py:2410] 2025-05-30 12:29:36,899 >>   Num examples = 13,746\n",
      "[INFO|trainer.py:2411] 2025-05-30 12:29:36,899 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2412] 2025-05-30 12:29:36,899 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2415] 2025-05-30 12:29:36,899 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2416] 2025-05-30 12:29:36,899 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2417] 2025-05-30 12:29:36,899 >>   Total optimization steps = 5,157\n",
      "[INFO|trainer.py:2418] 2025-05-30 12:29:36,901 >>   Number of trainable parameters = 483,905,536\n",
      "[INFO|integration_utils.py:832] 2025-05-30 12:29:37,007 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: emmanuelka (paderborn-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.11\n",
      "wandb: Run data is saved locally in /home/emmanuelka/wandb/run-20250530_122937-tpsnoldv\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa\n",
      "wandb: ⭐️ View project at https://wandb.ai/paderborn-university/huggingface\n",
      "wandb: 🚀 View run at https://wandb.ai/paderborn-university/huggingface/runs/tpsnoldv\n",
      "  0%|          | 0/5157 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-05-30 12:29:37,944 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "[WARNING|logging.py:328] 2025-05-30 12:29:37,961 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "[rank1]:[W530 12:29:38.416814488 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W530 12:29:38.435471475 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      " 10%|▉         | 500/5157 [04:46<44:20,  1.75it/s][INFO|trainer.py:3993] 2025-05-30 12:34:24,305 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-500\n",
      "/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/transformers-4.53.0.dev0-py3.11.egg/transformers/modeling_utils.py:3585: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 12:34:24,310 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 12:34:24,311 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3028, 'grad_norm': 4.6282172203063965, 'learning_rate': 4.516191584254412e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 12:34:28,475 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 12:34:28,479 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 12:34:28,479 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 12:34:28,480 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-500/added_tokens.json\n",
      " 19%|█▉        | 1000/5157 [09:53<39:56,  1.73it/s][INFO|trainer.py:3993] 2025-05-30 12:39:30,600 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-1000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 12:39:30,602 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-1000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 12:39:30,602 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-1000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.539, 'grad_norm': 7.469968795776367, 'learning_rate': 4.031413612565445e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 12:39:35,166 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-1000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 12:39:35,168 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 12:39:35,169 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 12:39:35,170 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-1000/added_tokens.json\n",
      " 29%|██▉       | 1500/5157 [15:01<34:41,  1.76it/s][INFO|trainer.py:3993] 2025-05-30 12:44:38,593 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-1500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 12:44:38,595 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-1500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 12:44:38,595 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-1500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2881, 'grad_norm': 5.154331684112549, 'learning_rate': 3.546635640876479e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 12:44:43,497 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-1500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 12:44:43,500 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 12:44:43,501 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 12:44:43,502 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-1500/added_tokens.json\n",
      " 39%|███▉      | 2000/5157 [20:09<30:09,  1.74it/s][INFO|trainer.py:3993] 2025-05-30 12:49:46,805 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-2000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 12:49:46,807 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-2000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 12:49:46,808 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-2000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0073, 'grad_norm': 4.712835311889648, 'learning_rate': 3.0618576691875125e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 12:49:51,492 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-2000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 12:49:51,494 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 12:49:51,495 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 12:49:51,495 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-2000/added_tokens.json\n",
      " 48%|████▊     | 2500/5157 [25:16<25:13,  1.76it/s][INFO|trainer.py:3993] 2025-05-30 12:54:54,385 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-2500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 12:54:54,387 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-2500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 12:54:54,388 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-2500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8048, 'grad_norm': 4.299944877624512, 'learning_rate': 2.5770796974985456e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 12:54:58,819 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-2500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 12:54:58,822 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 12:54:58,822 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-2500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 12:54:58,822 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-2500/added_tokens.json\n",
      " 58%|█████▊    | 3000/5157 [30:23<20:37,  1.74it/s][INFO|trainer.py:3993] 2025-05-30 13:00:01,336 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-3000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 13:00:01,338 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-3000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 13:00:01,339 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-3000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7717, 'grad_norm': 3.9476075172424316, 'learning_rate': 2.0923017258095793e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 13:00:05,959 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-3000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 13:00:05,962 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 13:00:05,963 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-3000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 13:00:05,963 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-3000/added_tokens.json\n",
      " 68%|██████▊   | 3500/5157 [35:30<15:47,  1.75it/s][INFO|trainer.py:3993] 2025-05-30 13:05:08,062 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-3500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 13:05:08,064 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-3500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 13:05:08,065 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-3500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.704, 'grad_norm': 4.043184280395508, 'learning_rate': 1.6075237541206128e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 13:05:12,959 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-3500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 13:05:12,962 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 13:05:12,962 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-3500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 13:05:12,963 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-3500/added_tokens.json\n",
      " 78%|███████▊  | 4000/5157 [40:37<10:49,  1.78it/s][INFO|trainer.py:3993] 2025-05-30 13:10:15,279 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-4000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 13:10:15,281 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-4000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 13:10:15,281 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-4000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4782, 'grad_norm': 4.12581729888916, 'learning_rate': 1.1227457824316463e-05, 'epoch': 2.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 13:10:19,921 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-4000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 13:10:19,926 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 13:10:19,926 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-4000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 13:10:19,926 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-4000/added_tokens.json\n",
      " 87%|████████▋ | 4500/5157 [45:44<06:06,  1.79it/s][INFO|trainer.py:3993] 2025-05-30 13:15:22,501 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-4500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 13:15:22,503 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-4500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 13:15:22,503 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-4500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4406, 'grad_norm': 4.8821024894714355, 'learning_rate': 6.389373666860578e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 13:15:27,393 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-4500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 13:15:27,396 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 13:15:27,396 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-4500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 13:15:27,397 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-4500/added_tokens.json\n",
      " 97%|█████████▋| 5000/5157 [50:51<01:31,  1.71it/s][INFO|trainer.py:3993] 2025-05-30 13:20:29,154 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-5000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 13:20:29,156 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-5000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 13:20:29,156 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-5000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4354, 'grad_norm': 4.96586275100708, 'learning_rate': 1.5415939499709134e-06, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 13:20:34,636 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-5000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 13:20:34,638 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 13:20:34,639 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-5000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 13:20:34,639 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-5000/added_tokens.json\n",
      "100%|██████████| 5157/5157 [52:42<00:00,  1.75it/s][INFO|trainer.py:3993] 2025-05-30 13:22:20,554 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-5157\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 13:22:20,556 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-5157/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 13:22:20,557 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-5157/generation_config.json\n",
      "[INFO|modeling_utils.py:3845] 2025-05-30 13:22:25,350 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-5157/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 13:22:25,354 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-5157/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 13:22:25,354 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-5157/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 13:22:25,355 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/checkpoint-5157/added_tokens.json\n",
      "[INFO|trainer.py:2676] 2025-05-30 13:22:41,662 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 5157/5157 [53:04<00:00,  1.62it/s]\n",
      "[INFO|trainer.py:3993] 2025-05-30 13:22:41,665 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 13:22:41,667 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 13:22:41,668 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3184.7619, 'train_samples_per_second': 12.949, 'train_steps_per_second': 1.619, 'train_loss': 1.9595579598933337, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 13:22:51,603 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 13:22:51,606 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 13:22:51,606 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 13:22:51,607 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_4000_ssa/added_tokens.json\n",
      "[INFO|trainer.py:4327] 2025-05-30 13:22:51,715 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4329] 2025-05-30 13:22:51,715 >>   Num examples = 997\n",
      "[INFO|trainer.py:4332] 2025-05-30 13:22:51,715 >>   Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               =  4191449GF\n",
      "  train_loss               =     1.9596\n",
      "  train_runtime            = 0:53:04.76\n",
      "  train_samples            =      13746\n",
      "  train_samples_per_second =     12.949\n",
      "  train_steps_per_second   =      1.619\n",
      "05/30/2025 13:22:51 - INFO - __main__ - *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [05:57<00:00,  2.86s/it]\n",
      "[INFO|trainer.py:4327] 2025-05-30 13:28:53,181 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:4329] 2025-05-30 13:28:53,181 >>   Num examples = 1012\n",
      "[INFO|trainer.py:4332] 2025-05-30 13:28:53,182 >>   Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_bleu               =     6.0183\n",
      "  eval_chrf               =    26.5157\n",
      "  eval_gen_len            =     69.008\n",
      "  eval_loss               =     2.8793\n",
      "  eval_runtime            = 0:06:01.46\n",
      "  eval_samples            =        997\n",
      "  eval_samples_per_second =      2.758\n",
      "  eval_steps_per_second   =      0.346\n",
      "05/30/2025 13:28:53 - INFO - __main__ - *** Predict ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [06:21<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** predict metrics *****\n",
      "  predict_bleu               =     6.0212\n",
      "  predict_chrf               =     26.284\n",
      "  predict_gen_len            =    72.2766\n",
      "  predict_loss               =     2.8675\n",
      "  predict_runtime            = 0:06:25.39\n",
      "  predict_samples            =       1012\n",
      "  predict_samples_per_second =      2.626\n",
      "  predict_steps_per_second   =       0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modelcard.py:450] 2025-05-30 13:35:31,178 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 6.0183}]}\n",
      "[rank0]:[W530 13:35:31.025444392 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "language_pairs = [\"en-yo\"]  # Add more as needed\n",
    "text_nums = [1000 * 2 ** i for i in range(3)]\n",
    "base_dir = \"/home/emmanuelka\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "for lang_pair in language_pairs:\n",
    "    src, tgt = lang_pair.split(\"-\")\n",
    "\n",
    "    for num in text_nums:\n",
    "        train_file = f\"{base_dir}/AIMS-NLP-Project/data/nllb/ssa-comet-mtl/{lang_pair}/trainer_{num}.json\"\n",
    "        val_file = f\"{base_dir}/AIMS-NLP-Project/data/flores/{lang_pair}/dev.json\"\n",
    "        test_file = f\"{base_dir}/AIMS-NLP-Project/data/flores/{lang_pair}/devtest.json\"\n",
    "        output_dir = f\"{base_dir}/M2M-100/{lang_pair}/africomet_base_{num}_ssa\"\n",
    "\n",
    "        cmd = [\n",
    "            \"torchrun\",\n",
    "            \"--nproc_per_node=2\",\n",
    "            f\"{base_dir}/AIMS-NLP-Project/run_translation.py\",\n",
    "            \"--model_name_or_path\", \"facebook/m2m100_418M\",\n",
    "            \"--do_train\", \"--do_eval\", \"--do_predict\",\n",
    "            \"--source_lang\", src,\n",
    "            \"--target_lang\", tgt,\n",
    "            \"--train_file\", train_file,\n",
    "            \"--validation_file\", val_file,\n",
    "            \"--test_file\", test_file,\n",
    "            \"--num_beams\", \"10\",\n",
    "            \"--output_dir\", output_dir,\n",
    "            \"--per_device_train_batch_size\", \"4\",\n",
    "            \"--per_device_eval_batch_size\", \"4\",\n",
    "            \"--overwrite_output_dir\",\n",
    "            \"--predict_with_generate\",\n",
    "            \"--fp16\",  # comment this out if your GPU doesn’t support fp16\n",
    "        ]\n",
    "\n",
    "        # Launch multi-GPU run\n",
    "        subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92d0c1d-0988-4269-81c4-8657d6e11074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/emmanuelka/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmanuelka\u001b[0m (\u001b[33mpaderborn-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Log in with your API key\n",
    "wandb.login(key=\"7f13d9fe09d0856f7c12099a27ccda7aa15c8afd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a1824f-56ac-4313-aa94-99b35ec2a804",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0530 18:27:01.827000 82755 torch/distributed/run.py:766] \n",
      "W0530 18:27:01.827000 82755 torch/distributed/run.py:766] *****************************************\n",
      "W0530 18:27:01.827000 82755 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0530 18:27:01.827000 82755 torch/distributed/run.py:766] *****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 18:27:07 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "05/30/2025 18:27:07 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/runs/May30_18-27-06_yoruba-vm,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "05/30/2025 18:27:07 - INFO - datasets.builder - Using custom data configuration default-fd0e5690c25edd91\n",
      "05/30/2025 18:27:07 - INFO - datasets.info - Loading Dataset Infos from /home/emmanuelka/jupyter-env/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "05/30/2025 18:27:07 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "05/30/2025 18:27:07 - INFO - datasets.info - Loading Dataset info from /home/emmanuelka/.cache/huggingface/datasets/json/default-fd0e5690c25edd91/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
      "05/30/2025 18:27:07 - INFO - datasets.builder - Found cached dataset json (/home/emmanuelka/.cache/huggingface/datasets/json/default-fd0e5690c25edd91/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "05/30/2025 18:27:07 - INFO - datasets.info - Loading Dataset info from /home/emmanuelka/.cache/huggingface/datasets/json/default-fd0e5690c25edd91/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
      "05/30/2025 18:27:07 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fd0e5690c25edd91\n",
      "Loading Dataset Infos from /home/emmanuelka/jupyter-env/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/emmanuelka/.cache/huggingface/datasets/json/default-fd0e5690c25edd91/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
      "Found cached dataset json (/home/emmanuelka/.cache/huggingface/datasets/json/default-fd0e5690c25edd91/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "Loading Dataset info from /home/emmanuelka/.cache/huggingface/datasets/json/default-fd0e5690c25edd91/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
      "[INFO|configuration_utils.py:712] 2025-05-30 18:27:07,400 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-30 18:27:07,404 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:712] 2025-05-30 18:27:07,435 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-30 18:27:07,436 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 18:27:07,480 >> loading file vocab.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 18:27:07,481 >> loading file sentencepiece.bpe.model from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/sentencepiece.bpe.model\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 18:27:07,481 >> loading file tokenizer_config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 18:27:07,481 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 18:27:07,481 >> loading file special_tokens_map.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 18:27:07,481 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2012] 2025-05-30 18:27:07,481 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:712] 2025-05-30 18:27:07,481 >> loading configuration file config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/config.json\n",
      "[INFO|configuration_utils.py:775] 2025-05-30 18:27:07,482 >> Model config M2M100Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"M2M100ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.05,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.05,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"m2m_100\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128112\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1270] 2025-05-30 18:27:08,361 >> loading weights file pytorch_model.bin from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:1122] 2025-05-30 18:27:08,362 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "[INFO|safetensors_conversion.py:61] 2025-05-30 18:27:08,408 >> Attempting to create safetensors variant\n",
      "[INFO|safetensors_conversion.py:74] 2025-05-30 18:27:08,560 >> Safetensors PR exists\n",
      "[INFO|modeling_utils.py:5233] 2025-05-30 18:27:09,958 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:5241] 2025-05-30 18:27:09,958 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:1077] 2025-05-30 18:27:09,991 >> loading configuration file generation_config.json from cache at /home/emmanuelka/.cache/huggingface/hub/models--facebook--m2m100_418M/snapshots/55c2e61bbf05dfb8d7abccdc3fae6fc8512fd636/generation_config.json\n",
      "[INFO|configuration_utils.py:1122] 2025-05-30 18:27:09,991 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "[rank1]:[W530 18:27:10.414156026 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-fd0e5690c25edd91/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e9a3793234ed89a9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 18:27:11 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-fd0e5690c25edd91/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-e9a3793234ed89a9.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W530 18:27:11.784944697 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-fd0e5690c25edd91/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0ea9bdc0360fa393.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 18:27:12 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-fd0e5690c25edd91/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0ea9bdc0360fa393.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-fd0e5690c25edd91/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-139205ea03211532.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30/2025 18:27:13 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/emmanuelka/.cache/huggingface/datasets/json/default-fd0e5690c25edd91/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-139205ea03211532.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:756] 2025-05-30 18:27:15,838 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2409] 2025-05-30 18:27:17,084 >> ***** Running training *****\n",
      "[INFO|trainer.py:2410] 2025-05-30 18:27:17,084 >>   Num examples = 41,746\n",
      "[INFO|trainer.py:2411] 2025-05-30 18:27:17,084 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2412] 2025-05-30 18:27:17,085 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2415] 2025-05-30 18:27:17,085 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2416] 2025-05-30 18:27:17,085 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2417] 2025-05-30 18:27:17,085 >>   Total optimization steps = 15,657\n",
      "[INFO|trainer.py:2418] 2025-05-30 18:27:17,086 >>   Number of trainable parameters = 483,905,536\n",
      "[INFO|integration_utils.py:832] 2025-05-30 18:27:17,203 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: emmanuelka (paderborn-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.11\n",
      "wandb: Run data is saved locally in /home/emmanuelka/wandb/run-20250530_182717-0q9h709o\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa\n",
      "wandb: ⭐️ View project at https://wandb.ai/paderborn-university/huggingface\n",
      "wandb: 🚀 View run at https://wandb.ai/paderborn-university/huggingface/runs/0q9h709o\n",
      "  0%|          | 0/15657 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-05-30 18:27:18,237 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "[WARNING|logging.py:328] 2025-05-30 18:27:18,300 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "[rank1]:[W530 18:27:18.700799430 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W530 18:27:18.779172238 reducer.cpp:1430] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "  3%|▎         | 500/15657 [04:50<2:25:19,  1.74it/s][INFO|trainer.py:3993] 2025-05-30 18:32:07,892 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-500\n",
      "/home/emmanuelka/jupyter-env/lib/python3.11/site-packages/transformers-4.53.0.dev0-py3.11.egg/transformers/modeling_utils.py:3585: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 18:32:07,896 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 18:32:07,896 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6747, 'grad_norm': 7.148093223571777, 'learning_rate': 4.840646356262375e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 18:32:10,907 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 18:32:10,910 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 18:32:10,910 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 18:32:10,911 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-500/added_tokens.json\n",
      "  6%|▋         | 1000/15657 [09:59<2:21:27,  1.73it/s][INFO|trainer.py:3993] 2025-05-30 18:37:16,995 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-1000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 18:37:16,997 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-1000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 18:37:16,998 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-1000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0401, 'grad_norm': 5.329383373260498, 'learning_rate': 4.6809733665453156e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 18:37:20,736 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-1000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 18:37:20,738 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 18:37:20,739 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 18:37:20,739 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-1000/added_tokens.json\n",
      " 10%|▉         | 1500/15657 [15:09<2:15:56,  1.74it/s][INFO|trainer.py:3993] 2025-05-30 18:42:26,968 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-1500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 18:42:26,970 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-1500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 18:42:26,971 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-1500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8064, 'grad_norm': 4.487744331359863, 'learning_rate': 4.5213003768282557e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 18:42:30,625 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-1500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 18:42:30,640 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 18:42:30,641 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 18:42:30,641 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-1500/added_tokens.json\n",
      " 13%|█▎        | 2000/15657 [20:19<2:13:08,  1.71it/s][INFO|trainer.py:3993] 2025-05-30 18:47:36,847 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-2000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 18:47:36,849 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-2000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 18:47:36,850 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-2000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.69, 'grad_norm': 5.179208755493164, 'learning_rate': 4.3616273871111963e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 18:47:40,887 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-2000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 18:47:40,890 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 18:47:40,890 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 18:47:40,891 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-2000/added_tokens.json\n",
      " 16%|█▌        | 2500/15657 [25:29<2:04:53,  1.76it/s][INFO|trainer.py:3993] 2025-05-30 18:52:46,923 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-2500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 18:52:46,925 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-2500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 18:52:46,926 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-2500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5444, 'grad_norm': 5.484577178955078, 'learning_rate': 4.201954397394137e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 18:52:51,041 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-2500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 18:52:51,044 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 18:52:51,044 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-2500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 18:52:51,045 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-2500/added_tokens.json\n",
      " 19%|█▉        | 3000/15657 [30:39<2:02:35,  1.72it/s][INFO|trainer.py:3993] 2025-05-30 18:57:57,079 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-3000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 18:57:57,081 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-3000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 18:57:57,082 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-3000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4814, 'grad_norm': 4.389553546905518, 'learning_rate': 4.042281407677078e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 18:58:00,802 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-3000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 18:58:00,805 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 18:58:00,805 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-3000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 18:58:00,805 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-3000/added_tokens.json\n",
      " 22%|██▏       | 3500/15657 [35:49<1:56:33,  1.74it/s][INFO|trainer.py:3993] 2025-05-30 19:03:06,956 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-3500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 19:03:06,958 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-3500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 19:03:06,959 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-3500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4205, 'grad_norm': 5.008378982543945, 'learning_rate': 3.8829277639394524e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 19:03:11,137 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-3500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 19:03:11,139 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 19:03:11,140 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-3500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 19:03:11,140 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-3500/added_tokens.json\n",
      " 26%|██▌       | 4000/15657 [40:59<1:52:07,  1.73it/s][INFO|trainer.py:3993] 2025-05-30 19:08:17,095 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-4000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 19:08:17,097 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-4000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 19:08:17,097 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-4000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3595, 'grad_norm': 4.746139049530029, 'learning_rate': 3.7232547742223924e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 19:08:21,233 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-4000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 19:08:21,235 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 19:08:21,236 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-4000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 19:08:21,236 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-4000/added_tokens.json\n",
      " 29%|██▊       | 4500/15657 [46:09<1:47:13,  1.73it/s][INFO|trainer.py:3993] 2025-05-30 19:13:27,051 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-4500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 19:13:27,053 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-4500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 19:13:27,053 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-4500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3146, 'grad_norm': 5.941005229949951, 'learning_rate': 3.563581784505333e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 19:13:30,994 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-4500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 19:13:30,997 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 19:13:30,998 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-4500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 19:13:30,998 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-4500/added_tokens.json\n",
      " 32%|███▏      | 5000/15657 [51:18<1:42:51,  1.73it/s][INFO|trainer.py:3993] 2025-05-30 19:18:36,453 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-5000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 19:18:36,455 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-5000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 19:18:36,455 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-5000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2964, 'grad_norm': 4.863738536834717, 'learning_rate': 3.403908794788274e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 19:18:40,835 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-5000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 19:18:40,838 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 19:18:40,838 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-5000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 19:18:40,839 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-5000/added_tokens.json\n",
      " 35%|███▌      | 5500/15657 [56:28<1:38:45,  1.71it/s][INFO|trainer.py:3993] 2025-05-30 19:23:45,895 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-5500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 19:23:45,897 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-5500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 19:23:45,897 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-5500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1019, 'grad_norm': 4.619062900543213, 'learning_rate': 3.244235805071214e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 19:23:50,106 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-5500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 19:23:50,109 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 19:23:50,110 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-5500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 19:23:50,111 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-5500/added_tokens.json\n",
      " 38%|███▊      | 6000/15657 [1:01:37<1:32:16,  1.74it/s][INFO|trainer.py:3993] 2025-05-30 19:28:55,132 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-6000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 19:28:55,134 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-6000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 19:28:55,134 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-6000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9937, 'grad_norm': 4.7587480545043945, 'learning_rate': 3.0848821613335886e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 19:28:59,021 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-6000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 19:28:59,024 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 19:28:59,024 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-6000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 19:28:59,025 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-6000/added_tokens.json\n",
      " 42%|████▏     | 6500/15657 [1:06:47<1:28:51,  1.72it/s][INFO|trainer.py:3993] 2025-05-30 19:34:04,873 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-6500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 19:34:04,875 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-6500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 19:34:04,875 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-6500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9683, 'grad_norm': 3.711879253387451, 'learning_rate': 2.9252091716165296e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 19:34:09,256 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-6500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 19:34:09,261 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 19:34:09,262 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-6500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 19:34:09,263 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-6500/added_tokens.json\n",
      " 45%|████▍     | 7000/15657 [1:11:57<1:24:22,  1.71it/s][INFO|trainer.py:3993] 2025-05-30 19:39:15,291 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-7000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 19:39:15,293 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-7000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 19:39:15,293 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-7000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9527, 'grad_norm': 6.153733730316162, 'learning_rate': 2.7655361818994703e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 19:39:19,352 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-7000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 19:39:19,355 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 19:39:19,355 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-7000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 19:39:19,356 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-7000/added_tokens.json\n",
      " 48%|████▊     | 7500/15657 [1:17:06<1:18:32,  1.73it/s][INFO|trainer.py:3993] 2025-05-30 19:44:24,680 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-7500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 19:44:24,682 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-7500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 19:44:24,683 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-7500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.95, 'grad_norm': 4.75769567489624, 'learning_rate': 2.6058631921824106e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 19:44:28,649 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-7500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 19:44:28,651 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 19:44:28,652 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-7500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 19:44:28,652 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-7500/added_tokens.json\n",
      " 51%|█████     | 8000/15657 [1:22:16<1:14:19,  1.72it/s][INFO|trainer.py:3993] 2025-05-30 19:49:34,247 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-8000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 19:49:34,249 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-8000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 19:49:34,250 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-8000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.912, 'grad_norm': 3.508218288421631, 'learning_rate': 2.446190202465351e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 19:49:38,693 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-8000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 19:49:38,695 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 19:49:38,696 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-8000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 19:49:38,696 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-8000/added_tokens.json\n",
      " 54%|█████▍    | 8500/15657 [1:27:26<1:08:24,  1.74it/s][INFO|trainer.py:3993] 2025-05-30 19:54:44,067 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-8500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 19:54:44,069 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-8500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 19:54:44,069 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-8500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9036, 'grad_norm': 4.444437503814697, 'learning_rate': 2.2865172127482917e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 19:54:48,026 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-8500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 19:54:48,029 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 19:54:48,029 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-8500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 19:54:48,030 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-8500/added_tokens.json\n",
      " 57%|█████▋    | 9000/15657 [1:32:36<1:04:29,  1.72it/s][INFO|trainer.py:3993] 2025-05-30 19:59:54,176 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-9000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 19:59:54,178 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-9000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 19:59:54,179 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-9000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8779, 'grad_norm': 3.682636260986328, 'learning_rate': 2.126844223031232e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 19:59:58,728 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-9000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 19:59:58,731 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 19:59:58,731 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-9000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 19:59:58,732 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-9000/added_tokens.json\n",
      " 61%|██████    | 9500/15657 [1:37:46<59:06,  1.74it/s][INFO|trainer.py:3993] 2025-05-30 20:05:03,856 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-9500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 20:05:03,858 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-9500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 20:05:03,858 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-9500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8835, 'grad_norm': 5.038056373596191, 'learning_rate': 1.9671712333141727e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 20:05:08,022 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-9500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 20:05:08,025 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 20:05:08,025 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-9500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 20:05:08,026 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-9500/added_tokens.json\n",
      " 64%|██████▍   | 10000/15657 [1:42:55<54:24,  1.73it/s][INFO|trainer.py:3993] 2025-05-30 20:10:13,143 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-10000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 20:10:13,145 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-10000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 20:10:13,146 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-10000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8558, 'grad_norm': 3.6374804973602295, 'learning_rate': 1.8078175895765474e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 20:10:17,294 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-10000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 20:10:17,296 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 20:10:17,297 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-10000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 20:10:17,297 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-10000/added_tokens.json\n",
      " 67%|██████▋   | 10500/15657 [1:48:05<49:26,  1.74it/s][INFO|trainer.py:3993] 2025-05-30 20:15:22,900 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-10500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 20:15:22,902 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-10500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 20:15:22,903 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-10500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8527, 'grad_norm': 4.111948490142822, 'learning_rate': 1.6481445998594878e-05, 'epoch': 2.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 20:15:27,288 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-10500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 20:15:27,291 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 20:15:27,291 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-10500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 20:15:27,292 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-10500/added_tokens.json\n",
      " 70%|███████   | 11000/15657 [1:53:15<46:15,  1.68it/s][INFO|trainer.py:3993] 2025-05-30 20:20:32,798 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-11000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 20:20:32,800 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-11000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 20:20:32,801 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-11000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6294, 'grad_norm': 3.75001859664917, 'learning_rate': 1.4887909561218625e-05, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 20:20:37,902 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-11000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 20:20:37,905 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 20:20:37,906 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-11000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 20:20:37,906 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-11000/added_tokens.json\n",
      " 73%|███████▎  | 11500/15657 [1:58:25<40:08,  1.73it/s][INFO|trainer.py:3993] 2025-05-30 20:25:43,010 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-11500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 20:25:43,012 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-11500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 20:25:43,013 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-11500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6188, 'grad_norm': 5.014443874359131, 'learning_rate': 1.3291179664048032e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 20:25:47,107 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-11500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 20:25:47,110 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 20:25:47,111 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-11500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 20:25:47,111 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-11500/added_tokens.json\n",
      " 77%|███████▋  | 12000/15657 [2:03:35<35:13,  1.73it/s][INFO|trainer.py:3993] 2025-05-30 20:30:53,201 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-12000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 20:30:53,202 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-12000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 20:30:53,203 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-12000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5731, 'grad_norm': 5.038050174713135, 'learning_rate': 1.1694449766877435e-05, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 20:30:57,899 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-12000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 20:30:57,901 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 20:30:57,902 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-12000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 20:30:57,902 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-12000/added_tokens.json\n",
      " 80%|███████▉  | 12500/15657 [2:08:45<30:13,  1.74it/s][INFO|trainer.py:3993] 2025-05-30 20:36:02,848 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-12500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 20:36:02,850 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-12500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 20:36:02,851 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-12500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5938, 'grad_norm': 4.882561206817627, 'learning_rate': 1.009771986970684e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 20:36:07,229 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-12500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 20:36:07,232 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 20:36:07,233 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-12500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 20:36:07,233 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-12500/added_tokens.json\n",
      " 83%|████████▎ | 13000/15657 [2:13:54<25:30,  1.74it/s][INFO|trainer.py:3993] 2025-05-30 20:41:11,859 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-13000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 20:41:11,861 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-13000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 20:41:11,861 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-13000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5994, 'grad_norm': 3.9914140701293945, 'learning_rate': 8.500989972536247e-06, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 20:41:15,933 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-13000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 20:41:15,935 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 20:41:15,936 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-13000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 20:41:15,936 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-13000/added_tokens.json\n",
      " 86%|████████▌ | 13500/15657 [2:19:03<20:39,  1.74it/s][INFO|trainer.py:3993] 2025-05-30 20:46:21,406 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-13500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 20:46:21,407 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-13500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 20:46:21,408 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-13500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5832, 'grad_norm': 3.8670105934143066, 'learning_rate': 6.904260075365651e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 20:46:25,971 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-13500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 20:46:25,974 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 20:46:25,974 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-13500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 20:46:25,975 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-13500/added_tokens.json\n",
      " 89%|████████▉ | 14000/15657 [2:24:12<15:55,  1.73it/s][INFO|trainer.py:3993] 2025-05-30 20:51:30,545 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-14000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 20:51:30,547 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-14000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 20:51:30,548 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-14000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5905, 'grad_norm': 5.310188293457031, 'learning_rate': 5.307530178195057e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 20:51:34,550 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-14000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 20:51:34,553 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 20:51:34,553 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-14000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 20:51:34,554 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-14000/added_tokens.json\n",
      " 93%|█████████▎| 14500/15657 [2:29:21<11:03,  1.74it/s][INFO|trainer.py:3993] 2025-05-30 20:56:39,748 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-14500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 20:56:39,750 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-14500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 20:56:39,751 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-14500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5734, 'grad_norm': 4.461040019989014, 'learning_rate': 3.7108002810244625e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 20:56:44,030 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-14500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 20:56:44,032 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 20:56:44,033 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-14500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 20:56:44,033 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-14500/added_tokens.json\n",
      " 96%|█████████▌| 15000/15657 [2:34:31<06:22,  1.72it/s][INFO|trainer.py:3993] 2025-05-30 21:01:49,427 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15000\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 21:01:49,429 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15000/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 21:01:49,430 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5573, 'grad_norm': 3.5974369049072266, 'learning_rate': 2.1172638436482083e-06, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 21:01:53,884 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 21:01:53,887 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 21:01:53,888 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 21:01:53,888 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15000/added_tokens.json\n",
      " 99%|█████████▉| 15500/15657 [2:39:41<01:30,  1.74it/s][INFO|trainer.py:3993] 2025-05-30 21:06:59,402 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15500\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 21:06:59,404 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15500/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 21:06:59,405 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15500/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5527, 'grad_norm': 4.878178596496582, 'learning_rate': 5.205339464776139e-07, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 21:07:03,527 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 21:07:03,530 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 21:07:03,530 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 21:07:03,531 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15500/added_tokens.json\n",
      "100%|██████████| 15657/15657 [2:41:33<00:00,  1.73it/s][INFO|trainer.py:3993] 2025-05-30 21:08:51,102 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15657\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 21:08:51,104 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15657/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 21:08:51,104 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15657/generation_config.json\n",
      "[INFO|modeling_utils.py:3845] 2025-05-30 21:08:55,675 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15657/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 21:08:55,679 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15657/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 21:08:55,679 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15657/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 21:08:55,680 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/checkpoint-15657/added_tokens.json\n",
      "[INFO|trainer.py:2676] 2025-05-30 21:09:12,302 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 15657/15657 [2:41:54<00:00,  1.61it/s]\n",
      "[INFO|trainer.py:3993] 2025-05-30 21:09:12,306 >> Saving model checkpoint to /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa\n",
      "[INFO|configuration_utils.py:438] 2025-05-30 21:09:12,309 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/config.json\n",
      "[INFO|configuration_utils.py:891] 2025-05-30 21:09:12,310 >> Configuration saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 9715.2165, 'train_samples_per_second': 12.891, 'train_steps_per_second': 1.612, 'train_loss': 2.0517318861050695, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3845] 2025-05-30 21:09:21,844 >> Model weights saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2507] 2025-05-30 21:09:21,847 >> tokenizer config file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2516] 2025-05-30 21:09:21,847 >> Special tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2567] 2025-05-30 21:09:21,848 >> added tokens file saved in /home/emmanuelka/M2M-100/en-yo/africomet_base_32000_ssa/added_tokens.json\n",
      "[INFO|trainer.py:4327] 2025-05-30 21:09:21,962 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4329] 2025-05-30 21:09:21,963 >>   Num examples = 997\n",
      "[INFO|trainer.py:4332] 2025-05-30 21:09:21,963 >>   Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 10098746GF\n",
      "  train_loss               =     2.0517\n",
      "  train_runtime            = 2:41:55.21\n",
      "  train_samples            =      41746\n",
      "  train_samples_per_second =     12.891\n",
      "  train_steps_per_second   =      1.612\n",
      "05/30/2025 21:09:21 - INFO - __main__ - *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [05:07<00:00,  2.46s/it]\n",
      "[INFO|trainer.py:4327] 2025-05-30 21:14:33,236 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:4329] 2025-05-30 21:14:33,236 >>   Num examples = 1012\n",
      "[INFO|trainer.py:4332] 2025-05-30 21:14:33,236 >>   Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_bleu               =     4.9152\n",
      "  eval_chrf               =    25.0733\n",
      "  eval_gen_len            =     57.507\n",
      "  eval_loss               =     2.6288\n",
      "  eval_runtime            = 0:05:11.26\n",
      "  eval_samples            =        997\n",
      "  eval_samples_per_second =      3.203\n",
      "  eval_steps_per_second   =      0.402\n",
      "05/30/2025 21:14:33 - INFO - __main__ - *** Predict ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [05:18<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** predict metrics *****\n",
      "  predict_bleu               =     4.7853\n",
      "  predict_chrf               =    24.7578\n",
      "  predict_gen_len            =    59.1417\n",
      "  predict_loss               =     2.6185\n",
      "  predict_runtime            = 0:05:22.28\n",
      "  predict_samples            =       1012\n",
      "  predict_samples_per_second =       3.14\n",
      "  predict_steps_per_second   =      0.394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modelcard.py:450] 2025-05-30 21:20:07,719 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 4.9152}]}\n",
      "[rank0]:[W530 21:20:08.559323145 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "language_pairs = [\"en-yo\"]  # Add more as needed\n",
    "text_nums = [1000 * 2 ** i for i in range(5, 6)]\n",
    "base_dir = \"/home/emmanuelka\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "for lang_pair in language_pairs:\n",
    "    src, tgt = lang_pair.split(\"-\")\n",
    "\n",
    "    for num in text_nums:\n",
    "        train_file = f\"{base_dir}/AIMS-NLP-Project/data/nllb/ssa-comet-mtl/{lang_pair}/trainer_{num}.json\"\n",
    "        val_file = f\"{base_dir}/AIMS-NLP-Project/data/flores/{lang_pair}/dev.json\"\n",
    "        test_file = f\"{base_dir}/AIMS-NLP-Project/data/flores/{lang_pair}/devtest.json\"\n",
    "        output_dir = f\"{base_dir}/M2M-100/{lang_pair}/africomet_base_{num}_ssa\"\n",
    "\n",
    "        cmd = [\n",
    "            \"torchrun\",\n",
    "            \"--nproc_per_node=2\",\n",
    "            f\"{base_dir}/AIMS-NLP-Project/run_translation.py\",\n",
    "            \"--model_name_or_path\", \"facebook/m2m100_418M\",\n",
    "            \"--do_train\", \"--do_eval\", \"--do_predict\",\n",
    "            \"--source_lang\", src,\n",
    "            \"--target_lang\", tgt,\n",
    "            \"--train_file\", train_file,\n",
    "            \"--validation_file\", val_file,\n",
    "            \"--test_file\", test_file,\n",
    "            \"--num_beams\", \"10\",\n",
    "            \"--output_dir\", output_dir,\n",
    "            \"--per_device_train_batch_size\", \"4\",\n",
    "            \"--per_device_eval_batch_size\", \"4\",\n",
    "            \"--overwrite_output_dir\",\n",
    "            \"--predict_with_generate\",\n",
    "            \"--fp16\",  # comment this out if your GPU doesn’t support fp16\n",
    "        ]\n",
    "\n",
    "        # Launch multi-GPU run\n",
    "        subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d96b07-e155-42b6-af5b-1dd97cd87db0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
